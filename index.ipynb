{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IBM\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "complaints_onehot = tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(complaints_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_transform = le.transform(product) \n",
    "product_onehot = to_categorical(product_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(complaints_onehot, product_onehot, test_size=2000, random_state=42)\n",
    "#X_train = \n",
    "#X_test = \n",
    "#y_train = \n",
    "#y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\IBM\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\IBM\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7000/7000 [==============================] - 4s 606us/step - loss: 1.9412 - acc: 0.1580 - val_loss: 1.9288 - val_acc: 0.1680\n",
      "Epoch 2/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 1.9217 - acc: 0.1899 - val_loss: 1.9104 - val_acc: 0.1940\n",
      "Epoch 3/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.9010 - acc: 0.2140 - val_loss: 1.8891 - val_acc: 0.2250\n",
      "Epoch 4/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 1.8769 - acc: 0.2299 - val_loss: 1.8638 - val_acc: 0.2370\n",
      "Epoch 5/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.8491 - acc: 0.2414 - val_loss: 1.8355 - val_acc: 0.2480\n",
      "Epoch 6/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.8185 - acc: 0.2571 - val_loss: 1.8040 - val_acc: 0.2730\n",
      "Epoch 7/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.7847 - acc: 0.2834 - val_loss: 1.7684 - val_acc: 0.2990\n",
      "Epoch 8/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 1.7474 - acc: 0.3113 - val_loss: 1.7304 - val_acc: 0.3380\n",
      "Epoch 9/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.7065 - acc: 0.3497 - val_loss: 1.6886 - val_acc: 0.3640\n",
      "Epoch 10/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6614 - acc: 0.3807 - val_loss: 1.6425 - val_acc: 0.3870\n",
      "Epoch 11/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.6128 - acc: 0.4103 - val_loss: 1.5951 - val_acc: 0.4200\n",
      "Epoch 12/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.5621 - acc: 0.4486 - val_loss: 1.5462 - val_acc: 0.4510\n",
      "Epoch 13/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.5103 - acc: 0.4784 - val_loss: 1.4971 - val_acc: 0.4810\n",
      "Epoch 14/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.4575 - acc: 0.5060 - val_loss: 1.4466 - val_acc: 0.5050\n",
      "Epoch 15/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.4045 - acc: 0.5307 - val_loss: 1.3972 - val_acc: 0.5260\n",
      "Epoch 16/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 1.3523 - acc: 0.5581 - val_loss: 1.3491 - val_acc: 0.5350\n",
      "Epoch 17/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.3010 - acc: 0.5779 - val_loss: 1.3023 - val_acc: 0.5680\n",
      "Epoch 18/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.2513 - acc: 0.6024 - val_loss: 1.2575 - val_acc: 0.5870\n",
      "Epoch 19/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.2037 - acc: 0.6227 - val_loss: 1.2132 - val_acc: 0.6050\n",
      "Epoch 20/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.1584 - acc: 0.6414 - val_loss: 1.1734 - val_acc: 0.6310\n",
      "Epoch 21/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.1160 - acc: 0.6590 - val_loss: 1.1358 - val_acc: 0.6440\n",
      "Epoch 22/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.0759 - acc: 0.6716 - val_loss: 1.0999 - val_acc: 0.6570\n",
      "Epoch 23/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.0387 - acc: 0.6870 - val_loss: 1.0667 - val_acc: 0.6670\n",
      "Epoch 24/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.0046 - acc: 0.6971 - val_loss: 1.0367 - val_acc: 0.6720\n",
      "Epoch 25/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.9727 - acc: 0.7093 - val_loss: 1.0095 - val_acc: 0.6780\n",
      "Epoch 26/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9435 - acc: 0.7127 - val_loss: 0.9840 - val_acc: 0.6890\n",
      "Epoch 27/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9163 - acc: 0.7237 - val_loss: 0.9592 - val_acc: 0.6920\n",
      "Epoch 28/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.8911 - acc: 0.7293 - val_loss: 0.9385 - val_acc: 0.7050\n",
      "Epoch 29/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8682 - acc: 0.7331 - val_loss: 0.9189 - val_acc: 0.7060\n",
      "Epoch 30/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8468 - acc: 0.7377 - val_loss: 0.9003 - val_acc: 0.7100\n",
      "Epoch 31/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8269 - acc: 0.7429 - val_loss: 0.8827 - val_acc: 0.7100\n",
      "Epoch 32/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.8082 - acc: 0.7473 - val_loss: 0.8670 - val_acc: 0.7130\n",
      "Epoch 33/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.7906 - acc: 0.7524 - val_loss: 0.8560 - val_acc: 0.7100\n",
      "Epoch 34/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.7749 - acc: 0.7551 - val_loss: 0.8398 - val_acc: 0.7180\n",
      "Epoch 35/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.7594 - acc: 0.7583 - val_loss: 0.8271 - val_acc: 0.7150\n",
      "Epoch 36/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.7453 - acc: 0.7631 - val_loss: 0.8145 - val_acc: 0.7210\n",
      "Epoch 37/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.7315 - acc: 0.7644 - val_loss: 0.8039 - val_acc: 0.7220\n",
      "Epoch 38/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.7189 - acc: 0.7706 - val_loss: 0.7966 - val_acc: 0.7270\n",
      "Epoch 39/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.7068 - acc: 0.7737 - val_loss: 0.7841 - val_acc: 0.7220\n",
      "Epoch 40/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.6953 - acc: 0.7760 - val_loss: 0.7752 - val_acc: 0.7240\n",
      "Epoch 41/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.6844 - acc: 0.7796 - val_loss: 0.7659 - val_acc: 0.7290\n",
      "Epoch 42/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.6742 - acc: 0.7823 - val_loss: 0.7589 - val_acc: 0.7280\n",
      "Epoch 43/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.6639 - acc: 0.7859 - val_loss: 0.7540 - val_acc: 0.7280\n",
      "Epoch 44/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.6551 - acc: 0.7874 - val_loss: 0.7454 - val_acc: 0.7260\n",
      "Epoch 45/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.6452 - acc: 0.7899 - val_loss: 0.7417 - val_acc: 0.7330\n",
      "Epoch 46/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.6372 - acc: 0.7914 - val_loss: 0.7327 - val_acc: 0.7330\n",
      "Epoch 47/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.6286 - acc: 0.7950 - val_loss: 0.7270 - val_acc: 0.7330\n",
      "Epoch 48/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.6205 - acc: 0.7991 - val_loss: 0.7212 - val_acc: 0.7310\n",
      "Epoch 49/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.6127 - acc: 0.8007 - val_loss: 0.7174 - val_acc: 0.7260\n",
      "Epoch 50/120\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.6050 - acc: 0.8029 - val_loss: 0.7126 - val_acc: 0.7340\n",
      "Epoch 51/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.5980 - acc: 0.8047 - val_loss: 0.7068 - val_acc: 0.7360\n",
      "Epoch 52/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.5908 - acc: 0.8059 - val_loss: 0.7035 - val_acc: 0.7360\n",
      "Epoch 53/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.5843 - acc: 0.8091 - val_loss: 0.7002 - val_acc: 0.7360\n",
      "Epoch 54/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.5782 - acc: 0.8094 - val_loss: 0.6931 - val_acc: 0.7360\n",
      "Epoch 55/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.5713 - acc: 0.8123 - val_loss: 0.6909 - val_acc: 0.7370\n",
      "Epoch 56/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.5648 - acc: 0.8134 - val_loss: 0.6868 - val_acc: 0.7390\n",
      "Epoch 57/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.5595 - acc: 0.8166 - val_loss: 0.6842 - val_acc: 0.7390\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.5530 - acc: 0.8200 - val_loss: 0.6791 - val_acc: 0.7430\n",
      "Epoch 59/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.5473 - acc: 0.8196 - val_loss: 0.6757 - val_acc: 0.7410\n",
      "Epoch 60/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.5416 - acc: 0.8211 - val_loss: 0.6746 - val_acc: 0.7420\n",
      "Epoch 61/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.5369 - acc: 0.8223 - val_loss: 0.6694 - val_acc: 0.7390\n",
      "Epoch 62/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.5314 - acc: 0.8256 - val_loss: 0.6679 - val_acc: 0.7400\n",
      "Epoch 63/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.5264 - acc: 0.8254 - val_loss: 0.6646 - val_acc: 0.7470\n",
      "Epoch 64/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.5206 - acc: 0.8297 - val_loss: 0.6631 - val_acc: 0.7410\n",
      "Epoch 65/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.5162 - acc: 0.8291 - val_loss: 0.6596 - val_acc: 0.7460\n",
      "Epoch 66/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.5117 - acc: 0.8329 - val_loss: 0.6606 - val_acc: 0.7480\n",
      "Epoch 67/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.5068 - acc: 0.8317 - val_loss: 0.6550 - val_acc: 0.7460\n",
      "Epoch 68/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.5020 - acc: 0.8337 - val_loss: 0.6503 - val_acc: 0.7500\n",
      "Epoch 69/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.4976 - acc: 0.8366 - val_loss: 0.6512 - val_acc: 0.7430\n",
      "Epoch 70/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.4932 - acc: 0.8373 - val_loss: 0.6502 - val_acc: 0.7460\n",
      "Epoch 71/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.4885 - acc: 0.8383 - val_loss: 0.6470 - val_acc: 0.7490\n",
      "Epoch 72/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.4843 - acc: 0.8403 - val_loss: 0.6431 - val_acc: 0.7490\n",
      "Epoch 73/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.4799 - acc: 0.8423 - val_loss: 0.6424 - val_acc: 0.7510\n",
      "Epoch 74/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.4758 - acc: 0.8434 - val_loss: 0.6421 - val_acc: 0.7460\n",
      "Epoch 75/120\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 0.4716 - acc: 0.8450 - val_loss: 0.6387 - val_acc: 0.7500\n",
      "Epoch 76/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.4678 - acc: 0.8477 - val_loss: 0.6358 - val_acc: 0.7580\n",
      "Epoch 77/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.4639 - acc: 0.8484 - val_loss: 0.6361 - val_acc: 0.7490\n",
      "Epoch 78/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.4596 - acc: 0.8517 - val_loss: 0.6358 - val_acc: 0.7610\n",
      "Epoch 79/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.4560 - acc: 0.8526 - val_loss: 0.6317 - val_acc: 0.7550\n",
      "Epoch 80/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.4522 - acc: 0.8521 - val_loss: 0.6310 - val_acc: 0.7580\n",
      "Epoch 81/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.4486 - acc: 0.8543 - val_loss: 0.6316 - val_acc: 0.7570\n",
      "Epoch 82/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.4450 - acc: 0.8514 - val_loss: 0.6297 - val_acc: 0.7580\n",
      "Epoch 83/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.4416 - acc: 0.8559 - val_loss: 0.6271 - val_acc: 0.7560\n",
      "Epoch 84/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.4376 - acc: 0.8577 - val_loss: 0.6261 - val_acc: 0.7590\n",
      "Epoch 85/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.4341 - acc: 0.8567 - val_loss: 0.6271 - val_acc: 0.7590\n",
      "Epoch 86/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.4311 - acc: 0.8586 - val_loss: 0.6260 - val_acc: 0.7600\n",
      "Epoch 87/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.4275 - acc: 0.8609 - val_loss: 0.6226 - val_acc: 0.7620\n",
      "Epoch 88/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.4241 - acc: 0.8620 - val_loss: 0.6243 - val_acc: 0.7570\n",
      "Epoch 89/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.4209 - acc: 0.8627 - val_loss: 0.6242 - val_acc: 0.7660\n",
      "Epoch 90/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.4173 - acc: 0.8629 - val_loss: 0.6198 - val_acc: 0.7630\n",
      "Epoch 91/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.4143 - acc: 0.8643 - val_loss: 0.6204 - val_acc: 0.7620\n",
      "Epoch 92/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.4111 - acc: 0.8650 - val_loss: 0.6201 - val_acc: 0.7660\n",
      "Epoch 93/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.4079 - acc: 0.8674 - val_loss: 0.6220 - val_acc: 0.7570\n",
      "Epoch 94/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.4048 - acc: 0.8697 - val_loss: 0.6192 - val_acc: 0.7620\n",
      "Epoch 95/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.4013 - acc: 0.8694 - val_loss: 0.6185 - val_acc: 0.7640\n",
      "Epoch 96/120\n",
      "7000/7000 [==============================] - 1s 78us/step - loss: 0.3983 - acc: 0.8701 - val_loss: 0.6164 - val_acc: 0.7670\n",
      "Epoch 97/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.3953 - acc: 0.8711 - val_loss: 0.6177 - val_acc: 0.7690\n",
      "Epoch 98/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.3928 - acc: 0.8729 - val_loss: 0.6150 - val_acc: 0.7690\n",
      "Epoch 99/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.3894 - acc: 0.8729 - val_loss: 0.6204 - val_acc: 0.7560\n",
      "Epoch 100/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.3871 - acc: 0.8747 - val_loss: 0.6159 - val_acc: 0.7660\n",
      "Epoch 101/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.3841 - acc: 0.8760 - val_loss: 0.6161 - val_acc: 0.7680\n",
      "Epoch 102/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.3806 - acc: 0.8761 - val_loss: 0.6147 - val_acc: 0.7690\n",
      "Epoch 103/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.3781 - acc: 0.8783 - val_loss: 0.6136 - val_acc: 0.7660\n",
      "Epoch 104/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.3753 - acc: 0.8789 - val_loss: 0.6153 - val_acc: 0.7630\n",
      "Epoch 105/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.3724 - acc: 0.8806 - val_loss: 0.6126 - val_acc: 0.7630\n",
      "Epoch 106/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.3700 - acc: 0.8800 - val_loss: 0.6133 - val_acc: 0.7610\n",
      "Epoch 107/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.3670 - acc: 0.8821 - val_loss: 0.6153 - val_acc: 0.7640\n",
      "Epoch 108/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.3643 - acc: 0.8830 - val_loss: 0.6133 - val_acc: 0.7680\n",
      "Epoch 109/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.3614 - acc: 0.8844 - val_loss: 0.6144 - val_acc: 0.7690\n",
      "Epoch 110/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.3590 - acc: 0.8846 - val_loss: 0.6122 - val_acc: 0.7670\n",
      "Epoch 111/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.3563 - acc: 0.8863 - val_loss: 0.6143 - val_acc: 0.7630\n",
      "Epoch 112/120\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.3539 - acc: 0.8879 - val_loss: 0.6125 - val_acc: 0.7650\n",
      "Epoch 113/120\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.3513 - acc: 0.8900 - val_loss: 0.6115 - val_acc: 0.7700\n",
      "Epoch 114/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.3489 - acc: 0.8897 - val_loss: 0.6147 - val_acc: 0.7670\n",
      "Epoch 115/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.3465 - acc: 0.8897 - val_loss: 0.6116 - val_acc: 0.7720\n",
      "Epoch 116/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.3435 - acc: 0.8916 - val_loss: 0.6118 - val_acc: 0.7690\n",
      "Epoch 117/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.3408 - acc: 0.8930 - val_loss: 0.6162 - val_acc: 0.7690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.3388 - acc: 0.8933 - val_loss: 0.6117 - val_acc: 0.7670\n",
      "Epoch 119/120\n",
      "7000/7000 [==============================] - 1s 80us/step - loss: 0.3360 - acc: 0.8954 - val_loss: 0.6138 - val_acc: 0.7660\n",
      "Epoch 120/120\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.3336 - acc: 0.8957 - val_loss: 0.6141 - val_acc: 0.7670\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 69us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 70us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3311251732962472, 0.8942857143538339]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6225404253005982, 0.7685]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7000/7000 [==============================] - 1s 162us/step - loss: 1.9573 - acc: 0.1303 - val_loss: 1.9516 - val_acc: 0.1390\n",
      "Epoch 2/60\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.9379 - acc: 0.1597 - val_loss: 1.9363 - val_acc: 0.1550\n",
      "Epoch 3/60\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.9221 - acc: 0.1880 - val_loss: 1.9198 - val_acc: 0.1810\n",
      "Epoch 4/60\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.9041 - acc: 0.2040 - val_loss: 1.9000 - val_acc: 0.2060\n",
      "Epoch 5/60\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.8837 - acc: 0.2311 - val_loss: 1.8783 - val_acc: 0.2370\n",
      "Epoch 6/60\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.8611 - acc: 0.2547 - val_loss: 1.8550 - val_acc: 0.2550\n",
      "Epoch 7/60\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.8356 - acc: 0.2820 - val_loss: 1.8286 - val_acc: 0.2810\n",
      "Epoch 8/60\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.8060 - acc: 0.3159 - val_loss: 1.7978 - val_acc: 0.3100\n",
      "Epoch 9/60\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.7719 - acc: 0.3521 - val_loss: 1.7628 - val_acc: 0.3520\n",
      "Epoch 10/60\n",
      "7000/7000 [==============================] - 1s 94us/step - loss: 1.7335 - acc: 0.3851 - val_loss: 1.7234 - val_acc: 0.3830\n",
      "Epoch 11/60\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.6910 - acc: 0.4201 - val_loss: 1.6818 - val_acc: 0.4280\n",
      "Epoch 12/60\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.6458 - acc: 0.4520 - val_loss: 1.6372 - val_acc: 0.4400\n",
      "Epoch 13/60\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.5992 - acc: 0.4701 - val_loss: 1.5931 - val_acc: 0.4670\n",
      "Epoch 14/60\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.5522 - acc: 0.4904 - val_loss: 1.5489 - val_acc: 0.4840\n",
      "Epoch 15/60\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.5053 - acc: 0.5110 - val_loss: 1.5061 - val_acc: 0.4940\n",
      "Epoch 16/60\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.4594 - acc: 0.5250 - val_loss: 1.4634 - val_acc: 0.5180\n",
      "Epoch 17/60\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.4151 - acc: 0.5444 - val_loss: 1.4221 - val_acc: 0.5250\n",
      "Epoch 18/60\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.3721 - acc: 0.5549 - val_loss: 1.3844 - val_acc: 0.5420\n",
      "Epoch 19/60\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.3307 - acc: 0.5677 - val_loss: 1.3460 - val_acc: 0.5590\n",
      "Epoch 20/60\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.2906 - acc: 0.5846 - val_loss: 1.3102 - val_acc: 0.5740\n",
      "Epoch 21/60\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.2524 - acc: 0.6053 - val_loss: 1.2764 - val_acc: 0.5890\n",
      "Epoch 22/60\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.2155 - acc: 0.6176 - val_loss: 1.2437 - val_acc: 0.6000\n",
      "Epoch 23/60\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.1799 - acc: 0.6331 - val_loss: 1.2096 - val_acc: 0.6100\n",
      "Epoch 24/60\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.1459 - acc: 0.6437 - val_loss: 1.1788 - val_acc: 0.6180\n",
      "Epoch 25/60\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.1130 - acc: 0.6586 - val_loss: 1.1494 - val_acc: 0.6240\n",
      "Epoch 26/60\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.0815 - acc: 0.6694 - val_loss: 1.1196 - val_acc: 0.6300\n",
      "Epoch 27/60\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.0510 - acc: 0.6796 - val_loss: 1.0941 - val_acc: 0.6410\n",
      "Epoch 28/60\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0215 - acc: 0.6904 - val_loss: 1.0682 - val_acc: 0.6460\n",
      "Epoch 29/60\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9937 - acc: 0.6960 - val_loss: 1.0414 - val_acc: 0.6540\n",
      "Epoch 30/60\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.9669 - acc: 0.7056 - val_loss: 1.0186 - val_acc: 0.6730\n",
      "Epoch 31/60\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.9410 - acc: 0.7111 - val_loss: 0.9990 - val_acc: 0.6760\n",
      "Epoch 32/60\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9170 - acc: 0.7177 - val_loss: 0.9774 - val_acc: 0.6820\n",
      "Epoch 33/60\n",
      "7000/7000 [==============================] - 1s 108us/step - loss: 0.8938 - acc: 0.7239 - val_loss: 0.9586 - val_acc: 0.6940\n",
      "Epoch 34/60\n",
      "7000/7000 [==============================] - 1s 103us/step - loss: 0.8719 - acc: 0.7291 - val_loss: 0.9352 - val_acc: 0.6980\n",
      "Epoch 35/60\n",
      "7000/7000 [==============================] - 1s 94us/step - loss: 0.8514 - acc: 0.7353 - val_loss: 0.9185 - val_acc: 0.7030\n",
      "Epoch 36/60\n",
      "7000/7000 [==============================] - 1s 102us/step - loss: 0.8310 - acc: 0.7409 - val_loss: 0.9021 - val_acc: 0.7060\n",
      "Epoch 37/60\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.8128 - acc: 0.7433 - val_loss: 0.8877 - val_acc: 0.7110\n",
      "Epoch 38/60\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.7948 - acc: 0.7479 - val_loss: 0.8717 - val_acc: 0.7120\n",
      "Epoch 39/60\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.7782 - acc: 0.7526 - val_loss: 0.8559 - val_acc: 0.7230\n",
      "Epoch 40/60\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 0.7619 - acc: 0.7566 - val_loss: 0.8426 - val_acc: 0.7180\n",
      "Epoch 41/60\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.7462 - acc: 0.7600 - val_loss: 0.8318 - val_acc: 0.7290\n",
      "Epoch 42/60\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.7320 - acc: 0.7649 - val_loss: 0.8219 - val_acc: 0.7260\n",
      "Epoch 43/60\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.7186 - acc: 0.7683 - val_loss: 0.8100 - val_acc: 0.7300\n",
      "Epoch 44/60\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.7055 - acc: 0.7710 - val_loss: 0.7989 - val_acc: 0.7300\n",
      "Epoch 45/60\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.6939 - acc: 0.7750 - val_loss: 0.7907 - val_acc: 0.7340\n",
      "Epoch 46/60\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.6816 - acc: 0.7787 - val_loss: 0.7805 - val_acc: 0.7340\n",
      "Epoch 47/60\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.6707 - acc: 0.7820 - val_loss: 0.7737 - val_acc: 0.7350\n",
      "Epoch 48/60\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.6605 - acc: 0.7837 - val_loss: 0.7660 - val_acc: 0.7350\n",
      "Epoch 49/60\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.6499 - acc: 0.7874 - val_loss: 0.7609 - val_acc: 0.7330\n",
      "Epoch 50/60\n",
      "7000/7000 [==============================] - 1s 101us/step - loss: 0.6405 - acc: 0.7887 - val_loss: 0.7537 - val_acc: 0.7380\n",
      "Epoch 51/60\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.6315 - acc: 0.7904 - val_loss: 0.7438 - val_acc: 0.7410\n",
      "Epoch 52/60\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.6224 - acc: 0.7944 - val_loss: 0.7373 - val_acc: 0.7380\n",
      "Epoch 53/60\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.6137 - acc: 0.7993 - val_loss: 0.7334 - val_acc: 0.7390\n",
      "Epoch 54/60\n",
      "7000/7000 [==============================] - 1s 79us/step - loss: 0.6056 - acc: 0.8026 - val_loss: 0.7291 - val_acc: 0.7440\n",
      "Epoch 55/60\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.5980 - acc: 0.8019 - val_loss: 0.7211 - val_acc: 0.7430\n",
      "Epoch 56/60\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.5904 - acc: 0.8037 - val_loss: 0.7227 - val_acc: 0.7430\n",
      "Epoch 57/60\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.5832 - acc: 0.8070 - val_loss: 0.7121 - val_acc: 0.7530\n",
      "Epoch 58/60\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.5758 - acc: 0.808 - 1s 82us/step - loss: 0.5760 - acc: 0.8087 - val_loss: 0.7080 - val_acc: 0.7510\n",
      "Epoch 59/60\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.5691 - acc: 0.8099 - val_loss: 0.7019 - val_acc: 0.7480\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.5628 - acc: 0.8123 - val_loss: 0.6968 - val_acc: 0.7520\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 64us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 67us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5577560610089983, 0.8179999999318804]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6877339272499084, 0.7495]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7000/7000 [==============================] - 1s 210us/step - loss: 2.5789 - acc: 0.1944 - val_loss: 2.5679 - val_acc: 0.1970\n",
      "Epoch 2/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 2.5515 - acc: 0.2266 - val_loss: 2.5413 - val_acc: 0.2330\n",
      "Epoch 3/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 2.5230 - acc: 0.2500 - val_loss: 2.5126 - val_acc: 0.2550\n",
      "Epoch 4/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 2.4897 - acc: 0.2756 - val_loss: 2.4796 - val_acc: 0.2890\n",
      "Epoch 5/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 2.4504 - acc: 0.3081 - val_loss: 2.4397 - val_acc: 0.3170\n",
      "Epoch 6/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 2.4062 - acc: 0.3351 - val_loss: 2.3964 - val_acc: 0.3630\n",
      "Epoch 7/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 2.3597 - acc: 0.3714 - val_loss: 2.3501 - val_acc: 0.3930\n",
      "Epoch 8/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 2.3112 - acc: 0.4066 - val_loss: 2.3014 - val_acc: 0.4260\n",
      "Epoch 9/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.2610 - acc: 0.4366 - val_loss: 2.2524 - val_acc: 0.4550\n",
      "Epoch 10/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 2.2097 - acc: 0.4651 - val_loss: 2.2010 - val_acc: 0.4900\n",
      "Epoch 11/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 2.1571 - acc: 0.4936 - val_loss: 2.1505 - val_acc: 0.5190\n",
      "Epoch 12/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 2.1044 - acc: 0.5187 - val_loss: 2.0991 - val_acc: 0.5510\n",
      "Epoch 13/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.0513 - acc: 0.5511 - val_loss: 2.0478 - val_acc: 0.5760\n",
      "Epoch 14/120\n",
      "7000/7000 [==============================] - 1s 94us/step - loss: 1.9989 - acc: 0.5749 - val_loss: 1.9975 - val_acc: 0.5970\n",
      "Epoch 15/120\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.9474 - acc: 0.5973 - val_loss: 1.9476 - val_acc: 0.6090\n",
      "Epoch 16/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.8976 - acc: 0.6157 - val_loss: 1.9001 - val_acc: 0.6320\n",
      "Epoch 17/120\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.8496 - acc: 0.6380 - val_loss: 1.8559 - val_acc: 0.6600\n",
      "Epoch 18/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.8039 - acc: 0.6551 - val_loss: 1.8129 - val_acc: 0.6730\n",
      "Epoch 19/120\n",
      "7000/7000 [==============================] - 1s 94us/step - loss: 1.7604 - acc: 0.6680 - val_loss: 1.7755 - val_acc: 0.6820\n",
      "Epoch 20/120\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.7198 - acc: 0.6814 - val_loss: 1.7355 - val_acc: 0.6880\n",
      "Epoch 21/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.6816 - acc: 0.6914 - val_loss: 1.7006 - val_acc: 0.6950\n",
      "Epoch 22/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.6462 - acc: 0.6986 - val_loss: 1.6690 - val_acc: 0.6950\n",
      "Epoch 23/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.6128 - acc: 0.7079 - val_loss: 1.6396 - val_acc: 0.6970\n",
      "Epoch 24/120\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 1.5853 - acc: 0.714 - 1s 86us/step - loss: 1.5817 - acc: 0.7159 - val_loss: 1.6113 - val_acc: 0.6990\n",
      "Epoch 25/120\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 1.5529 - acc: 0.7199 - val_loss: 1.5869 - val_acc: 0.7000\n",
      "Epoch 26/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.5260 - acc: 0.7257 - val_loss: 1.5604 - val_acc: 0.7090\n",
      "Epoch 27/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.5011 - acc: 0.7333 - val_loss: 1.5401 - val_acc: 0.7150\n",
      "Epoch 28/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.4780 - acc: 0.7350 - val_loss: 1.5186 - val_acc: 0.7180\n",
      "Epoch 29/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.4560 - acc: 0.7424 - val_loss: 1.5006 - val_acc: 0.7220\n",
      "Epoch 30/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.4359 - acc: 0.7450 - val_loss: 1.4807 - val_acc: 0.7280\n",
      "Epoch 31/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.4165 - acc: 0.7491 - val_loss: 1.4636 - val_acc: 0.7310\n",
      "Epoch 32/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.3987 - acc: 0.7516 - val_loss: 1.4482 - val_acc: 0.7300\n",
      "Epoch 33/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.3819 - acc: 0.7547 - val_loss: 1.4342 - val_acc: 0.7320\n",
      "Epoch 34/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.3658 - acc: 0.7571 - val_loss: 1.4223 - val_acc: 0.7350\n",
      "Epoch 35/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.3510 - acc: 0.7597 - val_loss: 1.4087 - val_acc: 0.7370\n",
      "Epoch 36/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.3363 - acc: 0.7627 - val_loss: 1.3950 - val_acc: 0.7380\n",
      "Epoch 37/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.3226 - acc: 0.7657 - val_loss: 1.3841 - val_acc: 0.7370\n",
      "Epoch 38/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.3100 - acc: 0.7679 - val_loss: 1.3725 - val_acc: 0.7420\n",
      "Epoch 39/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.2979 - acc: 0.7711 - val_loss: 1.3637 - val_acc: 0.7420\n",
      "Epoch 40/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.2859 - acc: 0.7714 - val_loss: 1.3518 - val_acc: 0.7450\n",
      "Epoch 41/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.2747 - acc: 0.7750 - val_loss: 1.3436 - val_acc: 0.7440\n",
      "Epoch 42/120\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 1.2638 - acc: 0.7797 - val_loss: 1.3358 - val_acc: 0.7510\n",
      "Epoch 43/120\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 1.2534 - acc: 0.7801 - val_loss: 1.3264 - val_acc: 0.7500\n",
      "Epoch 44/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.2433 - acc: 0.7829 - val_loss: 1.3179 - val_acc: 0.7520\n",
      "Epoch 45/120\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 1.2337 - acc: 0.7877 - val_loss: 1.3101 - val_acc: 0.7480\n",
      "Epoch 46/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.2243 - acc: 0.7899 - val_loss: 1.3041 - val_acc: 0.7470\n",
      "Epoch 47/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.2153 - acc: 0.7901 - val_loss: 1.2967 - val_acc: 0.7500\n",
      "Epoch 48/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.2066 - acc: 0.7927 - val_loss: 1.2917 - val_acc: 0.7470\n",
      "Epoch 49/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.1980 - acc: 0.7957 - val_loss: 1.2842 - val_acc: 0.7540\n",
      "Epoch 50/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.1900 - acc: 0.7976 - val_loss: 1.2788 - val_acc: 0.7450\n",
      "Epoch 51/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.1816 - acc: 0.7999 - val_loss: 1.2732 - val_acc: 0.7510\n",
      "Epoch 52/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.1741 - acc: 0.7999 - val_loss: 1.2654 - val_acc: 0.7530\n",
      "Epoch 53/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.1667 - acc: 0.8020 - val_loss: 1.2603 - val_acc: 0.7560\n",
      "Epoch 54/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.1593 - acc: 0.8051 - val_loss: 1.2540 - val_acc: 0.7520\n",
      "Epoch 55/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.1520 - acc: 0.8066 - val_loss: 1.2497 - val_acc: 0.7540\n",
      "Epoch 56/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.1449 - acc: 0.8086 - val_loss: 1.2422 - val_acc: 0.7550\n",
      "Epoch 57/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.1380 - acc: 0.8106 - val_loss: 1.2375 - val_acc: 0.7610\n",
      "Epoch 58/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1312 - acc: 0.8090 - val_loss: 1.2364 - val_acc: 0.7590\n",
      "Epoch 59/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.1248 - acc: 0.8104 - val_loss: 1.2286 - val_acc: 0.7540\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.1179 - acc: 0.8126 - val_loss: 1.2242 - val_acc: 0.7640\n",
      "Epoch 61/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.1119 - acc: 0.8159 - val_loss: 1.2209 - val_acc: 0.7620\n",
      "Epoch 62/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.1055 - acc: 0.8161 - val_loss: 1.2164 - val_acc: 0.7590\n",
      "Epoch 63/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0993 - acc: 0.8176 - val_loss: 1.2122 - val_acc: 0.7620\n",
      "Epoch 64/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0933 - acc: 0.8199 - val_loss: 1.2089 - val_acc: 0.7580\n",
      "Epoch 65/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0873 - acc: 0.8209 - val_loss: 1.2006 - val_acc: 0.7560\n",
      "Epoch 66/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0816 - acc: 0.8253 - val_loss: 1.1977 - val_acc: 0.7540\n",
      "Epoch 67/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0757 - acc: 0.8240 - val_loss: 1.1964 - val_acc: 0.7650\n",
      "Epoch 68/120\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 1.0700 - acc: 0.8261 - val_loss: 1.1894 - val_acc: 0.7630\n",
      "Epoch 69/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0642 - acc: 0.8266 - val_loss: 1.1868 - val_acc: 0.7610\n",
      "Epoch 70/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0592 - acc: 0.8269 - val_loss: 1.1837 - val_acc: 0.7640\n",
      "Epoch 71/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0539 - acc: 0.8277 - val_loss: 1.1793 - val_acc: 0.7660\n",
      "Epoch 72/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0482 - acc: 0.8299 - val_loss: 1.1737 - val_acc: 0.7630\n",
      "Epoch 73/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0435 - acc: 0.8311 - val_loss: 1.1699 - val_acc: 0.7650\n",
      "Epoch 74/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0382 - acc: 0.8330 - val_loss: 1.1711 - val_acc: 0.7710\n",
      "Epoch 75/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0331 - acc: 0.8334 - val_loss: 1.1653 - val_acc: 0.7720\n",
      "Epoch 76/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0277 - acc: 0.8357 - val_loss: 1.1592 - val_acc: 0.7650\n",
      "Epoch 77/120\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.0227 - acc: 0.8381 - val_loss: 1.1569 - val_acc: 0.7720\n",
      "Epoch 78/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0179 - acc: 0.8369 - val_loss: 1.1565 - val_acc: 0.7720\n",
      "Epoch 79/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0129 - acc: 0.8374 - val_loss: 1.1550 - val_acc: 0.7710\n",
      "Epoch 80/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0083 - acc: 0.8399 - val_loss: 1.1556 - val_acc: 0.7700\n",
      "Epoch 81/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0034 - acc: 0.8403 - val_loss: 1.1451 - val_acc: 0.7720\n",
      "Epoch 82/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9986 - acc: 0.8411 - val_loss: 1.1405 - val_acc: 0.7730\n",
      "Epoch 83/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9938 - acc: 0.8463 - val_loss: 1.1392 - val_acc: 0.7740\n",
      "Epoch 84/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9890 - acc: 0.8439 - val_loss: 1.1348 - val_acc: 0.7790\n",
      "Epoch 85/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9848 - acc: 0.8451 - val_loss: 1.1328 - val_acc: 0.7740\n",
      "Epoch 86/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9804 - acc: 0.8461 - val_loss: 1.1317 - val_acc: 0.7770\n",
      "Epoch 87/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9758 - acc: 0.8479 - val_loss: 1.1283 - val_acc: 0.7770\n",
      "Epoch 88/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9713 - acc: 0.8493 - val_loss: 1.1246 - val_acc: 0.7740\n",
      "Epoch 89/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9675 - acc: 0.8490 - val_loss: 1.1203 - val_acc: 0.7790\n",
      "Epoch 90/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9626 - acc: 0.8524 - val_loss: 1.1221 - val_acc: 0.7730\n",
      "Epoch 91/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9583 - acc: 0.8511 - val_loss: 1.1159 - val_acc: 0.7770\n",
      "Epoch 92/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9539 - acc: 0.8536 - val_loss: 1.1117 - val_acc: 0.7730\n",
      "Epoch 93/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9498 - acc: 0.8550 - val_loss: 1.1113 - val_acc: 0.7790\n",
      "Epoch 94/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9459 - acc: 0.8569 - val_loss: 1.1095 - val_acc: 0.7780\n",
      "Epoch 95/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9417 - acc: 0.8551 - val_loss: 1.1068 - val_acc: 0.7810\n",
      "Epoch 96/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9378 - acc: 0.8566 - val_loss: 1.1039 - val_acc: 0.7790\n",
      "Epoch 97/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9335 - acc: 0.8584 - val_loss: 1.0983 - val_acc: 0.7760\n",
      "Epoch 98/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9299 - acc: 0.8600 - val_loss: 1.0991 - val_acc: 0.7800\n",
      "Epoch 99/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9257 - acc: 0.8616 - val_loss: 1.0972 - val_acc: 0.7810\n",
      "Epoch 100/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9219 - acc: 0.8614 - val_loss: 1.0918 - val_acc: 0.7830\n",
      "Epoch 101/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9179 - acc: 0.8627 - val_loss: 1.0900 - val_acc: 0.7800\n",
      "Epoch 102/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9136 - acc: 0.8626 - val_loss: 1.0885 - val_acc: 0.7790\n",
      "Epoch 103/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9098 - acc: 0.8650 - val_loss: 1.0868 - val_acc: 0.7830\n",
      "Epoch 104/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9062 - acc: 0.8640 - val_loss: 1.0847 - val_acc: 0.7840\n",
      "Epoch 105/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9023 - acc: 0.8656 - val_loss: 1.0871 - val_acc: 0.7800\n",
      "Epoch 106/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8987 - acc: 0.8670 - val_loss: 1.0806 - val_acc: 0.7830\n",
      "Epoch 107/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8950 - acc: 0.8661 - val_loss: 1.0769 - val_acc: 0.7880\n",
      "Epoch 108/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8912 - acc: 0.8689 - val_loss: 1.0756 - val_acc: 0.7860\n",
      "Epoch 109/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8877 - acc: 0.8690 - val_loss: 1.0744 - val_acc: 0.7840\n",
      "Epoch 110/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8836 - acc: 0.8707 - val_loss: 1.0773 - val_acc: 0.7820\n",
      "Epoch 111/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8805 - acc: 0.8700 - val_loss: 1.0681 - val_acc: 0.7800\n",
      "Epoch 112/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8769 - acc: 0.8710 - val_loss: 1.0665 - val_acc: 0.7850\n",
      "Epoch 113/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8732 - acc: 0.8720 - val_loss: 1.0640 - val_acc: 0.7880\n",
      "Epoch 114/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8699 - acc: 0.8721 - val_loss: 1.0622 - val_acc: 0.7830\n",
      "Epoch 115/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8663 - acc: 0.8746 - val_loss: 1.0619 - val_acc: 0.7840\n",
      "Epoch 116/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8629 - acc: 0.8754 - val_loss: 1.0593 - val_acc: 0.7880\n",
      "Epoch 117/120\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.8594 - acc: 0.8741 - val_loss: 1.0560 - val_acc: 0.7840\n",
      "Epoch 118/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8563 - acc: 0.8753 - val_loss: 1.0568 - val_acc: 0.7870\n",
      "Epoch 119/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8526 - acc: 0.8743 - val_loss: 1.0527 - val_acc: 0.7860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8491 - acc: 0.8784 - val_loss: 1.0541 - val_acc: 0.7830\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7000/7000 [==============================] - 2s 217us/step - loss: 16.0025 - acc: 0.1680 - val_loss: 15.6266 - val_acc: 0.1720\n",
      "Epoch 2/120\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 15.2983 - acc: 0.1840 - val_loss: 14.9347 - val_acc: 0.1900\n",
      "Epoch 3/120\n",
      "7000/7000 [==============================] - 1s 94us/step - loss: 14.6146 - acc: 0.2036 - val_loss: 14.2611 - val_acc: 0.2100\n",
      "Epoch 4/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 13.9486 - acc: 0.2259 - val_loss: 13.6049 - val_acc: 0.2320\n",
      "Epoch 5/120\n",
      "7000/7000 [==============================] - 1s 94us/step - loss: 13.2994 - acc: 0.2381 - val_loss: 12.9642 - val_acc: 0.2600\n",
      "Epoch 6/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 12.6664 - acc: 0.2551 - val_loss: 12.3400 - val_acc: 0.2710\n",
      "Epoch 7/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 12.0498 - acc: 0.2690 - val_loss: 11.7330 - val_acc: 0.2910\n",
      "Epoch 8/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 11.4492 - acc: 0.2933 - val_loss: 11.1415 - val_acc: 0.3070\n",
      "Epoch 9/120\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 10.8641 - acc: 0.3180 - val_loss: 10.5655 - val_acc: 0.3410\n",
      "Epoch 10/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 10.2948 - acc: 0.3453 - val_loss: 10.0058 - val_acc: 0.3650\n",
      "Epoch 11/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 9.7429 - acc: 0.3670 - val_loss: 9.4645 - val_acc: 0.3910\n",
      "Epoch 12/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 9.2089 - acc: 0.4000 - val_loss: 8.9410 - val_acc: 0.4050\n",
      "Epoch 13/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 8.6947 - acc: 0.4159 - val_loss: 8.4374 - val_acc: 0.4380\n",
      "Epoch 14/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 8.1987 - acc: 0.4409 - val_loss: 7.9524 - val_acc: 0.4580\n",
      "Epoch 15/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 7.7219 - acc: 0.4619 - val_loss: 7.4874 - val_acc: 0.4770\n",
      "Epoch 16/120\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 7.2648 - acc: 0.4753 - val_loss: 7.0407 - val_acc: 0.4950\n",
      "Epoch 17/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 6.8269 - acc: 0.5000 - val_loss: 6.6133 - val_acc: 0.5050\n",
      "Epoch 18/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 6.4087 - acc: 0.5204 - val_loss: 6.2058 - val_acc: 0.5220\n",
      "Epoch 19/120\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 6.0100 - acc: 0.5400 - val_loss: 5.8200 - val_acc: 0.5500\n",
      "Epoch 20/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 5.6319 - acc: 0.5587 - val_loss: 5.4532 - val_acc: 0.5590\n",
      "Epoch 21/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 5.2743 - acc: 0.5787 - val_loss: 5.1078 - val_acc: 0.5880\n",
      "Epoch 22/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 4.9367 - acc: 0.5941 - val_loss: 4.7807 - val_acc: 0.6170\n",
      "Epoch 23/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 4.6193 - acc: 0.6187 - val_loss: 4.4758 - val_acc: 0.6220\n",
      "Epoch 24/120\n",
      "7000/7000 [==============================] - 1s 94us/step - loss: 4.3217 - acc: 0.6310 - val_loss: 4.1902 - val_acc: 0.6190\n",
      "Epoch 25/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 4.0442 - acc: 0.6351 - val_loss: 3.9258 - val_acc: 0.6540\n",
      "Epoch 26/120\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 3.7875 - acc: 0.6559 - val_loss: 3.6785 - val_acc: 0.6500\n",
      "Epoch 27/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 3.5501 - acc: 0.6536 - val_loss: 3.4529 - val_acc: 0.6570\n",
      "Epoch 28/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 3.3326 - acc: 0.6649 - val_loss: 3.2458 - val_acc: 0.6630\n",
      "Epoch 29/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 3.1344 - acc: 0.6730 - val_loss: 3.0592 - val_acc: 0.6680\n",
      "Epoch 30/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.9556 - acc: 0.6774 - val_loss: 2.8914 - val_acc: 0.6550\n",
      "Epoch 31/120\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 2.7946 - acc: 0.6773 - val_loss: 2.7384 - val_acc: 0.6730\n",
      "Epoch 32/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 2.6519 - acc: 0.6801 - val_loss: 2.6065 - val_acc: 0.6760\n",
      "Epoch 33/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 2.5271 - acc: 0.6853 - val_loss: 2.4909 - val_acc: 0.6690\n",
      "Epoch 34/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.4200 - acc: 0.6843 - val_loss: 2.3929 - val_acc: 0.6800\n",
      "Epoch 35/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.3297 - acc: 0.6880 - val_loss: 2.3117 - val_acc: 0.6830\n",
      "Epoch 36/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.2559 - acc: 0.6857 - val_loss: 2.2454 - val_acc: 0.6830\n",
      "Epoch 37/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.1964 - acc: 0.6863 - val_loss: 2.1949 - val_acc: 0.6840\n",
      "Epoch 38/120\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 2.1508 - acc: 0.6930 - val_loss: 2.1580 - val_acc: 0.6770\n",
      "Epoch 39/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 2.1170 - acc: 0.6880 - val_loss: 2.1268 - val_acc: 0.6860\n",
      "Epoch 40/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.0908 - acc: 0.6910 - val_loss: 2.1059 - val_acc: 0.6800\n",
      "Epoch 41/120\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 2.0687 - acc: 0.6926 - val_loss: 2.0846 - val_acc: 0.6770\n",
      "Epoch 42/120\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 2.0484 - acc: 0.6907 - val_loss: 2.0658 - val_acc: 0.6770\n",
      "Epoch 43/120\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 2.0297 - acc: 0.6911 - val_loss: 2.0491 - val_acc: 0.6770\n",
      "Epoch 44/120\n",
      "7000/7000 [==============================] - 1s 94us/step - loss: 2.0123 - acc: 0.6911 - val_loss: 2.0311 - val_acc: 0.6770\n",
      "Epoch 45/120\n",
      "7000/7000 [==============================] - 1s 97us/step - loss: 1.9957 - acc: 0.6933 - val_loss: 2.0145 - val_acc: 0.6870\n",
      "Epoch 46/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.9794 - acc: 0.6967 - val_loss: 1.9995 - val_acc: 0.6820\n",
      "Epoch 47/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.9642 - acc: 0.6966 - val_loss: 1.9871 - val_acc: 0.6790\n",
      "Epoch 48/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.9491 - acc: 0.6950 - val_loss: 1.9698 - val_acc: 0.6830\n",
      "Epoch 49/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.9344 - acc: 0.6961 - val_loss: 1.9575 - val_acc: 0.6790\n",
      "Epoch 50/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.9197 - acc: 0.6966 - val_loss: 1.9433 - val_acc: 0.6880\n",
      "Epoch 51/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.9057 - acc: 0.7003 - val_loss: 1.9290 - val_acc: 0.6890\n",
      "Epoch 52/120\n",
      "7000/7000 [==============================] - 1s 97us/step - loss: 1.8912 - acc: 0.7001 - val_loss: 1.9151 - val_acc: 0.6900\n",
      "Epoch 53/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.8775 - acc: 0.7009 - val_loss: 1.9019 - val_acc: 0.6870\n",
      "Epoch 54/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.8641 - acc: 0.7019 - val_loss: 1.8916 - val_acc: 0.6880\n",
      "Epoch 55/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.8508 - acc: 0.7023 - val_loss: 1.8770 - val_acc: 0.6870\n",
      "Epoch 56/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.8374 - acc: 0.7030 - val_loss: 1.8638 - val_acc: 0.6920\n",
      "Epoch 57/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.8247 - acc: 0.7046 - val_loss: 1.8510 - val_acc: 0.6940\n",
      "Epoch 58/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.8118 - acc: 0.7046 - val_loss: 1.8401 - val_acc: 0.7000\n",
      "Epoch 59/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.7992 - acc: 0.7066 - val_loss: 1.8265 - val_acc: 0.7010\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.7862 - acc: 0.7073 - val_loss: 1.8143 - val_acc: 0.6930\n",
      "Epoch 61/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.7745 - acc: 0.7076 - val_loss: 1.8077 - val_acc: 0.6900\n",
      "Epoch 62/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.7627 - acc: 0.7093 - val_loss: 1.7945 - val_acc: 0.6900\n",
      "Epoch 63/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.7508 - acc: 0.7080 - val_loss: 1.7836 - val_acc: 0.6920\n",
      "Epoch 64/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.7397 - acc: 0.7086 - val_loss: 1.7693 - val_acc: 0.6940\n",
      "Epoch 65/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.7281 - acc: 0.7091 - val_loss: 1.7628 - val_acc: 0.6960\n",
      "Epoch 66/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.7176 - acc: 0.7110 - val_loss: 1.7502 - val_acc: 0.6900\n",
      "Epoch 67/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.7067 - acc: 0.7117 - val_loss: 1.7384 - val_acc: 0.6990\n",
      "Epoch 68/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6958 - acc: 0.7099 - val_loss: 1.7289 - val_acc: 0.7000\n",
      "Epoch 69/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6852 - acc: 0.7104 - val_loss: 1.7182 - val_acc: 0.6960\n",
      "Epoch 70/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6753 - acc: 0.7111 - val_loss: 1.7099 - val_acc: 0.6980\n",
      "Epoch 71/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6656 - acc: 0.7117 - val_loss: 1.7016 - val_acc: 0.7000\n",
      "Epoch 72/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.6556 - acc: 0.7131 - val_loss: 1.6901 - val_acc: 0.6960\n",
      "Epoch 73/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.6459 - acc: 0.7114 - val_loss: 1.6851 - val_acc: 0.6970\n",
      "Epoch 74/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.6373 - acc: 0.7150 - val_loss: 1.6810 - val_acc: 0.6900\n",
      "Epoch 75/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6279 - acc: 0.7144 - val_loss: 1.6656 - val_acc: 0.6990\n",
      "Epoch 76/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.6185 - acc: 0.7157 - val_loss: 1.6547 - val_acc: 0.7040\n",
      "Epoch 77/120\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.6093 - acc: 0.7160 - val_loss: 1.6495 - val_acc: 0.6980\n",
      "Epoch 78/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6011 - acc: 0.7166 - val_loss: 1.6386 - val_acc: 0.7020\n",
      "Epoch 79/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.5928 - acc: 0.7164 - val_loss: 1.6320 - val_acc: 0.7010\n",
      "Epoch 80/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.5845 - acc: 0.7173 - val_loss: 1.6224 - val_acc: 0.7040\n",
      "Epoch 81/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.5764 - acc: 0.7159 - val_loss: 1.6131 - val_acc: 0.7040\n",
      "Epoch 82/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.5684 - acc: 0.7179 - val_loss: 1.6061 - val_acc: 0.7030\n",
      "Epoch 83/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.5609 - acc: 0.7187 - val_loss: 1.5971 - val_acc: 0.7020\n",
      "Epoch 84/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.5527 - acc: 0.7186 - val_loss: 1.5933 - val_acc: 0.7030\n",
      "Epoch 85/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.5450 - acc: 0.7189 - val_loss: 1.5853 - val_acc: 0.7060\n",
      "Epoch 86/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.5376 - acc: 0.7199 - val_loss: 1.5789 - val_acc: 0.7040\n",
      "Epoch 87/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.5303 - acc: 0.7201 - val_loss: 1.5700 - val_acc: 0.7040\n",
      "Epoch 88/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.5225 - acc: 0.7216 - val_loss: 1.5633 - val_acc: 0.7030\n",
      "Epoch 89/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.5161 - acc: 0.7216 - val_loss: 1.5579 - val_acc: 0.7030\n",
      "Epoch 90/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.5087 - acc: 0.7224 - val_loss: 1.5492 - val_acc: 0.7050\n",
      "Epoch 91/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.5017 - acc: 0.7244 - val_loss: 1.5428 - val_acc: 0.7070\n",
      "Epoch 92/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.4954 - acc: 0.7229 - val_loss: 1.5400 - val_acc: 0.7020\n",
      "Epoch 93/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.4882 - acc: 0.7243 - val_loss: 1.5315 - val_acc: 0.7060\n",
      "Epoch 94/120\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.4822 - acc: 0.7241 - val_loss: 1.5229 - val_acc: 0.7070\n",
      "Epoch 95/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.4749 - acc: 0.7231 - val_loss: 1.5160 - val_acc: 0.7090\n",
      "Epoch 96/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.4686 - acc: 0.7241 - val_loss: 1.5103 - val_acc: 0.7070\n",
      "Epoch 97/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.4626 - acc: 0.7249 - val_loss: 1.5033 - val_acc: 0.7080\n",
      "Epoch 98/120\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.4565 - acc: 0.7266 - val_loss: 1.4993 - val_acc: 0.7080\n",
      "Epoch 99/120\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.4499 - acc: 0.7266 - val_loss: 1.4923 - val_acc: 0.7030\n",
      "Epoch 100/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.4438 - acc: 0.7273 - val_loss: 1.4888 - val_acc: 0.7040\n",
      "Epoch 101/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.4376 - acc: 0.7267 - val_loss: 1.4827 - val_acc: 0.7080\n",
      "Epoch 102/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.4321 - acc: 0.7277 - val_loss: 1.4735 - val_acc: 0.7100\n",
      "Epoch 103/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.4255 - acc: 0.7279 - val_loss: 1.4702 - val_acc: 0.7120\n",
      "Epoch 104/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.4201 - acc: 0.7269 - val_loss: 1.4672 - val_acc: 0.7070\n",
      "Epoch 105/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.4147 - acc: 0.7273 - val_loss: 1.4567 - val_acc: 0.7080\n",
      "Epoch 106/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.4090 - acc: 0.7277 - val_loss: 1.4516 - val_acc: 0.7100\n",
      "Epoch 107/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.4035 - acc: 0.7301 - val_loss: 1.4490 - val_acc: 0.7060\n",
      "Epoch 108/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3978 - acc: 0.7293 - val_loss: 1.4472 - val_acc: 0.7010\n",
      "Epoch 109/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3920 - acc: 0.7299 - val_loss: 1.4365 - val_acc: 0.7120\n",
      "Epoch 110/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3867 - acc: 0.7296 - val_loss: 1.4316 - val_acc: 0.7120\n",
      "Epoch 111/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3812 - acc: 0.7290 - val_loss: 1.4256 - val_acc: 0.7120\n",
      "Epoch 112/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3758 - acc: 0.7303 - val_loss: 1.4228 - val_acc: 0.7090\n",
      "Epoch 113/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3708 - acc: 0.7317 - val_loss: 1.4168 - val_acc: 0.7100\n",
      "Epoch 114/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3656 - acc: 0.7310 - val_loss: 1.4116 - val_acc: 0.7110\n",
      "Epoch 115/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.3606 - acc: 0.7307 - val_loss: 1.4070 - val_acc: 0.7110\n",
      "Epoch 116/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.3554 - acc: 0.7313 - val_loss: 1.4015 - val_acc: 0.7100\n",
      "Epoch 117/120\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.3505 - acc: 0.7309 - val_loss: 1.3959 - val_acc: 0.7140\n",
      "Epoch 118/120\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3455 - acc: 0.7313 - val_loss: 1.3946 - val_acc: 0.7100\n",
      "Epoch 119/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.3404 - acc: 0.7309 - val_loss: 1.3868 - val_acc: 0.7110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.3359 - acc: 0.7323 - val_loss: 1.3818 - val_acc: 0.7130\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7000/7000 [==============================] - 1s 188us/step - loss: 16.0291 - acc: 0.1589 - val_loss: 15.6582 - val_acc: 0.1510\n",
      "Epoch 2/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 15.3169 - acc: 0.1614 - val_loss: 14.9607 - val_acc: 0.1520\n",
      "Epoch 3/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 14.6302 - acc: 0.1723 - val_loss: 14.2841 - val_acc: 0.1640\n",
      "Epoch 4/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 13.9633 - acc: 0.1850 - val_loss: 13.6270 - val_acc: 0.1740\n",
      "Epoch 5/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 13.3153 - acc: 0.2020 - val_loss: 12.9885 - val_acc: 0.1850\n",
      "Epoch 6/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 12.6854 - acc: 0.2109 - val_loss: 12.3680 - val_acc: 0.1990\n",
      "Epoch 7/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 12.0733 - acc: 0.2313 - val_loss: 11.7657 - val_acc: 0.2180\n",
      "Epoch 8/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 11.4787 - acc: 0.2459 - val_loss: 11.1802 - val_acc: 0.2420\n",
      "Epoch 9/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 10.9008 - acc: 0.2637 - val_loss: 10.6114 - val_acc: 0.2610\n",
      "Epoch 10/1000\n",
      "7000/7000 [==============================] - 1s 99us/step - loss: 10.3392 - acc: 0.2836 - val_loss: 10.0586 - val_acc: 0.2790\n",
      "Epoch 11/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 9.7940 - acc: 0.3060 - val_loss: 9.5233 - val_acc: 0.3120\n",
      "Epoch 12/1000\n",
      "7000/7000 [==============================] - 1s 99us/step - loss: 9.2661 - acc: 0.3281 - val_loss: 9.0054 - val_acc: 0.3380\n",
      "Epoch 13/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 8.7561 - acc: 0.3590 - val_loss: 8.5062 - val_acc: 0.3660\n",
      "Epoch 14/1000\n",
      "7000/7000 [==============================] - 1s 96us/step - loss: 8.2642 - acc: 0.3929 - val_loss: 8.0235 - val_acc: 0.4020\n",
      "Epoch 15/1000\n",
      "7000/7000 [==============================] - 1s 98us/step - loss: 7.7904 - acc: 0.4286 - val_loss: 7.5603 - val_acc: 0.4310\n",
      "Epoch 16/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 7.3348 - acc: 0.4680 - val_loss: 7.1144 - val_acc: 0.4660\n",
      "Epoch 17/1000\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 6.8973 - acc: 0.5021 - val_loss: 6.6866 - val_acc: 0.4910\n",
      "Epoch 18/1000\n",
      "7000/7000 [==============================] - 1s 111us/step - loss: 6.4776 - acc: 0.5324 - val_loss: 6.2784 - val_acc: 0.5580\n",
      "Epoch 19/1000\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 6.0771 - acc: 0.5721 - val_loss: 5.8874 - val_acc: 0.5690\n",
      "Epoch 20/1000\n",
      "7000/7000 [==============================] - 1s 98us/step - loss: 5.6953 - acc: 0.6001 - val_loss: 5.5163 - val_acc: 0.5840\n",
      "Epoch 21/1000\n",
      "7000/7000 [==============================] - 1s 97us/step - loss: 5.3322 - acc: 0.6190 - val_loss: 5.1629 - val_acc: 0.6160\n",
      "Epoch 22/1000\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 4.9877 - acc: 0.6409 - val_loss: 4.8299 - val_acc: 0.6260\n",
      "Epoch 23/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 4.6631 - acc: 0.6526 - val_loss: 4.5155 - val_acc: 0.6420\n",
      "Epoch 24/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 4.3584 - acc: 0.6653 - val_loss: 4.2214 - val_acc: 0.6640\n",
      "Epoch 25/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 4.0736 - acc: 0.6793 - val_loss: 3.9468 - val_acc: 0.6680\n",
      "Epoch 26/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 3.8092 - acc: 0.6813 - val_loss: 3.6944 - val_acc: 0.6680\n",
      "Epoch 27/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 3.5647 - acc: 0.6871 - val_loss: 3.4599 - val_acc: 0.6750\n",
      "Epoch 28/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 3.3396 - acc: 0.6907 - val_loss: 3.2453 - val_acc: 0.6740\n",
      "Epoch 29/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 3.1341 - acc: 0.6943 - val_loss: 3.0487 - val_acc: 0.6730\n",
      "Epoch 30/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.9473 - acc: 0.6901 - val_loss: 2.8748 - val_acc: 0.6780\n",
      "Epoch 31/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.7797 - acc: 0.6941 - val_loss: 2.7169 - val_acc: 0.6770\n",
      "Epoch 32/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 2.6301 - acc: 0.6941 - val_loss: 2.5761 - val_acc: 0.6730\n",
      "Epoch 33/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 2.4993 - acc: 0.6919 - val_loss: 2.4565 - val_acc: 0.6770\n",
      "Epoch 34/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 2.3864 - acc: 0.6934 - val_loss: 2.3519 - val_acc: 0.6820\n",
      "Epoch 35/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 2.2909 - acc: 0.6924 - val_loss: 2.2656 - val_acc: 0.6760\n",
      "Epoch 36/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 2.2119 - acc: 0.6933 - val_loss: 2.1937 - val_acc: 0.6810\n",
      "Epoch 37/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.1481 - acc: 0.6891 - val_loss: 2.1401 - val_acc: 0.6830\n",
      "Epoch 38/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.0989 - acc: 0.6913 - val_loss: 2.1006 - val_acc: 0.6830\n",
      "Epoch 39/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.0619 - acc: 0.6934 - val_loss: 2.0634 - val_acc: 0.6850\n",
      "Epoch 40/1000\n",
      "7000/7000 [==============================] - 1s 97us/step - loss: 2.0318 - acc: 0.6943 - val_loss: 2.0369 - val_acc: 0.6840\n",
      "Epoch 41/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 2.0066 - acc: 0.6936 - val_loss: 2.0145 - val_acc: 0.6890\n",
      "Epoch 42/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.9843 - acc: 0.6954 - val_loss: 1.9936 - val_acc: 0.6880\n",
      "Epoch 43/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.9635 - acc: 0.6960 - val_loss: 1.9737 - val_acc: 0.6850\n",
      "Epoch 44/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.9447 - acc: 0.6950 - val_loss: 1.9566 - val_acc: 0.6880\n",
      "Epoch 45/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 1.9270 - acc: 0.6986 - val_loss: 1.9388 - val_acc: 0.6900\n",
      "Epoch 46/1000\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 1.9097 - acc: 0.6979 - val_loss: 1.9255 - val_acc: 0.6920\n",
      "Epoch 47/1000\n",
      "7000/7000 [==============================] - 1s 94us/step - loss: 1.8936 - acc: 0.6974 - val_loss: 1.9107 - val_acc: 0.6960\n",
      "Epoch 48/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.8783 - acc: 0.6999 - val_loss: 1.8962 - val_acc: 0.6910\n",
      "Epoch 49/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.8633 - acc: 0.7007 - val_loss: 1.8811 - val_acc: 0.6940\n",
      "Epoch 50/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.8498 - acc: 0.7017 - val_loss: 1.8663 - val_acc: 0.6980\n",
      "Epoch 51/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.8356 - acc: 0.7021 - val_loss: 1.8527 - val_acc: 0.6930\n",
      "Epoch 52/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.8223 - acc: 0.7044 - val_loss: 1.8414 - val_acc: 0.6930\n",
      "Epoch 53/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.8096 - acc: 0.7037 - val_loss: 1.8284 - val_acc: 0.6950\n",
      "Epoch 54/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.7971 - acc: 0.7053 - val_loss: 1.8149 - val_acc: 0.6970\n",
      "Epoch 55/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.7851 - acc: 0.7071 - val_loss: 1.8048 - val_acc: 0.6970\n",
      "Epoch 56/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.7733 - acc: 0.7060 - val_loss: 1.7959 - val_acc: 0.7020\n",
      "Epoch 57/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.7620 - acc: 0.7084 - val_loss: 1.7829 - val_acc: 0.6990\n",
      "Epoch 58/1000\n",
      "7000/7000 [==============================] - 1s 102us/step - loss: 1.7511 - acc: 0.7066 - val_loss: 1.7715 - val_acc: 0.7020\n",
      "Epoch 59/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 1.7407 - acc: 0.7076 - val_loss: 1.7601 - val_acc: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.7299 - acc: 0.7091 - val_loss: 1.7520 - val_acc: 0.7030\n",
      "Epoch 61/1000\n",
      "7000/7000 [==============================] - 1s 102us/step - loss: 1.7195 - acc: 0.7077 - val_loss: 1.7413 - val_acc: 0.7040\n",
      "Epoch 62/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.7096 - acc: 0.7086 - val_loss: 1.7343 - val_acc: 0.7050\n",
      "Epoch 63/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.6999 - acc: 0.7100 - val_loss: 1.7203 - val_acc: 0.7020\n",
      "Epoch 64/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.6905 - acc: 0.7114 - val_loss: 1.7142 - val_acc: 0.7010\n",
      "Epoch 65/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6810 - acc: 0.7109 - val_loss: 1.7065 - val_acc: 0.7050\n",
      "Epoch 66/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6717 - acc: 0.7110 - val_loss: 1.6949 - val_acc: 0.7080\n",
      "Epoch 67/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.6625 - acc: 0.7133 - val_loss: 1.6846 - val_acc: 0.7060\n",
      "Epoch 68/1000\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.6539 - acc: 0.7130 - val_loss: 1.6822 - val_acc: 0.7040\n",
      "Epoch 69/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6456 - acc: 0.7131 - val_loss: 1.6714 - val_acc: 0.7060\n",
      "Epoch 70/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.6367 - acc: 0.7143 - val_loss: 1.6637 - val_acc: 0.7080\n",
      "Epoch 71/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6289 - acc: 0.7131 - val_loss: 1.6519 - val_acc: 0.7080\n",
      "Epoch 72/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6200 - acc: 0.7137 - val_loss: 1.6448 - val_acc: 0.7080\n",
      "Epoch 73/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.6123 - acc: 0.7154 - val_loss: 1.6371 - val_acc: 0.7070\n",
      "Epoch 74/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.6049 - acc: 0.7160 - val_loss: 1.6306 - val_acc: 0.7100\n",
      "Epoch 75/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.5970 - acc: 0.7157 - val_loss: 1.6235 - val_acc: 0.7040\n",
      "Epoch 76/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.5894 - acc: 0.7166 - val_loss: 1.6136 - val_acc: 0.7090\n",
      "Epoch 77/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.5818 - acc: 0.7173 - val_loss: 1.6086 - val_acc: 0.7090\n",
      "Epoch 78/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.5746 - acc: 0.7173 - val_loss: 1.6011 - val_acc: 0.7040\n",
      "Epoch 79/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.5675 - acc: 0.7171 - val_loss: 1.5953 - val_acc: 0.7000\n",
      "Epoch 80/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.5606 - acc: 0.7164 - val_loss: 1.5878 - val_acc: 0.7100\n",
      "Epoch 81/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.5533 - acc: 0.7181 - val_loss: 1.5806 - val_acc: 0.7120\n",
      "Epoch 82/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 1.5462 - acc: 0.7180 - val_loss: 1.5717 - val_acc: 0.7110\n",
      "Epoch 83/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.5396 - acc: 0.7190 - val_loss: 1.5652 - val_acc: 0.7130\n",
      "Epoch 84/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.5334 - acc: 0.7193 - val_loss: 1.5585 - val_acc: 0.7070\n",
      "Epoch 85/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.5261 - acc: 0.7199 - val_loss: 1.5531 - val_acc: 0.7110\n",
      "Epoch 86/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.5195 - acc: 0.7209 - val_loss: 1.5457 - val_acc: 0.7150\n",
      "Epoch 87/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.5135 - acc: 0.7203 - val_loss: 1.5394 - val_acc: 0.7140\n",
      "Epoch 88/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.5067 - acc: 0.7206 - val_loss: 1.5347 - val_acc: 0.7150\n",
      "Epoch 89/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.5004 - acc: 0.7197 - val_loss: 1.5303 - val_acc: 0.7110\n",
      "Epoch 90/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.4938 - acc: 0.7210 - val_loss: 1.5260 - val_acc: 0.7110\n",
      "Epoch 91/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.4881 - acc: 0.7203 - val_loss: 1.5166 - val_acc: 0.7150\n",
      "Epoch 92/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.4811 - acc: 0.7224 - val_loss: 1.5091 - val_acc: 0.7130\n",
      "Epoch 93/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.4750 - acc: 0.7234 - val_loss: 1.5023 - val_acc: 0.7140\n",
      "Epoch 94/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.4690 - acc: 0.7224 - val_loss: 1.4992 - val_acc: 0.7120\n",
      "Epoch 95/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 1.4634 - acc: 0.7239 - val_loss: 1.4908 - val_acc: 0.7170\n",
      "Epoch 96/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.4574 - acc: 0.7239 - val_loss: 1.4841 - val_acc: 0.7160\n",
      "Epoch 97/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.4510 - acc: 0.7229 - val_loss: 1.4792 - val_acc: 0.7140\n",
      "Epoch 98/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.4453 - acc: 0.7256 - val_loss: 1.4750 - val_acc: 0.7130\n",
      "Epoch 99/1000\n",
      "7000/7000 [==============================] - 1s 100us/step - loss: 1.4396 - acc: 0.7240 - val_loss: 1.4682 - val_acc: 0.7100\n",
      "Epoch 100/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.4344 - acc: 0.7250 - val_loss: 1.4623 - val_acc: 0.7130\n",
      "Epoch 101/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.4292 - acc: 0.7241 - val_loss: 1.4575 - val_acc: 0.7140\n",
      "Epoch 102/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.4231 - acc: 0.7250 - val_loss: 1.4520 - val_acc: 0.7200\n",
      "Epoch 103/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.4174 - acc: 0.7247 - val_loss: 1.4496 - val_acc: 0.7170\n",
      "Epoch 104/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.4121 - acc: 0.7256 - val_loss: 1.4402 - val_acc: 0.7200\n",
      "Epoch 105/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.4066 - acc: 0.7273 - val_loss: 1.4376 - val_acc: 0.7200\n",
      "Epoch 106/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.4014 - acc: 0.7270 - val_loss: 1.4314 - val_acc: 0.7180\n",
      "Epoch 107/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.3962 - acc: 0.7284 - val_loss: 1.4266 - val_acc: 0.7200\n",
      "Epoch 108/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.3911 - acc: 0.7283 - val_loss: 1.4246 - val_acc: 0.7190\n",
      "Epoch 109/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.3858 - acc: 0.7286 - val_loss: 1.4179 - val_acc: 0.7140\n",
      "Epoch 110/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.3809 - acc: 0.7287 - val_loss: 1.4111 - val_acc: 0.7200\n",
      "Epoch 111/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3757 - acc: 0.7303 - val_loss: 1.4095 - val_acc: 0.7170\n",
      "Epoch 112/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.3707 - acc: 0.7294 - val_loss: 1.3992 - val_acc: 0.7220\n",
      "Epoch 113/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 1.3656 - acc: 0.7294 - val_loss: 1.4024 - val_acc: 0.7170\n",
      "Epoch 114/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3608 - acc: 0.7301 - val_loss: 1.3931 - val_acc: 0.7230\n",
      "Epoch 115/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.3566 - acc: 0.7296 - val_loss: 1.3886 - val_acc: 0.7210\n",
      "Epoch 116/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.3514 - acc: 0.7311 - val_loss: 1.3853 - val_acc: 0.7190\n",
      "Epoch 117/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3469 - acc: 0.7320 - val_loss: 1.3773 - val_acc: 0.7210\n",
      "Epoch 118/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.3420 - acc: 0.7321 - val_loss: 1.3777 - val_acc: 0.7210\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.3370 - acc: 0.7321 - val_loss: 1.3728 - val_acc: 0.7190\n",
      "Epoch 120/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3325 - acc: 0.7340 - val_loss: 1.3658 - val_acc: 0.7220\n",
      "Epoch 121/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.3279 - acc: 0.7320 - val_loss: 1.3608 - val_acc: 0.7230\n",
      "Epoch 122/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.3232 - acc: 0.7320 - val_loss: 1.3551 - val_acc: 0.7240\n",
      "Epoch 123/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3183 - acc: 0.7339 - val_loss: 1.3513 - val_acc: 0.7230\n",
      "Epoch 124/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3136 - acc: 0.7340 - val_loss: 1.3490 - val_acc: 0.7220\n",
      "Epoch 125/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.3098 - acc: 0.7339 - val_loss: 1.3442 - val_acc: 0.7210\n",
      "Epoch 126/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.3050 - acc: 0.7343 - val_loss: 1.3403 - val_acc: 0.7260\n",
      "Epoch 127/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.3009 - acc: 0.7339 - val_loss: 1.3345 - val_acc: 0.7230\n",
      "Epoch 128/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.2970 - acc: 0.7343 - val_loss: 1.3296 - val_acc: 0.7250\n",
      "Epoch 129/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.2922 - acc: 0.7349 - val_loss: 1.3278 - val_acc: 0.7230\n",
      "Epoch 130/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.2880 - acc: 0.7357 - val_loss: 1.3221 - val_acc: 0.7220\n",
      "Epoch 131/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.2840 - acc: 0.7366 - val_loss: 1.3173 - val_acc: 0.7270\n",
      "Epoch 132/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.2793 - acc: 0.7364 - val_loss: 1.3168 - val_acc: 0.7270\n",
      "Epoch 133/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.2763 - acc: 0.7363 - val_loss: 1.3115 - val_acc: 0.7280\n",
      "Epoch 134/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.2716 - acc: 0.7367 - val_loss: 1.3055 - val_acc: 0.7240\n",
      "Epoch 135/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.2675 - acc: 0.7361 - val_loss: 1.3023 - val_acc: 0.7290\n",
      "Epoch 136/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.2634 - acc: 0.7357 - val_loss: 1.2993 - val_acc: 0.7260\n",
      "Epoch 137/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.2599 - acc: 0.7376 - val_loss: 1.2928 - val_acc: 0.7280\n",
      "Epoch 138/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.2554 - acc: 0.7384 - val_loss: 1.2956 - val_acc: 0.7280\n",
      "Epoch 139/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.2520 - acc: 0.7353 - val_loss: 1.2894 - val_acc: 0.7280\n",
      "Epoch 140/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.2479 - acc: 0.7374 - val_loss: 1.2845 - val_acc: 0.7310\n",
      "Epoch 141/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.2441 - acc: 0.7400 - val_loss: 1.2832 - val_acc: 0.7310\n",
      "Epoch 142/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.2404 - acc: 0.7393 - val_loss: 1.2812 - val_acc: 0.7240\n",
      "Epoch 143/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.2372 - acc: 0.7387 - val_loss: 1.2757 - val_acc: 0.7300\n",
      "Epoch 144/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.2335 - acc: 0.7379 - val_loss: 1.2686 - val_acc: 0.7300\n",
      "Epoch 145/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.2299 - acc: 0.7404 - val_loss: 1.2651 - val_acc: 0.7310\n",
      "Epoch 146/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.2266 - acc: 0.7391 - val_loss: 1.2653 - val_acc: 0.7290\n",
      "Epoch 147/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.2228 - acc: 0.7401 - val_loss: 1.2662 - val_acc: 0.7310\n",
      "Epoch 148/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.2194 - acc: 0.7416 - val_loss: 1.2566 - val_acc: 0.7300\n",
      "Epoch 149/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.2159 - acc: 0.7406 - val_loss: 1.2520 - val_acc: 0.7300\n",
      "Epoch 150/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.2123 - acc: 0.7407 - val_loss: 1.2499 - val_acc: 0.7260\n",
      "Epoch 151/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.2092 - acc: 0.7404 - val_loss: 1.2474 - val_acc: 0.7320\n",
      "Epoch 152/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.2055 - acc: 0.7417 - val_loss: 1.2420 - val_acc: 0.7340\n",
      "Epoch 153/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.2025 - acc: 0.7407 - val_loss: 1.2396 - val_acc: 0.7300\n",
      "Epoch 154/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1995 - acc: 0.7403 - val_loss: 1.2371 - val_acc: 0.7310\n",
      "Epoch 155/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.1959 - acc: 0.7420 - val_loss: 1.2351 - val_acc: 0.7310\n",
      "Epoch 156/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1930 - acc: 0.7419 - val_loss: 1.2312 - val_acc: 0.7300\n",
      "Epoch 157/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1900 - acc: 0.7427 - val_loss: 1.2259 - val_acc: 0.7300\n",
      "Epoch 158/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.1868 - acc: 0.7427 - val_loss: 1.2253 - val_acc: 0.7310\n",
      "Epoch 159/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.1838 - acc: 0.7421 - val_loss: 1.2239 - val_acc: 0.7330\n",
      "Epoch 160/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1811 - acc: 0.7429 - val_loss: 1.2202 - val_acc: 0.7320\n",
      "Epoch 161/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.1784 - acc: 0.7427 - val_loss: 1.2156 - val_acc: 0.7340\n",
      "Epoch 162/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1755 - acc: 0.7439 - val_loss: 1.2170 - val_acc: 0.7250\n",
      "Epoch 163/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.1729 - acc: 0.7419 - val_loss: 1.2145 - val_acc: 0.7330\n",
      "Epoch 164/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.1700 - acc: 0.7464 - val_loss: 1.2078 - val_acc: 0.7330\n",
      "Epoch 165/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 1.1672 - acc: 0.7441 - val_loss: 1.2060 - val_acc: 0.7340\n",
      "Epoch 166/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.1648 - acc: 0.7430 - val_loss: 1.2167 - val_acc: 0.7300\n",
      "Epoch 167/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.1612 - acc: 0.7447 - val_loss: 1.2040 - val_acc: 0.7370\n",
      "Epoch 168/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.1588 - acc: 0.7433 - val_loss: 1.1999 - val_acc: 0.7330\n",
      "Epoch 169/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 1.1564 - acc: 0.7457 - val_loss: 1.1958 - val_acc: 0.7340\n",
      "Epoch 170/1000\n",
      "7000/7000 [==============================] - 1s 100us/step - loss: 1.1534 - acc: 0.7453 - val_loss: 1.1950 - val_acc: 0.7330\n",
      "Epoch 171/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.1510 - acc: 0.7484 - val_loss: 1.1937 - val_acc: 0.7350\n",
      "Epoch 172/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1488 - acc: 0.7459 - val_loss: 1.1897 - val_acc: 0.7360\n",
      "Epoch 173/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.1461 - acc: 0.7477 - val_loss: 1.1863 - val_acc: 0.7370\n",
      "Epoch 174/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1435 - acc: 0.7481 - val_loss: 1.1841 - val_acc: 0.7350\n",
      "Epoch 175/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.1403 - acc: 0.7473 - val_loss: 1.1845 - val_acc: 0.7390\n",
      "Epoch 176/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1388 - acc: 0.7476 - val_loss: 1.1783 - val_acc: 0.7330\n",
      "Epoch 177/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.1359 - acc: 0.7487 - val_loss: 1.1774 - val_acc: 0.7360\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.1336 - acc: 0.7481 - val_loss: 1.1746 - val_acc: 0.7340\n",
      "Epoch 179/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1314 - acc: 0.7481 - val_loss: 1.1736 - val_acc: 0.7390\n",
      "Epoch 180/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.1289 - acc: 0.7464 - val_loss: 1.1726 - val_acc: 0.7360\n",
      "Epoch 181/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1270 - acc: 0.7469 - val_loss: 1.1719 - val_acc: 0.7400\n",
      "Epoch 182/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.1248 - acc: 0.7491 - val_loss: 1.1652 - val_acc: 0.7360\n",
      "Epoch 183/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.1230 - acc: 0.7494 - val_loss: 1.1647 - val_acc: 0.7400\n",
      "Epoch 184/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.1204 - acc: 0.7501 - val_loss: 1.1640 - val_acc: 0.7400\n",
      "Epoch 185/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1184 - acc: 0.7493 - val_loss: 1.1598 - val_acc: 0.7360\n",
      "Epoch 186/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1162 - acc: 0.7484 - val_loss: 1.1640 - val_acc: 0.7370\n",
      "Epoch 187/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.1146 - acc: 0.7469 - val_loss: 1.1594 - val_acc: 0.7430\n",
      "Epoch 188/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.1122 - acc: 0.7504 - val_loss: 1.1564 - val_acc: 0.7380\n",
      "Epoch 189/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1098 - acc: 0.7493 - val_loss: 1.1535 - val_acc: 0.7360\n",
      "Epoch 190/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1084 - acc: 0.7509 - val_loss: 1.1551 - val_acc: 0.7380\n",
      "Epoch 191/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.1064 - acc: 0.7511 - val_loss: 1.1514 - val_acc: 0.7380\n",
      "Epoch 192/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1045 - acc: 0.7504 - val_loss: 1.1477 - val_acc: 0.7360\n",
      "Epoch 193/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.1033 - acc: 0.7517 - val_loss: 1.1491 - val_acc: 0.7380\n",
      "Epoch 194/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.1010 - acc: 0.7519 - val_loss: 1.1452 - val_acc: 0.7410\n",
      "Epoch 195/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0992 - acc: 0.7504 - val_loss: 1.1436 - val_acc: 0.7400\n",
      "Epoch 196/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0975 - acc: 0.7506 - val_loss: 1.1449 - val_acc: 0.7350\n",
      "Epoch 197/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0956 - acc: 0.7510 - val_loss: 1.1430 - val_acc: 0.7350\n",
      "Epoch 198/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0942 - acc: 0.7517 - val_loss: 1.1373 - val_acc: 0.7410\n",
      "Epoch 199/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0924 - acc: 0.7517 - val_loss: 1.1401 - val_acc: 0.7380\n",
      "Epoch 200/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0909 - acc: 0.7510 - val_loss: 1.1453 - val_acc: 0.7460\n",
      "Epoch 201/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0898 - acc: 0.7544 - val_loss: 1.1342 - val_acc: 0.7380\n",
      "Epoch 202/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0874 - acc: 0.7527 - val_loss: 1.1332 - val_acc: 0.7410\n",
      "Epoch 203/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.0860 - acc: 0.7520 - val_loss: 1.1297 - val_acc: 0.7400\n",
      "Epoch 204/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0835 - acc: 0.7539 - val_loss: 1.1336 - val_acc: 0.7430\n",
      "Epoch 205/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0834 - acc: 0.7514 - val_loss: 1.1306 - val_acc: 0.7420\n",
      "Epoch 206/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0814 - acc: 0.7544 - val_loss: 1.1266 - val_acc: 0.7400\n",
      "Epoch 207/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0800 - acc: 0.7544 - val_loss: 1.1254 - val_acc: 0.7390\n",
      "Epoch 208/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0782 - acc: 0.7541 - val_loss: 1.1272 - val_acc: 0.7420\n",
      "Epoch 209/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0767 - acc: 0.7560 - val_loss: 1.1251 - val_acc: 0.7420\n",
      "Epoch 210/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0755 - acc: 0.7543 - val_loss: 1.1254 - val_acc: 0.7370\n",
      "Epoch 211/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0740 - acc: 0.7561 - val_loss: 1.1196 - val_acc: 0.7410\n",
      "Epoch 212/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0726 - acc: 0.7550 - val_loss: 1.1185 - val_acc: 0.7420\n",
      "Epoch 213/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.0712 - acc: 0.7554 - val_loss: 1.1178 - val_acc: 0.7440\n",
      "Epoch 214/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0696 - acc: 0.7553 - val_loss: 1.1161 - val_acc: 0.7410\n",
      "Epoch 215/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0689 - acc: 0.7541 - val_loss: 1.1199 - val_acc: 0.7400\n",
      "Epoch 216/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0677 - acc: 0.7559 - val_loss: 1.1143 - val_acc: 0.7410\n",
      "Epoch 217/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0659 - acc: 0.7550 - val_loss: 1.1131 - val_acc: 0.7440\n",
      "Epoch 218/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0645 - acc: 0.7570 - val_loss: 1.1114 - val_acc: 0.7420\n",
      "Epoch 219/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0630 - acc: 0.7557 - val_loss: 1.1134 - val_acc: 0.7440\n",
      "Epoch 220/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0619 - acc: 0.7567 - val_loss: 1.1109 - val_acc: 0.7420\n",
      "Epoch 221/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0610 - acc: 0.7553 - val_loss: 1.1076 - val_acc: 0.7420\n",
      "Epoch 222/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.0601 - acc: 0.7570 - val_loss: 1.1091 - val_acc: 0.7420\n",
      "Epoch 223/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0583 - acc: 0.7574 - val_loss: 1.1066 - val_acc: 0.7410\n",
      "Epoch 224/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0573 - acc: 0.7563 - val_loss: 1.1036 - val_acc: 0.7440\n",
      "Epoch 225/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0561 - acc: 0.7563 - val_loss: 1.1067 - val_acc: 0.7410\n",
      "Epoch 226/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0549 - acc: 0.7564 - val_loss: 1.1021 - val_acc: 0.7410\n",
      "Epoch 227/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0534 - acc: 0.7573 - val_loss: 1.1036 - val_acc: 0.7360\n",
      "Epoch 228/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0524 - acc: 0.7573 - val_loss: 1.1064 - val_acc: 0.7340\n",
      "Epoch 229/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0519 - acc: 0.7591 - val_loss: 1.0998 - val_acc: 0.7370\n",
      "Epoch 230/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0504 - acc: 0.7587 - val_loss: 1.1027 - val_acc: 0.7400\n",
      "Epoch 231/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0492 - acc: 0.7590 - val_loss: 1.1005 - val_acc: 0.7400\n",
      "Epoch 232/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0478 - acc: 0.7571 - val_loss: 1.0995 - val_acc: 0.7390\n",
      "Epoch 233/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0468 - acc: 0.7593 - val_loss: 1.1089 - val_acc: 0.7330\n",
      "Epoch 234/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0459 - acc: 0.7581 - val_loss: 1.0987 - val_acc: 0.7410\n",
      "Epoch 235/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0442 - acc: 0.7590 - val_loss: 1.0924 - val_acc: 0.7420\n",
      "Epoch 236/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0432 - acc: 0.7591 - val_loss: 1.0922 - val_acc: 0.7380\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0425 - acc: 0.7576 - val_loss: 1.0903 - val_acc: 0.7420\n",
      "Epoch 238/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 1.0410 - acc: 0.7597 - val_loss: 1.0903 - val_acc: 0.7400\n",
      "Epoch 239/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.0401 - acc: 0.7579 - val_loss: 1.0939 - val_acc: 0.7440\n",
      "Epoch 240/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0393 - acc: 0.7594 - val_loss: 1.0872 - val_acc: 0.7400\n",
      "Epoch 241/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0379 - acc: 0.7597 - val_loss: 1.0878 - val_acc: 0.7430\n",
      "Epoch 242/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0370 - acc: 0.7601 - val_loss: 1.0874 - val_acc: 0.7410\n",
      "Epoch 243/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0355 - acc: 0.7599 - val_loss: 1.0901 - val_acc: 0.7390\n",
      "Epoch 244/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0357 - acc: 0.7597 - val_loss: 1.0865 - val_acc: 0.7430\n",
      "Epoch 245/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0350 - acc: 0.7606 - val_loss: 1.0835 - val_acc: 0.7400\n",
      "Epoch 246/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0329 - acc: 0.7600 - val_loss: 1.0834 - val_acc: 0.7440\n",
      "Epoch 247/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0320 - acc: 0.7621 - val_loss: 1.0889 - val_acc: 0.7400\n",
      "Epoch 248/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0314 - acc: 0.7613 - val_loss: 1.0810 - val_acc: 0.7420\n",
      "Epoch 249/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0305 - acc: 0.7607 - val_loss: 1.0796 - val_acc: 0.7430\n",
      "Epoch 250/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0296 - acc: 0.7614 - val_loss: 1.0860 - val_acc: 0.7360\n",
      "Epoch 251/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0288 - acc: 0.7597 - val_loss: 1.0791 - val_acc: 0.7440\n",
      "Epoch 252/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0274 - acc: 0.7603 - val_loss: 1.0772 - val_acc: 0.7410\n",
      "Epoch 253/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0267 - acc: 0.7601 - val_loss: 1.0755 - val_acc: 0.7400\n",
      "Epoch 254/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0258 - acc: 0.7599 - val_loss: 1.0821 - val_acc: 0.7450\n",
      "Epoch 255/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0254 - acc: 0.7607 - val_loss: 1.0767 - val_acc: 0.7450\n",
      "Epoch 256/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0241 - acc: 0.7636 - val_loss: 1.0818 - val_acc: 0.7350\n",
      "Epoch 257/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0240 - acc: 0.7633 - val_loss: 1.0780 - val_acc: 0.7390\n",
      "Epoch 258/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0225 - acc: 0.7616 - val_loss: 1.0794 - val_acc: 0.7390\n",
      "Epoch 259/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0210 - acc: 0.7601 - val_loss: 1.0745 - val_acc: 0.7390\n",
      "Epoch 260/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0197 - acc: 0.7616 - val_loss: 1.0765 - val_acc: 0.7450\n",
      "Epoch 261/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0196 - acc: 0.7620 - val_loss: 1.0713 - val_acc: 0.7420\n",
      "Epoch 262/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.0195 - acc: 0.7616 - val_loss: 1.0758 - val_acc: 0.7340\n",
      "Epoch 263/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0179 - acc: 0.7624 - val_loss: 1.0709 - val_acc: 0.7410\n",
      "Epoch 264/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.0165 - acc: 0.7627 - val_loss: 1.0711 - val_acc: 0.7390\n",
      "Epoch 265/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 1.0159 - acc: 0.7624 - val_loss: 1.0749 - val_acc: 0.7400\n",
      "Epoch 266/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0151 - acc: 0.7614 - val_loss: 1.0772 - val_acc: 0.7380\n",
      "Epoch 267/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0143 - acc: 0.7611 - val_loss: 1.0712 - val_acc: 0.7380\n",
      "Epoch 268/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0136 - acc: 0.7636 - val_loss: 1.0698 - val_acc: 0.7410\n",
      "Epoch 269/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0132 - acc: 0.7626 - val_loss: 1.0644 - val_acc: 0.7460\n",
      "Epoch 270/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0116 - acc: 0.7646 - val_loss: 1.0679 - val_acc: 0.7400\n",
      "Epoch 271/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0110 - acc: 0.7639 - val_loss: 1.0664 - val_acc: 0.7400\n",
      "Epoch 272/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0101 - acc: 0.7631 - val_loss: 1.0701 - val_acc: 0.7360\n",
      "Epoch 273/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0095 - acc: 0.7626 - val_loss: 1.0648 - val_acc: 0.7430\n",
      "Epoch 274/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0083 - acc: 0.7627 - val_loss: 1.0655 - val_acc: 0.7420\n",
      "Epoch 275/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0083 - acc: 0.7657 - val_loss: 1.0644 - val_acc: 0.7400\n",
      "Epoch 276/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0074 - acc: 0.7636 - val_loss: 1.0651 - val_acc: 0.7440\n",
      "Epoch 277/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0057 - acc: 0.7659 - val_loss: 1.0629 - val_acc: 0.7420\n",
      "Epoch 278/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0056 - acc: 0.7641 - val_loss: 1.0637 - val_acc: 0.7430\n",
      "Epoch 279/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0047 - acc: 0.7639 - val_loss: 1.0621 - val_acc: 0.7430\n",
      "Epoch 280/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.0036 - acc: 0.7653 - val_loss: 1.0599 - val_acc: 0.7420\n",
      "Epoch 281/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 1.0033 - acc: 0.7643 - val_loss: 1.0573 - val_acc: 0.7410\n",
      "Epoch 282/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0021 - acc: 0.7639 - val_loss: 1.0634 - val_acc: 0.7410\n",
      "Epoch 283/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 1.0017 - acc: 0.7649 - val_loss: 1.0599 - val_acc: 0.7420\n",
      "Epoch 284/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 1.0009 - acc: 0.7636 - val_loss: 1.0604 - val_acc: 0.7390\n",
      "Epoch 285/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 1.0009 - acc: 0.7641 - val_loss: 1.0637 - val_acc: 0.7410\n",
      "Epoch 286/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.0004 - acc: 0.7636 - val_loss: 1.0554 - val_acc: 0.7420\n",
      "Epoch 287/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9985 - acc: 0.7647 - val_loss: 1.0637 - val_acc: 0.7390\n",
      "Epoch 288/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.9982 - acc: 0.7650 - val_loss: 1.0599 - val_acc: 0.7380\n",
      "Epoch 289/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9973 - acc: 0.7659 - val_loss: 1.0541 - val_acc: 0.7430\n",
      "Epoch 290/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9970 - acc: 0.7650 - val_loss: 1.0555 - val_acc: 0.7430\n",
      "Epoch 291/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9963 - acc: 0.7654 - val_loss: 1.0572 - val_acc: 0.7400\n",
      "Epoch 292/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9956 - acc: 0.7653 - val_loss: 1.0520 - val_acc: 0.7420\n",
      "Epoch 293/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9949 - acc: 0.7649 - val_loss: 1.0494 - val_acc: 0.7430\n",
      "Epoch 294/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9940 - acc: 0.7656 - val_loss: 1.0557 - val_acc: 0.7420\n",
      "Epoch 295/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9936 - acc: 0.7664 - val_loss: 1.0547 - val_acc: 0.7400\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9921 - acc: 0.7666 - val_loss: 1.0477 - val_acc: 0.7430\n",
      "Epoch 297/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9919 - acc: 0.7677 - val_loss: 1.0620 - val_acc: 0.7390\n",
      "Epoch 298/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9916 - acc: 0.7667 - val_loss: 1.0498 - val_acc: 0.7440\n",
      "Epoch 299/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9900 - acc: 0.7659 - val_loss: 1.0465 - val_acc: 0.7450\n",
      "Epoch 300/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9899 - acc: 0.7676 - val_loss: 1.0499 - val_acc: 0.7420\n",
      "Epoch 301/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9889 - acc: 0.7674 - val_loss: 1.0477 - val_acc: 0.7410\n",
      "Epoch 302/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9880 - acc: 0.7663 - val_loss: 1.0538 - val_acc: 0.7390\n",
      "Epoch 303/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9880 - acc: 0.7667 - val_loss: 1.0494 - val_acc: 0.7410\n",
      "Epoch 304/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9867 - acc: 0.7677 - val_loss: 1.0440 - val_acc: 0.7450\n",
      "Epoch 305/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9863 - acc: 0.7664 - val_loss: 1.0498 - val_acc: 0.7420\n",
      "Epoch 306/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9863 - acc: 0.7659 - val_loss: 1.0450 - val_acc: 0.7420\n",
      "Epoch 307/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9850 - acc: 0.7687 - val_loss: 1.0477 - val_acc: 0.7430\n",
      "Epoch 308/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9844 - acc: 0.7673 - val_loss: 1.0443 - val_acc: 0.7420\n",
      "Epoch 309/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9842 - acc: 0.7679 - val_loss: 1.0471 - val_acc: 0.7420\n",
      "Epoch 310/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9833 - acc: 0.7690 - val_loss: 1.0459 - val_acc: 0.7430\n",
      "Epoch 311/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9824 - acc: 0.7660 - val_loss: 1.0431 - val_acc: 0.7460\n",
      "Epoch 312/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9825 - acc: 0.7687 - val_loss: 1.0435 - val_acc: 0.7430\n",
      "Epoch 313/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9821 - acc: 0.7666 - val_loss: 1.0397 - val_acc: 0.7450\n",
      "Epoch 314/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9810 - acc: 0.7670 - val_loss: 1.0499 - val_acc: 0.7360\n",
      "Epoch 315/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9799 - acc: 0.7679 - val_loss: 1.0400 - val_acc: 0.7430\n",
      "Epoch 316/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9797 - acc: 0.7674 - val_loss: 1.0438 - val_acc: 0.7430\n",
      "Epoch 317/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9783 - acc: 0.7671 - val_loss: 1.0396 - val_acc: 0.7430\n",
      "Epoch 318/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9785 - acc: 0.7664 - val_loss: 1.0386 - val_acc: 0.7450\n",
      "Epoch 319/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9782 - acc: 0.7681 - val_loss: 1.0386 - val_acc: 0.7440\n",
      "Epoch 320/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9766 - acc: 0.7703 - val_loss: 1.0370 - val_acc: 0.7440\n",
      "Epoch 321/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9761 - acc: 0.7689 - val_loss: 1.0391 - val_acc: 0.7460\n",
      "Epoch 322/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9759 - acc: 0.7689 - val_loss: 1.0384 - val_acc: 0.7400\n",
      "Epoch 323/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9749 - acc: 0.7706 - val_loss: 1.0463 - val_acc: 0.7390\n",
      "Epoch 324/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9754 - acc: 0.7677 - val_loss: 1.0362 - val_acc: 0.7400\n",
      "Epoch 325/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9732 - acc: 0.7707 - val_loss: 1.0344 - val_acc: 0.7440\n",
      "Epoch 326/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9733 - acc: 0.7694 - val_loss: 1.0330 - val_acc: 0.7400\n",
      "Epoch 327/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9724 - acc: 0.7693 - val_loss: 1.0367 - val_acc: 0.7410\n",
      "Epoch 328/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9718 - acc: 0.7703 - val_loss: 1.0333 - val_acc: 0.7420\n",
      "Epoch 329/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9709 - acc: 0.7690 - val_loss: 1.0372 - val_acc: 0.7370\n",
      "Epoch 330/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9712 - acc: 0.7693 - val_loss: 1.0314 - val_acc: 0.7430\n",
      "Epoch 331/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9705 - acc: 0.7703 - val_loss: 1.0318 - val_acc: 0.7440\n",
      "Epoch 332/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9697 - acc: 0.7691 - val_loss: 1.0334 - val_acc: 0.7350\n",
      "Epoch 333/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9694 - acc: 0.7711 - val_loss: 1.0300 - val_acc: 0.7450\n",
      "Epoch 334/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9682 - acc: 0.7714 - val_loss: 1.0341 - val_acc: 0.7410\n",
      "Epoch 335/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9682 - acc: 0.7706 - val_loss: 1.0308 - val_acc: 0.7420\n",
      "Epoch 336/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9675 - acc: 0.7690 - val_loss: 1.0323 - val_acc: 0.7380\n",
      "Epoch 337/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9667 - acc: 0.7703 - val_loss: 1.0288 - val_acc: 0.7450\n",
      "Epoch 338/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9669 - acc: 0.7716 - val_loss: 1.0298 - val_acc: 0.7410\n",
      "Epoch 339/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9655 - acc: 0.7701 - val_loss: 1.0300 - val_acc: 0.7440\n",
      "Epoch 340/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9655 - acc: 0.7701 - val_loss: 1.0296 - val_acc: 0.7440\n",
      "Epoch 341/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9647 - acc: 0.7710 - val_loss: 1.0275 - val_acc: 0.7450\n",
      "Epoch 342/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9635 - acc: 0.7699 - val_loss: 1.0261 - val_acc: 0.7440\n",
      "Epoch 343/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9637 - acc: 0.7709 - val_loss: 1.0314 - val_acc: 0.7440\n",
      "Epoch 344/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9636 - acc: 0.7706 - val_loss: 1.0302 - val_acc: 0.7420\n",
      "Epoch 345/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9629 - acc: 0.7721 - val_loss: 1.0276 - val_acc: 0.7450\n",
      "Epoch 346/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9620 - acc: 0.7703 - val_loss: 1.0335 - val_acc: 0.7430\n",
      "Epoch 347/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9615 - acc: 0.7704 - val_loss: 1.0337 - val_acc: 0.7380\n",
      "Epoch 348/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9616 - acc: 0.7707 - val_loss: 1.0267 - val_acc: 0.7410\n",
      "Epoch 349/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9605 - acc: 0.7729 - val_loss: 1.0285 - val_acc: 0.7420\n",
      "Epoch 350/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9595 - acc: 0.7699 - val_loss: 1.0231 - val_acc: 0.7450\n",
      "Epoch 351/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9595 - acc: 0.7706 - val_loss: 1.0250 - val_acc: 0.7390\n",
      "Epoch 352/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9581 - acc: 0.7709 - val_loss: 1.0228 - val_acc: 0.7420\n",
      "Epoch 353/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9582 - acc: 0.7720 - val_loss: 1.0202 - val_acc: 0.7470\n",
      "Epoch 354/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9586 - acc: 0.7719 - val_loss: 1.0258 - val_acc: 0.7450\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9568 - acc: 0.7716 - val_loss: 1.0212 - val_acc: 0.7420\n",
      "Epoch 356/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9573 - acc: 0.7723 - val_loss: 1.0221 - val_acc: 0.7430\n",
      "Epoch 357/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9561 - acc: 0.7721 - val_loss: 1.0382 - val_acc: 0.7370\n",
      "Epoch 358/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9561 - acc: 0.7710 - val_loss: 1.0204 - val_acc: 0.7450\n",
      "Epoch 359/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9547 - acc: 0.7724 - val_loss: 1.0203 - val_acc: 0.7430\n",
      "Epoch 360/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9551 - acc: 0.7711 - val_loss: 1.0269 - val_acc: 0.7390\n",
      "Epoch 361/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9544 - acc: 0.7726 - val_loss: 1.0197 - val_acc: 0.7460\n",
      "Epoch 362/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9533 - acc: 0.7720 - val_loss: 1.0176 - val_acc: 0.7400\n",
      "Epoch 363/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9528 - acc: 0.7703 - val_loss: 1.0150 - val_acc: 0.7450\n",
      "Epoch 364/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9526 - acc: 0.7710 - val_loss: 1.0356 - val_acc: 0.7420\n",
      "Epoch 365/1000\n",
      "7000/7000 [==============================] - 1s 98us/step - loss: 0.9515 - acc: 0.7730 - val_loss: 1.0197 - val_acc: 0.7400\n",
      "Epoch 366/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9511 - acc: 0.7726 - val_loss: 1.0193 - val_acc: 0.7460\n",
      "Epoch 367/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9508 - acc: 0.7723 - val_loss: 1.0210 - val_acc: 0.7420\n",
      "Epoch 368/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9497 - acc: 0.7717 - val_loss: 1.0153 - val_acc: 0.7460\n",
      "Epoch 369/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9497 - acc: 0.7731 - val_loss: 1.0162 - val_acc: 0.7440\n",
      "Epoch 370/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9492 - acc: 0.7734 - val_loss: 1.0142 - val_acc: 0.7470\n",
      "Epoch 371/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9489 - acc: 0.7724 - val_loss: 1.0151 - val_acc: 0.7430\n",
      "Epoch 372/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9482 - acc: 0.7727 - val_loss: 1.0136 - val_acc: 0.7440\n",
      "Epoch 373/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9485 - acc: 0.7720 - val_loss: 1.0127 - val_acc: 0.7460\n",
      "Epoch 374/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9468 - acc: 0.7726 - val_loss: 1.0185 - val_acc: 0.7420\n",
      "Epoch 375/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.9468 - acc: 0.7730 - val_loss: 1.0152 - val_acc: 0.7450\n",
      "Epoch 376/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9458 - acc: 0.7747 - val_loss: 1.0195 - val_acc: 0.7360\n",
      "Epoch 377/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9463 - acc: 0.7727 - val_loss: 1.0121 - val_acc: 0.7440\n",
      "Epoch 378/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9448 - acc: 0.7736 - val_loss: 1.0148 - val_acc: 0.7410\n",
      "Epoch 379/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9452 - acc: 0.7726 - val_loss: 1.0149 - val_acc: 0.7420\n",
      "Epoch 380/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9448 - acc: 0.7744 - val_loss: 1.0143 - val_acc: 0.7380\n",
      "Epoch 381/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9440 - acc: 0.7727 - val_loss: 1.0155 - val_acc: 0.7410\n",
      "Epoch 382/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9444 - acc: 0.7749 - val_loss: 1.0130 - val_acc: 0.7440\n",
      "Epoch 383/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9430 - acc: 0.7734 - val_loss: 1.0108 - val_acc: 0.7430\n",
      "Epoch 384/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9424 - acc: 0.7744 - val_loss: 1.0079 - val_acc: 0.74700s - loss: 0.9313 - acc: 0.\n",
      "Epoch 385/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9420 - acc: 0.7740 - val_loss: 1.0075 - val_acc: 0.7470\n",
      "Epoch 386/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9417 - acc: 0.7750 - val_loss: 1.0181 - val_acc: 0.7450\n",
      "Epoch 387/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9411 - acc: 0.7747 - val_loss: 1.0089 - val_acc: 0.7400\n",
      "Epoch 388/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9399 - acc: 0.7761 - val_loss: 1.0155 - val_acc: 0.7380\n",
      "Epoch 389/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9407 - acc: 0.7750 - val_loss: 1.0094 - val_acc: 0.7450\n",
      "Epoch 390/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9397 - acc: 0.7746 - val_loss: 1.0077 - val_acc: 0.7440\n",
      "Epoch 391/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9389 - acc: 0.7743 - val_loss: 1.0172 - val_acc: 0.7430\n",
      "Epoch 392/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9399 - acc: 0.7743 - val_loss: 1.0061 - val_acc: 0.7440\n",
      "Epoch 393/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9381 - acc: 0.7736 - val_loss: 1.0063 - val_acc: 0.7410\n",
      "Epoch 394/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9381 - acc: 0.7743 - val_loss: 1.0058 - val_acc: 0.7460\n",
      "Epoch 395/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.9374 - acc: 0.7749 - val_loss: 1.0108 - val_acc: 0.7360\n",
      "Epoch 396/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9366 - acc: 0.7751 - val_loss: 1.0060 - val_acc: 0.7400\n",
      "Epoch 397/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9363 - acc: 0.7746 - val_loss: 1.0032 - val_acc: 0.7470\n",
      "Epoch 398/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9354 - acc: 0.7756 - val_loss: 1.0099 - val_acc: 0.7390\n",
      "Epoch 399/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9351 - acc: 0.7751 - val_loss: 1.0172 - val_acc: 0.7370\n",
      "Epoch 400/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9364 - acc: 0.7724 - val_loss: 1.0062 - val_acc: 0.7450\n",
      "Epoch 401/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9346 - acc: 0.7744 - val_loss: 1.0178 - val_acc: 0.7380\n",
      "Epoch 402/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9343 - acc: 0.7730 - val_loss: 1.0069 - val_acc: 0.7460\n",
      "Epoch 403/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9344 - acc: 0.7754 - val_loss: 1.0031 - val_acc: 0.7490\n",
      "Epoch 404/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9325 - acc: 0.7756 - val_loss: 1.0072 - val_acc: 0.7420\n",
      "Epoch 405/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9325 - acc: 0.7754 - val_loss: 1.0029 - val_acc: 0.7440\n",
      "Epoch 406/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9322 - acc: 0.7760 - val_loss: 1.0058 - val_acc: 0.7410\n",
      "Epoch 407/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9328 - acc: 0.7747 - val_loss: 1.0020 - val_acc: 0.7480\n",
      "Epoch 408/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9310 - acc: 0.7773 - val_loss: 1.0060 - val_acc: 0.7390\n",
      "Epoch 409/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9309 - acc: 0.7770 - val_loss: 1.0126 - val_acc: 0.7420\n",
      "Epoch 410/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9311 - acc: 0.7737 - val_loss: 1.0002 - val_acc: 0.7440\n",
      "Epoch 411/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9298 - acc: 0.7751 - val_loss: 1.0017 - val_acc: 0.7410\n",
      "Epoch 412/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.9288 - acc: 0.7771 - val_loss: 1.0008 - val_acc: 0.7340\n",
      "Epoch 413/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.9293 - acc: 0.7760 - val_loss: 1.0091 - val_acc: 0.7390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9291 - acc: 0.7770 - val_loss: 1.0056 - val_acc: 0.7380\n",
      "Epoch 415/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9286 - acc: 0.7779 - val_loss: 0.9988 - val_acc: 0.7450\n",
      "Epoch 416/1000\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 0.9276 - acc: 0.7780 - val_loss: 0.9960 - val_acc: 0.7420\n",
      "Epoch 417/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.9276 - acc: 0.7771 - val_loss: 0.9959 - val_acc: 0.7470\n",
      "Epoch 418/1000\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 0.9274 - acc: 0.7769 - val_loss: 1.0012 - val_acc: 0.7440\n",
      "Epoch 419/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9265 - acc: 0.7770 - val_loss: 1.0044 - val_acc: 0.7410\n",
      "Epoch 420/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9267 - acc: 0.7747 - val_loss: 0.9956 - val_acc: 0.7480\n",
      "Epoch 421/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9257 - acc: 0.7773 - val_loss: 0.9996 - val_acc: 0.7440\n",
      "Epoch 422/1000\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 0.9257 - acc: 0.7771 - val_loss: 1.0003 - val_acc: 0.7400\n",
      "Epoch 423/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9253 - acc: 0.7769 - val_loss: 1.0019 - val_acc: 0.7450\n",
      "Epoch 424/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.9242 - acc: 0.7773 - val_loss: 0.9952 - val_acc: 0.7410\n",
      "Epoch 425/1000\n",
      "7000/7000 [==============================] - 1s 108us/step - loss: 0.9240 - acc: 0.7777 - val_loss: 0.9958 - val_acc: 0.7450\n",
      "Epoch 426/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9236 - acc: 0.7764 - val_loss: 0.9948 - val_acc: 0.7480\n",
      "Epoch 427/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9230 - acc: 0.7780 - val_loss: 0.9971 - val_acc: 0.7490\n",
      "Epoch 428/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9232 - acc: 0.7781 - val_loss: 0.9925 - val_acc: 0.7470\n",
      "Epoch 429/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.9219 - acc: 0.7759 - val_loss: 0.9999 - val_acc: 0.7460\n",
      "Epoch 430/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.9225 - acc: 0.7787 - val_loss: 0.9947 - val_acc: 0.7490\n",
      "Epoch 431/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.9209 - acc: 0.7784 - val_loss: 0.9967 - val_acc: 0.7410\n",
      "Epoch 432/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.9213 - acc: 0.7766 - val_loss: 0.9941 - val_acc: 0.7430\n",
      "Epoch 433/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.9213 - acc: 0.7779 - val_loss: 0.9929 - val_acc: 0.7400\n",
      "Epoch 434/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9201 - acc: 0.7769 - val_loss: 0.9930 - val_acc: 0.7430\n",
      "Epoch 435/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9201 - acc: 0.7740 - val_loss: 0.9934 - val_acc: 0.7460\n",
      "Epoch 436/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.9198 - acc: 0.7749 - val_loss: 0.9901 - val_acc: 0.7440\n",
      "Epoch 437/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.9199 - acc: 0.7790 - val_loss: 0.9902 - val_acc: 0.7470\n",
      "Epoch 438/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9182 - acc: 0.7787 - val_loss: 0.9915 - val_acc: 0.7400\n",
      "Epoch 439/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.9180 - acc: 0.7800 - val_loss: 0.9916 - val_acc: 0.7470\n",
      "Epoch 440/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9187 - acc: 0.7784 - val_loss: 0.9914 - val_acc: 0.7440\n",
      "Epoch 441/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.9179 - acc: 0.7779 - val_loss: 0.9897 - val_acc: 0.7470\n",
      "Epoch 442/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9172 - acc: 0.7774 - val_loss: 0.9904 - val_acc: 0.7490\n",
      "Epoch 443/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.9160 - acc: 0.7799 - val_loss: 1.0001 - val_acc: 0.7370\n",
      "Epoch 444/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9167 - acc: 0.7771 - val_loss: 0.9994 - val_acc: 0.7360\n",
      "Epoch 445/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.9166 - acc: 0.7784 - val_loss: 0.9886 - val_acc: 0.7460\n",
      "Epoch 446/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9156 - acc: 0.7771 - val_loss: 0.9924 - val_acc: 0.7430\n",
      "Epoch 447/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9147 - acc: 0.7800 - val_loss: 0.9896 - val_acc: 0.7440\n",
      "Epoch 448/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9154 - acc: 0.7784 - val_loss: 0.9959 - val_acc: 0.7380\n",
      "Epoch 449/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9145 - acc: 0.7787 - val_loss: 0.9869 - val_acc: 0.7430\n",
      "Epoch 450/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9138 - acc: 0.7793 - val_loss: 0.9861 - val_acc: 0.7460\n",
      "Epoch 451/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9140 - acc: 0.7777 - val_loss: 0.9866 - val_acc: 0.7450\n",
      "Epoch 452/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.9133 - acc: 0.7807 - val_loss: 0.9883 - val_acc: 0.7450\n",
      "Epoch 453/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9135 - acc: 0.7806 - val_loss: 0.9884 - val_acc: 0.7390\n",
      "Epoch 454/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9128 - acc: 0.7801 - val_loss: 0.9836 - val_acc: 0.7510\n",
      "Epoch 455/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9123 - acc: 0.7797 - val_loss: 1.0013 - val_acc: 0.7380\n",
      "Epoch 456/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9127 - acc: 0.7790 - val_loss: 0.9897 - val_acc: 0.7430\n",
      "Epoch 457/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9115 - acc: 0.7796 - val_loss: 0.9881 - val_acc: 0.7450\n",
      "Epoch 458/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9113 - acc: 0.7794 - val_loss: 0.9861 - val_acc: 0.7400\n",
      "Epoch 459/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.9105 - acc: 0.7817 - val_loss: 1.0007 - val_acc: 0.7440\n",
      "Epoch 460/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9110 - acc: 0.7797 - val_loss: 0.9959 - val_acc: 0.7420\n",
      "Epoch 461/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.9109 - acc: 0.7796 - val_loss: 0.9919 - val_acc: 0.7460\n",
      "Epoch 462/1000\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.9103 - acc: 0.778 - 1s 108us/step - loss: 0.9104 - acc: 0.7779 - val_loss: 0.9870 - val_acc: 0.7410\n",
      "Epoch 463/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9104 - acc: 0.7803 - val_loss: 0.9875 - val_acc: 0.7420\n",
      "Epoch 464/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9090 - acc: 0.7807 - val_loss: 0.9810 - val_acc: 0.7470\n",
      "Epoch 465/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9085 - acc: 0.7799 - val_loss: 0.9867 - val_acc: 0.7370\n",
      "Epoch 466/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9086 - acc: 0.7796 - val_loss: 0.9891 - val_acc: 0.7510\n",
      "Epoch 467/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9087 - acc: 0.7796 - val_loss: 0.9830 - val_acc: 0.7440\n",
      "Epoch 468/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9075 - acc: 0.7830 - val_loss: 0.9902 - val_acc: 0.7390\n",
      "Epoch 469/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9083 - acc: 0.7796 - val_loss: 0.9801 - val_acc: 0.7480\n",
      "Epoch 470/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9079 - acc: 0.7803 - val_loss: 0.9815 - val_acc: 0.7470\n",
      "Epoch 471/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.9074 - acc: 0.7797 - val_loss: 0.9839 - val_acc: 0.7430\n",
      "Epoch 472/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9076 - acc: 0.7801 - val_loss: 0.9873 - val_acc: 0.7350\n",
      "Epoch 473/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.9068 - acc: 0.7800 - val_loss: 0.9811 - val_acc: 0.7510\n",
      "Epoch 474/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9067 - acc: 0.7807 - val_loss: 0.9789 - val_acc: 0.7450\n",
      "Epoch 475/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9053 - acc: 0.7807 - val_loss: 0.9822 - val_acc: 0.7480\n",
      "Epoch 476/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9060 - acc: 0.7814 - val_loss: 0.9825 - val_acc: 0.7490\n",
      "Epoch 477/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9048 - acc: 0.7820 - val_loss: 0.9819 - val_acc: 0.7410\n",
      "Epoch 478/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9051 - acc: 0.7817 - val_loss: 0.9804 - val_acc: 0.7490\n",
      "Epoch 479/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9043 - acc: 0.7799 - val_loss: 0.9751 - val_acc: 0.7520\n",
      "Epoch 480/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9043 - acc: 0.7789 - val_loss: 0.9759 - val_acc: 0.7490\n",
      "Epoch 481/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9041 - acc: 0.7794 - val_loss: 0.9820 - val_acc: 0.7430\n",
      "Epoch 482/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9042 - acc: 0.7800 - val_loss: 0.9793 - val_acc: 0.7480\n",
      "Epoch 483/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9027 - acc: 0.7793 - val_loss: 0.9799 - val_acc: 0.7490\n",
      "Epoch 484/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9030 - acc: 0.7817 - val_loss: 0.9800 - val_acc: 0.7410\n",
      "Epoch 485/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.9015 - acc: 0.7807 - val_loss: 0.9818 - val_acc: 0.7420\n",
      "Epoch 486/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9017 - acc: 0.7817 - val_loss: 0.9820 - val_acc: 0.7410\n",
      "Epoch 487/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.9016 - acc: 0.7820 - val_loss: 0.9799 - val_acc: 0.7420\n",
      "Epoch 488/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.9017 - acc: 0.7813 - val_loss: 0.9760 - val_acc: 0.7510\n",
      "Epoch 489/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9016 - acc: 0.7817 - val_loss: 0.9793 - val_acc: 0.7460\n",
      "Epoch 490/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9012 - acc: 0.7791 - val_loss: 0.9774 - val_acc: 0.7450\n",
      "Epoch 491/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9005 - acc: 0.7804 - val_loss: 0.9737 - val_acc: 0.7540\n",
      "Epoch 492/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.9004 - acc: 0.7801 - val_loss: 0.9781 - val_acc: 0.7390\n",
      "Epoch 493/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.9000 - acc: 0.7829 - val_loss: 0.9810 - val_acc: 0.7420\n",
      "Epoch 494/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.9002 - acc: 0.7810 - val_loss: 0.9753 - val_acc: 0.7510\n",
      "Epoch 495/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8990 - acc: 0.7813 - val_loss: 0.9808 - val_acc: 0.7420\n",
      "Epoch 496/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8990 - acc: 0.7821 - val_loss: 0.9729 - val_acc: 0.7500\n",
      "Epoch 497/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8987 - acc: 0.7826 - val_loss: 0.9797 - val_acc: 0.7420\n",
      "Epoch 498/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8983 - acc: 0.7827 - val_loss: 0.9725 - val_acc: 0.7500\n",
      "Epoch 499/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8979 - acc: 0.7826 - val_loss: 0.9741 - val_acc: 0.7450\n",
      "Epoch 500/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8975 - acc: 0.7837 - val_loss: 0.9744 - val_acc: 0.7440\n",
      "Epoch 501/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8969 - acc: 0.7843 - val_loss: 0.9733 - val_acc: 0.7500\n",
      "Epoch 502/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8968 - acc: 0.7819 - val_loss: 0.9822 - val_acc: 0.7390\n",
      "Epoch 503/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8981 - acc: 0.7807 - val_loss: 0.9800 - val_acc: 0.7420\n",
      "Epoch 504/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8965 - acc: 0.7829 - val_loss: 0.9698 - val_acc: 0.7470\n",
      "Epoch 505/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8952 - acc: 0.7833 - val_loss: 0.9707 - val_acc: 0.7500\n",
      "Epoch 506/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8957 - acc: 0.7829 - val_loss: 0.9716 - val_acc: 0.7510\n",
      "Epoch 507/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8956 - acc: 0.7824 - val_loss: 0.9720 - val_acc: 0.7490\n",
      "Epoch 508/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8952 - acc: 0.7846 - val_loss: 0.9743 - val_acc: 0.7480\n",
      "Epoch 509/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.8958 - acc: 0.7814 - val_loss: 0.9858 - val_acc: 0.7390\n",
      "Epoch 510/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8966 - acc: 0.7826 - val_loss: 0.9735 - val_acc: 0.7500\n",
      "Epoch 511/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8959 - acc: 0.7813 - val_loss: 0.9711 - val_acc: 0.7440\n",
      "Epoch 512/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8948 - acc: 0.7841 - val_loss: 0.9752 - val_acc: 0.7390\n",
      "Epoch 513/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8932 - acc: 0.7827 - val_loss: 0.9811 - val_acc: 0.7380\n",
      "Epoch 514/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8943 - acc: 0.7846 - val_loss: 0.9685 - val_acc: 0.7480\n",
      "Epoch 515/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8934 - acc: 0.7819 - val_loss: 0.9753 - val_acc: 0.7460\n",
      "Epoch 516/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8930 - acc: 0.7810 - val_loss: 0.9783 - val_acc: 0.7500\n",
      "Epoch 517/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8926 - acc: 0.7837 - val_loss: 0.9699 - val_acc: 0.7500\n",
      "Epoch 518/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8932 - acc: 0.7824 - val_loss: 0.9709 - val_acc: 0.7520\n",
      "Epoch 519/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8918 - acc: 0.7844 - val_loss: 0.9754 - val_acc: 0.7410\n",
      "Epoch 520/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.8912 - acc: 0.7837 - val_loss: 0.9667 - val_acc: 0.7500\n",
      "Epoch 521/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8927 - acc: 0.7831 - val_loss: 0.9679 - val_acc: 0.7510\n",
      "Epoch 522/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8916 - acc: 0.7821 - val_loss: 0.9671 - val_acc: 0.7550\n",
      "Epoch 523/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8914 - acc: 0.7849 - val_loss: 0.9670 - val_acc: 0.7500\n",
      "Epoch 524/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8906 - acc: 0.7831 - val_loss: 0.9661 - val_acc: 0.7500\n",
      "Epoch 525/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8902 - acc: 0.7817 - val_loss: 0.9791 - val_acc: 0.7410\n",
      "Epoch 526/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8904 - acc: 0.7826 - val_loss: 0.9687 - val_acc: 0.7510\n",
      "Epoch 527/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8906 - acc: 0.7836 - val_loss: 0.9709 - val_acc: 0.7440\n",
      "Epoch 528/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8899 - acc: 0.7834 - val_loss: 0.9713 - val_acc: 0.7420\n",
      "Epoch 529/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8902 - acc: 0.7836 - val_loss: 0.9702 - val_acc: 0.7550\n",
      "Epoch 530/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.8895 - acc: 0.7850 - val_loss: 0.9664 - val_acc: 0.7450\n",
      "Epoch 531/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8887 - acc: 0.7819 - val_loss: 0.9758 - val_acc: 0.7430\n",
      "Epoch 532/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8898 - acc: 0.7821 - val_loss: 0.9681 - val_acc: 0.7510\n",
      "Epoch 533/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8888 - acc: 0.7844 - val_loss: 0.9704 - val_acc: 0.7540\n",
      "Epoch 534/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8881 - acc: 0.7840 - val_loss: 0.9628 - val_acc: 0.7540\n",
      "Epoch 535/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8873 - acc: 0.7836 - val_loss: 0.9712 - val_acc: 0.7480\n",
      "Epoch 536/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8878 - acc: 0.7834 - val_loss: 0.9621 - val_acc: 0.7540\n",
      "Epoch 537/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8886 - acc: 0.7833 - val_loss: 0.9678 - val_acc: 0.7500\n",
      "Epoch 538/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8872 - acc: 0.7857 - val_loss: 0.9803 - val_acc: 0.7470\n",
      "Epoch 539/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8876 - acc: 0.7839 - val_loss: 0.9669 - val_acc: 0.7500\n",
      "Epoch 540/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8856 - acc: 0.7851 - val_loss: 0.9651 - val_acc: 0.7470\n",
      "Epoch 541/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8859 - acc: 0.7837 - val_loss: 0.9674 - val_acc: 0.7440\n",
      "Epoch 542/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8861 - acc: 0.7854 - val_loss: 0.9663 - val_acc: 0.7410\n",
      "Epoch 543/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8851 - acc: 0.7847 - val_loss: 0.9667 - val_acc: 0.7440\n",
      "Epoch 544/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8848 - acc: 0.7836 - val_loss: 0.9656 - val_acc: 0.7480\n",
      "Epoch 545/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8846 - acc: 0.7873 - val_loss: 0.9672 - val_acc: 0.7540\n",
      "Epoch 546/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8854 - acc: 0.7866 - val_loss: 0.9666 - val_acc: 0.7500\n",
      "Epoch 547/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8850 - acc: 0.7870 - val_loss: 0.9616 - val_acc: 0.7540\n",
      "Epoch 548/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8835 - acc: 0.7861 - val_loss: 0.9627 - val_acc: 0.7530\n",
      "Epoch 549/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8834 - acc: 0.7866 - val_loss: 0.9633 - val_acc: 0.7490\n",
      "Epoch 550/1000\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.8822 - acc: 0.786 - 1s 87us/step - loss: 0.8836 - acc: 0.7849 - val_loss: 0.9639 - val_acc: 0.7530\n",
      "Epoch 551/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8835 - acc: 0.7853 - val_loss: 0.9631 - val_acc: 0.7530\n",
      "Epoch 552/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8829 - acc: 0.7850 - val_loss: 0.9604 - val_acc: 0.7450\n",
      "Epoch 553/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8821 - acc: 0.7847 - val_loss: 0.9720 - val_acc: 0.7450\n",
      "Epoch 554/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8829 - acc: 0.7850 - val_loss: 0.9611 - val_acc: 0.7500\n",
      "Epoch 555/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8825 - acc: 0.7861 - val_loss: 0.9714 - val_acc: 0.7460\n",
      "Epoch 556/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8823 - acc: 0.7857 - val_loss: 0.9626 - val_acc: 0.7530\n",
      "Epoch 557/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8818 - acc: 0.7863 - val_loss: 0.9586 - val_acc: 0.7510\n",
      "Epoch 558/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8808 - acc: 0.7869 - val_loss: 0.9763 - val_acc: 0.7440\n",
      "Epoch 559/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8811 - acc: 0.7869 - val_loss: 0.9663 - val_acc: 0.7500\n",
      "Epoch 560/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8817 - acc: 0.7873 - val_loss: 0.9672 - val_acc: 0.7480\n",
      "Epoch 561/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8822 - acc: 0.7870 - val_loss: 0.9637 - val_acc: 0.7500\n",
      "Epoch 562/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8794 - acc: 0.7876 - val_loss: 0.9655 - val_acc: 0.7480\n",
      "Epoch 563/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8806 - acc: 0.7860 - val_loss: 0.9612 - val_acc: 0.7480\n",
      "Epoch 564/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8793 - acc: 0.7854 - val_loss: 0.9761 - val_acc: 0.7430\n",
      "Epoch 565/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8804 - acc: 0.7873 - val_loss: 0.9685 - val_acc: 0.7490\n",
      "Epoch 566/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8803 - acc: 0.7847 - val_loss: 0.9596 - val_acc: 0.7550\n",
      "Epoch 567/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8790 - acc: 0.7863 - val_loss: 0.9575 - val_acc: 0.7600\n",
      "Epoch 568/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8795 - acc: 0.7860 - val_loss: 0.9600 - val_acc: 0.7590\n",
      "Epoch 569/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8786 - acc: 0.7877 - val_loss: 0.9625 - val_acc: 0.7430\n",
      "Epoch 570/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8787 - acc: 0.7861 - val_loss: 0.9605 - val_acc: 0.7470\n",
      "Epoch 571/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8778 - acc: 0.7867 - val_loss: 0.9585 - val_acc: 0.7480\n",
      "Epoch 572/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8777 - acc: 0.7869 - val_loss: 0.9592 - val_acc: 0.7570\n",
      "Epoch 573/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8777 - acc: 0.7859 - val_loss: 0.9578 - val_acc: 0.7570\n",
      "Epoch 574/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8776 - acc: 0.7859 - val_loss: 0.9577 - val_acc: 0.7540\n",
      "Epoch 575/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8776 - acc: 0.7873 - val_loss: 0.9592 - val_acc: 0.7540\n",
      "Epoch 576/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8766 - acc: 0.7871 - val_loss: 0.9609 - val_acc: 0.7560\n",
      "Epoch 577/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8766 - acc: 0.7864 - val_loss: 0.9564 - val_acc: 0.7550\n",
      "Epoch 578/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8779 - acc: 0.7856 - val_loss: 0.9626 - val_acc: 0.7450\n",
      "Epoch 579/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8754 - acc: 0.7877 - val_loss: 0.9572 - val_acc: 0.7540\n",
      "Epoch 580/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8756 - acc: 0.7886 - val_loss: 0.9659 - val_acc: 0.7530\n",
      "Epoch 581/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8777 - acc: 0.7877 - val_loss: 0.9570 - val_acc: 0.7540\n",
      "Epoch 582/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8740 - acc: 0.7887 - val_loss: 0.9551 - val_acc: 0.7570\n",
      "Epoch 583/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8751 - acc: 0.7883 - val_loss: 0.9576 - val_acc: 0.7450\n",
      "Epoch 584/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8752 - acc: 0.7873 - val_loss: 0.9641 - val_acc: 0.7480\n",
      "Epoch 585/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8756 - acc: 0.7860 - val_loss: 0.9660 - val_acc: 0.7450\n",
      "Epoch 586/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8753 - acc: 0.7877 - val_loss: 0.9683 - val_acc: 0.7500\n",
      "Epoch 587/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8748 - acc: 0.7891 - val_loss: 0.9715 - val_acc: 0.7480\n",
      "Epoch 588/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8745 - acc: 0.7863 - val_loss: 0.9526 - val_acc: 0.7570\n",
      "Epoch 589/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8736 - acc: 0.7869 - val_loss: 0.9626 - val_acc: 0.7510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/1000\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.8721 - acc: 0.788 - 1s 86us/step - loss: 0.8730 - acc: 0.7886 - val_loss: 0.9586 - val_acc: 0.7510\n",
      "Epoch 591/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8735 - acc: 0.7894 - val_loss: 0.9566 - val_acc: 0.7510\n",
      "Epoch 592/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8736 - acc: 0.7887 - val_loss: 0.9557 - val_acc: 0.7460\n",
      "Epoch 593/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8730 - acc: 0.7881 - val_loss: 0.9744 - val_acc: 0.7340\n",
      "Epoch 594/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8728 - acc: 0.7889 - val_loss: 0.9604 - val_acc: 0.7510\n",
      "Epoch 595/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8727 - acc: 0.7879 - val_loss: 0.9581 - val_acc: 0.7490\n",
      "Epoch 596/1000\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.8727 - acc: 0.7881 - val_loss: 0.9509 - val_acc: 0.7580\n",
      "Epoch 597/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8715 - acc: 0.7889 - val_loss: 0.9582 - val_acc: 0.7450\n",
      "Epoch 598/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8724 - acc: 0.7883 - val_loss: 0.9622 - val_acc: 0.7480\n",
      "Epoch 599/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8716 - acc: 0.7867 - val_loss: 0.9517 - val_acc: 0.7530\n",
      "Epoch 600/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8697 - acc: 0.7883 - val_loss: 0.9561 - val_acc: 0.7490\n",
      "Epoch 601/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8716 - acc: 0.7881 - val_loss: 0.9545 - val_acc: 0.7500\n",
      "Epoch 602/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8706 - acc: 0.7881 - val_loss: 0.9609 - val_acc: 0.7540\n",
      "Epoch 603/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8702 - acc: 0.7871 - val_loss: 0.9568 - val_acc: 0.7410\n",
      "Epoch 604/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8698 - acc: 0.7889 - val_loss: 0.9550 - val_acc: 0.7580\n",
      "Epoch 605/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8708 - acc: 0.7889 - val_loss: 0.9543 - val_acc: 0.7510\n",
      "Epoch 606/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8696 - acc: 0.7891 - val_loss: 0.9555 - val_acc: 0.7510\n",
      "Epoch 607/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8704 - acc: 0.7897 - val_loss: 0.9545 - val_acc: 0.7540\n",
      "Epoch 608/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8679 - acc: 0.7891 - val_loss: 0.9493 - val_acc: 0.7620\n",
      "Epoch 609/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.8684 - acc: 0.7909 - val_loss: 0.9491 - val_acc: 0.7530\n",
      "Epoch 610/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8696 - acc: 0.7891 - val_loss: 0.9621 - val_acc: 0.7530\n",
      "Epoch 611/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8702 - acc: 0.7871 - val_loss: 0.9533 - val_acc: 0.7480\n",
      "Epoch 612/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8682 - acc: 0.7889 - val_loss: 0.9491 - val_acc: 0.7570\n",
      "Epoch 613/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8674 - acc: 0.7897 - val_loss: 0.9476 - val_acc: 0.7590\n",
      "Epoch 614/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8679 - acc: 0.7886 - val_loss: 0.9470 - val_acc: 0.7560\n",
      "Epoch 615/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8665 - acc: 0.7903 - val_loss: 0.9589 - val_acc: 0.7420\n",
      "Epoch 616/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8673 - acc: 0.7900 - val_loss: 0.9462 - val_acc: 0.7630\n",
      "Epoch 617/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8671 - acc: 0.7880 - val_loss: 0.9541 - val_acc: 0.7480\n",
      "Epoch 618/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8670 - acc: 0.7889 - val_loss: 0.9551 - val_acc: 0.7440\n",
      "Epoch 619/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8664 - acc: 0.7896 - val_loss: 0.9480 - val_acc: 0.7600\n",
      "Epoch 620/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8655 - acc: 0.7909 - val_loss: 0.9568 - val_acc: 0.7470\n",
      "Epoch 621/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8682 - acc: 0.7897 - val_loss: 0.9548 - val_acc: 0.7520\n",
      "Epoch 622/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8666 - acc: 0.7891 - val_loss: 0.9507 - val_acc: 0.7540\n",
      "Epoch 623/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8652 - acc: 0.7893 - val_loss: 0.9492 - val_acc: 0.7580\n",
      "Epoch 624/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8649 - acc: 0.7893 - val_loss: 0.9457 - val_acc: 0.7560\n",
      "Epoch 625/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8651 - acc: 0.7900 - val_loss: 0.9440 - val_acc: 0.7550\n",
      "Epoch 626/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8646 - acc: 0.7906 - val_loss: 0.9523 - val_acc: 0.7580\n",
      "Epoch 627/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.8650 - acc: 0.7917 - val_loss: 0.9503 - val_acc: 0.7540\n",
      "Epoch 628/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8635 - acc: 0.7887 - val_loss: 0.9483 - val_acc: 0.7530\n",
      "Epoch 629/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8645 - acc: 0.7909 - val_loss: 0.9547 - val_acc: 0.7560\n",
      "Epoch 630/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8659 - acc: 0.7916 - val_loss: 0.9613 - val_acc: 0.7460\n",
      "Epoch 631/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8638 - acc: 0.7901 - val_loss: 0.9451 - val_acc: 0.7570\n",
      "Epoch 632/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8640 - acc: 0.7913 - val_loss: 0.9607 - val_acc: 0.7440\n",
      "Epoch 633/1000\n",
      "7000/7000 [==============================] - 1s 96us/step - loss: 0.8645 - acc: 0.7916 - val_loss: 0.9560 - val_acc: 0.7330\n",
      "Epoch 634/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8628 - acc: 0.7897 - val_loss: 0.9633 - val_acc: 0.7430\n",
      "Epoch 635/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8642 - acc: 0.7913 - val_loss: 0.9470 - val_acc: 0.7540\n",
      "Epoch 636/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8634 - acc: 0.7914 - val_loss: 0.9459 - val_acc: 0.7580\n",
      "Epoch 637/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8619 - acc: 0.7906 - val_loss: 0.9467 - val_acc: 0.7500\n",
      "Epoch 638/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8620 - acc: 0.7897 - val_loss: 0.9436 - val_acc: 0.7590\n",
      "Epoch 639/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8608 - acc: 0.7894 - val_loss: 0.9459 - val_acc: 0.7560\n",
      "Epoch 640/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8621 - acc: 0.7917 - val_loss: 0.9484 - val_acc: 0.7430\n",
      "Epoch 641/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8607 - acc: 0.7927 - val_loss: 0.9571 - val_acc: 0.7530\n",
      "Epoch 642/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8620 - acc: 0.7917 - val_loss: 0.9569 - val_acc: 0.7520\n",
      "Epoch 643/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8620 - acc: 0.7907 - val_loss: 0.9477 - val_acc: 0.7510\n",
      "Epoch 644/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8607 - acc: 0.7900 - val_loss: 0.9445 - val_acc: 0.7550\n",
      "Epoch 645/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8599 - acc: 0.7907 - val_loss: 0.9434 - val_acc: 0.7570\n",
      "Epoch 646/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8601 - acc: 0.7920 - val_loss: 0.9497 - val_acc: 0.7530\n",
      "Epoch 647/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8604 - acc: 0.7916 - val_loss: 0.9557 - val_acc: 0.7500\n",
      "Epoch 648/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8598 - acc: 0.7926 - val_loss: 0.9453 - val_acc: 0.7510\n",
      "Epoch 649/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8586 - acc: 0.7923 - val_loss: 0.9460 - val_acc: 0.7580\n",
      "Epoch 650/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8597 - acc: 0.7923 - val_loss: 0.9443 - val_acc: 0.7540\n",
      "Epoch 651/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8597 - acc: 0.7917 - val_loss: 0.9536 - val_acc: 0.7470\n",
      "Epoch 652/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8599 - acc: 0.7911 - val_loss: 0.9416 - val_acc: 0.7500\n",
      "Epoch 653/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8584 - acc: 0.7923 - val_loss: 0.9435 - val_acc: 0.7510\n",
      "Epoch 654/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8584 - acc: 0.7936 - val_loss: 0.9565 - val_acc: 0.7440\n",
      "Epoch 655/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8587 - acc: 0.7927 - val_loss: 0.9420 - val_acc: 0.7540\n",
      "Epoch 656/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8581 - acc: 0.7927 - val_loss: 0.9464 - val_acc: 0.7480\n",
      "Epoch 657/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8577 - acc: 0.7911 - val_loss: 0.9403 - val_acc: 0.7560\n",
      "Epoch 658/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8576 - acc: 0.7923 - val_loss: 0.9436 - val_acc: 0.7580\n",
      "Epoch 659/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.8578 - acc: 0.7920 - val_loss: 0.9376 - val_acc: 0.7570\n",
      "Epoch 660/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8578 - acc: 0.7910 - val_loss: 0.9396 - val_acc: 0.7560\n",
      "Epoch 661/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8572 - acc: 0.7914 - val_loss: 0.9454 - val_acc: 0.7480\n",
      "Epoch 662/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8568 - acc: 0.7920 - val_loss: 0.9440 - val_acc: 0.7500\n",
      "Epoch 663/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8564 - acc: 0.7921 - val_loss: 0.9440 - val_acc: 0.7560\n",
      "Epoch 664/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8571 - acc: 0.7914 - val_loss: 0.9454 - val_acc: 0.7520\n",
      "Epoch 665/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8566 - acc: 0.7921 - val_loss: 0.9403 - val_acc: 0.7570\n",
      "Epoch 666/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8553 - acc: 0.7921 - val_loss: 0.9525 - val_acc: 0.7500\n",
      "Epoch 667/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8555 - acc: 0.7934 - val_loss: 0.9398 - val_acc: 0.7530\n",
      "Epoch 668/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8554 - acc: 0.7939 - val_loss: 0.9495 - val_acc: 0.7460\n",
      "Epoch 669/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8560 - acc: 0.7909 - val_loss: 0.9415 - val_acc: 0.7570\n",
      "Epoch 670/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8547 - acc: 0.7941 - val_loss: 0.9416 - val_acc: 0.7490\n",
      "Epoch 671/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8549 - acc: 0.7933 - val_loss: 0.9369 - val_acc: 0.7530\n",
      "Epoch 672/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8544 - acc: 0.7924 - val_loss: 0.9401 - val_acc: 0.7510\n",
      "Epoch 673/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8534 - acc: 0.7941 - val_loss: 0.9430 - val_acc: 0.7490\n",
      "Epoch 674/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.8549 - acc: 0.7913 - val_loss: 0.9408 - val_acc: 0.7520\n",
      "Epoch 675/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8541 - acc: 0.7926 - val_loss: 0.9385 - val_acc: 0.7580\n",
      "Epoch 676/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8532 - acc: 0.7940 - val_loss: 0.9495 - val_acc: 0.7510\n",
      "Epoch 677/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8542 - acc: 0.7926 - val_loss: 0.9374 - val_acc: 0.7560\n",
      "Epoch 678/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8535 - acc: 0.7927 - val_loss: 0.9356 - val_acc: 0.7560\n",
      "Epoch 679/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8526 - acc: 0.7937 - val_loss: 0.9374 - val_acc: 0.7510\n",
      "Epoch 680/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8535 - acc: 0.7933 - val_loss: 0.9400 - val_acc: 0.7540\n",
      "Epoch 681/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8516 - acc: 0.7944 - val_loss: 0.9403 - val_acc: 0.7590\n",
      "Epoch 682/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8526 - acc: 0.7931 - val_loss: 0.9571 - val_acc: 0.7430\n",
      "Epoch 683/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8533 - acc: 0.7920 - val_loss: 0.9447 - val_acc: 0.7520\n",
      "Epoch 684/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8522 - acc: 0.7929 - val_loss: 0.9408 - val_acc: 0.7500\n",
      "Epoch 685/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8516 - acc: 0.7929 - val_loss: 0.9405 - val_acc: 0.7530\n",
      "Epoch 686/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8513 - acc: 0.7954 - val_loss: 0.9337 - val_acc: 0.7580\n",
      "Epoch 687/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8509 - acc: 0.7943 - val_loss: 0.9439 - val_acc: 0.7510\n",
      "Epoch 688/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8512 - acc: 0.7944 - val_loss: 0.9406 - val_acc: 0.7520\n",
      "Epoch 689/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8503 - acc: 0.7927 - val_loss: 0.9356 - val_acc: 0.7540\n",
      "Epoch 690/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8505 - acc: 0.7927 - val_loss: 0.9353 - val_acc: 0.7560\n",
      "Epoch 691/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8504 - acc: 0.7939 - val_loss: 0.9504 - val_acc: 0.7440\n",
      "Epoch 692/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8514 - acc: 0.7917 - val_loss: 0.9581 - val_acc: 0.7460\n",
      "Epoch 693/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8508 - acc: 0.7936 - val_loss: 0.9324 - val_acc: 0.7580\n",
      "Epoch 694/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8494 - acc: 0.7931 - val_loss: 0.9463 - val_acc: 0.7420\n",
      "Epoch 695/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8490 - acc: 0.7934 - val_loss: 0.9367 - val_acc: 0.7540\n",
      "Epoch 696/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8503 - acc: 0.7967 - val_loss: 0.9420 - val_acc: 0.7490\n",
      "Epoch 697/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8499 - acc: 0.7954 - val_loss: 0.9338 - val_acc: 0.7560\n",
      "Epoch 698/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8483 - acc: 0.7941 - val_loss: 0.9461 - val_acc: 0.7490\n",
      "Epoch 699/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8493 - acc: 0.7954 - val_loss: 0.9400 - val_acc: 0.7510\n",
      "Epoch 700/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8482 - acc: 0.7920 - val_loss: 0.9369 - val_acc: 0.7560\n",
      "Epoch 701/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8481 - acc: 0.7969 - val_loss: 0.9554 - val_acc: 0.7450\n",
      "Epoch 702/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8511 - acc: 0.7941 - val_loss: 0.9377 - val_acc: 0.7470\n",
      "Epoch 703/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8479 - acc: 0.7953 - val_loss: 0.9404 - val_acc: 0.7530\n",
      "Epoch 704/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8489 - acc: 0.7961 - val_loss: 0.9395 - val_acc: 0.7520\n",
      "Epoch 705/1000\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8471 - acc: 0.7939 - val_loss: 0.9312 - val_acc: 0.7530\n",
      "Epoch 706/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8465 - acc: 0.7947 - val_loss: 0.9462 - val_acc: 0.7390\n",
      "Epoch 707/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8476 - acc: 0.7964 - val_loss: 0.9333 - val_acc: 0.7560\n",
      "Epoch 708/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8469 - acc: 0.7951 - val_loss: 0.9475 - val_acc: 0.7550\n",
      "Epoch 709/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8476 - acc: 0.7940 - val_loss: 0.9339 - val_acc: 0.7550\n",
      "Epoch 710/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8466 - acc: 0.7966 - val_loss: 0.9364 - val_acc: 0.7570\n",
      "Epoch 711/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8461 - acc: 0.7964 - val_loss: 0.9338 - val_acc: 0.7500\n",
      "Epoch 712/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8462 - acc: 0.7959 - val_loss: 0.9395 - val_acc: 0.7490\n",
      "Epoch 713/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8459 - acc: 0.7963 - val_loss: 0.9503 - val_acc: 0.7460\n",
      "Epoch 714/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8479 - acc: 0.7934 - val_loss: 0.9405 - val_acc: 0.7560\n",
      "Epoch 715/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8445 - acc: 0.7954 - val_loss: 0.9364 - val_acc: 0.7590\n",
      "Epoch 716/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8449 - acc: 0.7981 - val_loss: 0.9347 - val_acc: 0.7490\n",
      "Epoch 717/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8444 - acc: 0.7973 - val_loss: 0.9335 - val_acc: 0.7490\n",
      "Epoch 718/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8453 - acc: 0.7969 - val_loss: 0.9305 - val_acc: 0.7550\n",
      "Epoch 719/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8443 - acc: 0.7969 - val_loss: 0.9347 - val_acc: 0.7520\n",
      "Epoch 720/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8462 - acc: 0.7949 - val_loss: 0.9373 - val_acc: 0.7520\n",
      "Epoch 721/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8454 - acc: 0.7957 - val_loss: 0.9376 - val_acc: 0.7550\n",
      "Epoch 722/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8439 - acc: 0.7970 - val_loss: 0.9336 - val_acc: 0.7550\n",
      "Epoch 723/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8441 - acc: 0.7964 - val_loss: 0.9414 - val_acc: 0.7520\n",
      "Epoch 724/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8438 - acc: 0.7954 - val_loss: 0.9338 - val_acc: 0.7530\n",
      "Epoch 725/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8440 - acc: 0.7977 - val_loss: 0.9373 - val_acc: 0.7560\n",
      "Epoch 726/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8428 - acc: 0.7977 - val_loss: 0.9349 - val_acc: 0.7510\n",
      "Epoch 727/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8435 - acc: 0.7961 - val_loss: 0.9490 - val_acc: 0.7430\n",
      "Epoch 728/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8429 - acc: 0.7964 - val_loss: 0.9304 - val_acc: 0.7560\n",
      "Epoch 729/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8434 - acc: 0.7967 - val_loss: 0.9590 - val_acc: 0.7480\n",
      "Epoch 730/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8428 - acc: 0.7950 - val_loss: 0.9274 - val_acc: 0.7560\n",
      "Epoch 731/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8421 - acc: 0.7996 - val_loss: 0.9449 - val_acc: 0.7430\n",
      "Epoch 732/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8417 - acc: 0.7994 - val_loss: 0.9514 - val_acc: 0.7520\n",
      "Epoch 733/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.8427 - acc: 0.7971 - val_loss: 0.9329 - val_acc: 0.7560\n",
      "Epoch 734/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8431 - acc: 0.7977 - val_loss: 0.9429 - val_acc: 0.7570\n",
      "Epoch 735/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8433 - acc: 0.7961 - val_loss: 0.9340 - val_acc: 0.7500\n",
      "Epoch 736/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8416 - acc: 0.7980 - val_loss: 0.9367 - val_acc: 0.7490\n",
      "Epoch 737/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8404 - acc: 0.7976 - val_loss: 0.9372 - val_acc: 0.7480\n",
      "Epoch 738/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8411 - acc: 0.7991 - val_loss: 0.9333 - val_acc: 0.7510\n",
      "Epoch 739/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8413 - acc: 0.7957 - val_loss: 0.9292 - val_acc: 0.7520\n",
      "Epoch 740/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8413 - acc: 0.7970 - val_loss: 0.9331 - val_acc: 0.7530\n",
      "Epoch 741/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8395 - acc: 0.7980 - val_loss: 0.9343 - val_acc: 0.7560\n",
      "Epoch 742/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8410 - acc: 0.7971 - val_loss: 0.9235 - val_acc: 0.7550\n",
      "Epoch 743/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8399 - acc: 0.7974 - val_loss: 0.9292 - val_acc: 0.7480\n",
      "Epoch 744/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8396 - acc: 0.7984 - val_loss: 0.9550 - val_acc: 0.7460\n",
      "Epoch 745/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8412 - acc: 0.7973 - val_loss: 0.9306 - val_acc: 0.7500\n",
      "Epoch 746/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8387 - acc: 0.7986 - val_loss: 0.9281 - val_acc: 0.7560\n",
      "Epoch 747/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8402 - acc: 0.7956 - val_loss: 0.9290 - val_acc: 0.7560\n",
      "Epoch 748/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8394 - acc: 0.7966 - val_loss: 0.9385 - val_acc: 0.7520\n",
      "Epoch 749/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8382 - acc: 0.7993 - val_loss: 0.9295 - val_acc: 0.7510\n",
      "Epoch 750/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8393 - acc: 0.7996 - val_loss: 0.9472 - val_acc: 0.7470\n",
      "Epoch 751/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8391 - acc: 0.7974 - val_loss: 0.9271 - val_acc: 0.7590\n",
      "Epoch 752/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8386 - acc: 0.7977 - val_loss: 0.9274 - val_acc: 0.7500\n",
      "Epoch 753/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8380 - acc: 0.7994 - val_loss: 0.9302 - val_acc: 0.7560\n",
      "Epoch 754/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8391 - acc: 0.7987 - val_loss: 0.9407 - val_acc: 0.7520\n",
      "Epoch 755/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8385 - acc: 0.7989 - val_loss: 0.9371 - val_acc: 0.7530\n",
      "Epoch 756/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8377 - acc: 0.7959 - val_loss: 0.9322 - val_acc: 0.7500\n",
      "Epoch 757/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.8380 - acc: 0.7986 - val_loss: 0.9305 - val_acc: 0.7560\n",
      "Epoch 758/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8370 - acc: 0.8000 - val_loss: 0.9419 - val_acc: 0.7450\n",
      "Epoch 759/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8375 - acc: 0.7976 - val_loss: 0.9270 - val_acc: 0.7530\n",
      "Epoch 760/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8367 - acc: 0.8001 - val_loss: 0.9321 - val_acc: 0.7500\n",
      "Epoch 761/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8375 - acc: 0.7973 - val_loss: 0.9371 - val_acc: 0.7500\n",
      "Epoch 762/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8371 - acc: 0.7976 - val_loss: 0.9395 - val_acc: 0.7480\n",
      "Epoch 763/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8367 - acc: 0.7987 - val_loss: 0.9369 - val_acc: 0.7520\n",
      "Epoch 764/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8371 - acc: 0.8001 - val_loss: 0.9383 - val_acc: 0.7490\n",
      "Epoch 765/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8351 - acc: 0.8020 - val_loss: 0.9234 - val_acc: 0.7540\n",
      "Epoch 766/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8369 - acc: 0.7991 - val_loss: 0.9260 - val_acc: 0.7540\n",
      "Epoch 767/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8351 - acc: 0.8014 - val_loss: 0.9464 - val_acc: 0.7430\n",
      "Epoch 768/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8358 - acc: 0.8003 - val_loss: 0.9365 - val_acc: 0.7530\n",
      "Epoch 769/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8347 - acc: 0.7996 - val_loss: 0.9368 - val_acc: 0.7510\n",
      "Epoch 770/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8346 - acc: 0.8004 - val_loss: 0.9317 - val_acc: 0.7580\n",
      "Epoch 771/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8354 - acc: 0.8009 - val_loss: 0.9315 - val_acc: 0.7550\n",
      "Epoch 772/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8350 - acc: 0.8006 - val_loss: 0.9255 - val_acc: 0.7580\n",
      "Epoch 773/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8341 - acc: 0.8003 - val_loss: 0.9395 - val_acc: 0.7480\n",
      "Epoch 774/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8347 - acc: 0.7991 - val_loss: 0.9234 - val_acc: 0.7600\n",
      "Epoch 775/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8346 - acc: 0.8006 - val_loss: 0.9385 - val_acc: 0.7520\n",
      "Epoch 776/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8343 - acc: 0.8010 - val_loss: 0.9352 - val_acc: 0.7500\n",
      "Epoch 777/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8341 - acc: 0.8010 - val_loss: 0.9197 - val_acc: 0.7560\n",
      "Epoch 778/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8333 - acc: 0.8019 - val_loss: 0.9345 - val_acc: 0.7560\n",
      "Epoch 779/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8349 - acc: 0.8017 - val_loss: 0.9207 - val_acc: 0.7570\n",
      "Epoch 780/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8340 - acc: 0.8020 - val_loss: 0.9305 - val_acc: 0.7540\n",
      "Epoch 781/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8327 - acc: 0.8026 - val_loss: 0.9225 - val_acc: 0.7580\n",
      "Epoch 782/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8337 - acc: 0.8023 - val_loss: 0.9228 - val_acc: 0.7480\n",
      "Epoch 783/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8322 - acc: 0.8014 - val_loss: 0.9215 - val_acc: 0.7610\n",
      "Epoch 784/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8330 - acc: 0.7993 - val_loss: 0.9311 - val_acc: 0.7530\n",
      "Epoch 785/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8318 - acc: 0.8006 - val_loss: 0.9358 - val_acc: 0.7510\n",
      "Epoch 786/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8324 - acc: 0.8033 - val_loss: 0.9315 - val_acc: 0.7560\n",
      "Epoch 787/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8324 - acc: 0.8004 - val_loss: 0.9245 - val_acc: 0.7580\n",
      "Epoch 788/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8316 - acc: 0.7999 - val_loss: 0.9258 - val_acc: 0.7590\n",
      "Epoch 789/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8306 - acc: 0.8023 - val_loss: 0.9272 - val_acc: 0.7540\n",
      "Epoch 790/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8309 - acc: 0.8013 - val_loss: 0.9206 - val_acc: 0.7550\n",
      "Epoch 791/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8323 - acc: 0.8016 - val_loss: 0.9245 - val_acc: 0.7550\n",
      "Epoch 792/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8321 - acc: 0.8016 - val_loss: 0.9322 - val_acc: 0.7540\n",
      "Epoch 793/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8310 - acc: 0.7983 - val_loss: 0.9256 - val_acc: 0.7530\n",
      "Epoch 794/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8302 - acc: 0.8013 - val_loss: 0.9312 - val_acc: 0.7560\n",
      "Epoch 795/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8302 - acc: 0.8021 - val_loss: 0.9333 - val_acc: 0.7540\n",
      "Epoch 796/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8308 - acc: 0.8027 - val_loss: 0.9272 - val_acc: 0.7620\n",
      "Epoch 797/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.8307 - acc: 0.8024 - val_loss: 0.9203 - val_acc: 0.7550\n",
      "Epoch 798/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8295 - acc: 0.8020 - val_loss: 0.9303 - val_acc: 0.7570\n",
      "Epoch 799/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8292 - acc: 0.8040 - val_loss: 0.9198 - val_acc: 0.7580\n",
      "Epoch 800/1000\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 0.8295 - acc: 0.8050 - val_loss: 0.9376 - val_acc: 0.7570\n",
      "Epoch 801/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.8301 - acc: 0.8010 - val_loss: 0.9264 - val_acc: 0.7540\n",
      "Epoch 802/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8293 - acc: 0.8036 - val_loss: 0.9233 - val_acc: 0.7530\n",
      "Epoch 803/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8286 - acc: 0.8029 - val_loss: 0.9220 - val_acc: 0.7520\n",
      "Epoch 804/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8285 - acc: 0.8047 - val_loss: 0.9397 - val_acc: 0.7520\n",
      "Epoch 805/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8303 - acc: 0.8024 - val_loss: 0.9199 - val_acc: 0.7590\n",
      "Epoch 806/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.8287 - acc: 0.8021 - val_loss: 0.9185 - val_acc: 0.7560\n",
      "Epoch 807/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8280 - acc: 0.8024 - val_loss: 0.9290 - val_acc: 0.7560\n",
      "Epoch 808/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8285 - acc: 0.8021 - val_loss: 0.9595 - val_acc: 0.7440\n",
      "Epoch 809/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8296 - acc: 0.8030 - val_loss: 0.9341 - val_acc: 0.7530\n",
      "Epoch 810/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8279 - acc: 0.8053 - val_loss: 0.9233 - val_acc: 0.7560\n",
      "Epoch 811/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8277 - acc: 0.8033 - val_loss: 0.9249 - val_acc: 0.7570\n",
      "Epoch 812/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8280 - acc: 0.8004 - val_loss: 0.9304 - val_acc: 0.7580\n",
      "Epoch 813/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8269 - acc: 0.8040 - val_loss: 0.9313 - val_acc: 0.7600\n",
      "Epoch 814/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8283 - acc: 0.8029 - val_loss: 0.9174 - val_acc: 0.7600\n",
      "Epoch 815/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8277 - acc: 0.8053 - val_loss: 0.9274 - val_acc: 0.7660\n",
      "Epoch 816/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8273 - acc: 0.8027 - val_loss: 0.9214 - val_acc: 0.7580\n",
      "Epoch 817/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8272 - acc: 0.8041 - val_loss: 0.9292 - val_acc: 0.7550\n",
      "Epoch 818/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8262 - acc: 0.8033 - val_loss: 0.9426 - val_acc: 0.7470\n",
      "Epoch 819/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8252 - acc: 0.8036 - val_loss: 0.9237 - val_acc: 0.7590\n",
      "Epoch 820/1000\n",
      "7000/7000 [==============================] - 1s 100us/step - loss: 0.8254 - acc: 0.8030 - val_loss: 0.9217 - val_acc: 0.7550\n",
      "Epoch 821/1000\n",
      "7000/7000 [==============================] - 1s 98us/step - loss: 0.8259 - acc: 0.8046 - val_loss: 0.9232 - val_acc: 0.7560\n",
      "Epoch 822/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8251 - acc: 0.8051 - val_loss: 0.9296 - val_acc: 0.7580\n",
      "Epoch 823/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8250 - acc: 0.8040 - val_loss: 0.9228 - val_acc: 0.7590\n",
      "Epoch 824/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8251 - acc: 0.8053 - val_loss: 0.9189 - val_acc: 0.7580\n",
      "Epoch 825/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8239 - acc: 0.8047 - val_loss: 0.9342 - val_acc: 0.7520\n",
      "Epoch 826/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8249 - acc: 0.8044 - val_loss: 0.9202 - val_acc: 0.7560\n",
      "Epoch 827/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8246 - acc: 0.8061 - val_loss: 0.9168 - val_acc: 0.7600\n",
      "Epoch 828/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8244 - acc: 0.8043 - val_loss: 0.9266 - val_acc: 0.7630\n",
      "Epoch 829/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8254 - acc: 0.8046 - val_loss: 0.9174 - val_acc: 0.7590\n",
      "Epoch 830/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8245 - acc: 0.8041 - val_loss: 0.9160 - val_acc: 0.7580\n",
      "Epoch 831/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8230 - acc: 0.8049 - val_loss: 0.9123 - val_acc: 0.7600\n",
      "Epoch 832/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8229 - acc: 0.8047 - val_loss: 0.9257 - val_acc: 0.7660\n",
      "Epoch 833/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8258 - acc: 0.8051 - val_loss: 0.9138 - val_acc: 0.7580\n",
      "Epoch 834/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8227 - acc: 0.8039 - val_loss: 0.9254 - val_acc: 0.7530\n",
      "Epoch 835/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8238 - acc: 0.8063 - val_loss: 0.9358 - val_acc: 0.7510\n",
      "Epoch 836/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8240 - acc: 0.8059 - val_loss: 0.9251 - val_acc: 0.7590\n",
      "Epoch 837/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8227 - acc: 0.8076 - val_loss: 0.9220 - val_acc: 0.7540\n",
      "Epoch 838/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8228 - acc: 0.8054 - val_loss: 0.9146 - val_acc: 0.7580\n",
      "Epoch 839/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8225 - acc: 0.8060 - val_loss: 0.9326 - val_acc: 0.7560\n",
      "Epoch 840/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8225 - acc: 0.8053 - val_loss: 0.9461 - val_acc: 0.7580\n",
      "Epoch 841/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8225 - acc: 0.8066 - val_loss: 0.9227 - val_acc: 0.7560\n",
      "Epoch 842/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8228 - acc: 0.8077 - val_loss: 0.9490 - val_acc: 0.7550\n",
      "Epoch 843/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8226 - acc: 0.8047 - val_loss: 0.9161 - val_acc: 0.7610\n",
      "Epoch 844/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8209 - acc: 0.8074 - val_loss: 0.9156 - val_acc: 0.7590\n",
      "Epoch 845/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8221 - acc: 0.8053 - val_loss: 0.9363 - val_acc: 0.7560\n",
      "Epoch 846/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8226 - acc: 0.8069 - val_loss: 0.9295 - val_acc: 0.7520\n",
      "Epoch 847/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8217 - acc: 0.8067 - val_loss: 0.9190 - val_acc: 0.7530\n",
      "Epoch 848/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8199 - acc: 0.8084 - val_loss: 0.9190 - val_acc: 0.7580\n",
      "Epoch 849/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8207 - acc: 0.8071 - val_loss: 0.9232 - val_acc: 0.7570\n",
      "Epoch 850/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8214 - acc: 0.8056 - val_loss: 0.9169 - val_acc: 0.7560\n",
      "Epoch 851/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8204 - acc: 0.8073 - val_loss: 0.9266 - val_acc: 0.7550\n",
      "Epoch 852/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8211 - acc: 0.8037 - val_loss: 0.9220 - val_acc: 0.7640\n",
      "Epoch 853/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8204 - acc: 0.8076 - val_loss: 0.9338 - val_acc: 0.7460\n",
      "Epoch 854/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8210 - acc: 0.8053 - val_loss: 0.9161 - val_acc: 0.7580\n",
      "Epoch 855/1000\n",
      "7000/7000 [==============================] - 1s 94us/step - loss: 0.8192 - acc: 0.8073 - val_loss: 0.9229 - val_acc: 0.7640\n",
      "Epoch 856/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8205 - acc: 0.8073 - val_loss: 0.9181 - val_acc: 0.7670\n",
      "Epoch 857/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8202 - acc: 0.8077 - val_loss: 0.9174 - val_acc: 0.7570\n",
      "Epoch 858/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8206 - acc: 0.8041 - val_loss: 0.9194 - val_acc: 0.7600\n",
      "Epoch 859/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8189 - acc: 0.8069 - val_loss: 0.9139 - val_acc: 0.7610\n",
      "Epoch 860/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8190 - acc: 0.8076 - val_loss: 0.9204 - val_acc: 0.7550\n",
      "Epoch 861/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8187 - acc: 0.8059 - val_loss: 0.9131 - val_acc: 0.7560\n",
      "Epoch 862/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8173 - acc: 0.8079 - val_loss: 0.9249 - val_acc: 0.7590\n",
      "Epoch 863/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8182 - acc: 0.8060 - val_loss: 0.9350 - val_acc: 0.7570\n",
      "Epoch 864/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8181 - acc: 0.8060 - val_loss: 0.9189 - val_acc: 0.7670\n",
      "Epoch 865/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8173 - acc: 0.8107 - val_loss: 0.9286 - val_acc: 0.7520\n",
      "Epoch 866/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8197 - acc: 0.8054 - val_loss: 0.9224 - val_acc: 0.7590\n",
      "Epoch 867/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8184 - acc: 0.8077 - val_loss: 0.9319 - val_acc: 0.7560\n",
      "Epoch 868/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8175 - acc: 0.8071 - val_loss: 0.9255 - val_acc: 0.7650\n",
      "Epoch 869/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8172 - acc: 0.8079 - val_loss: 0.9185 - val_acc: 0.7650\n",
      "Epoch 870/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8169 - acc: 0.8086 - val_loss: 0.9297 - val_acc: 0.7590\n",
      "Epoch 871/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8172 - acc: 0.8067 - val_loss: 0.9239 - val_acc: 0.7550\n",
      "Epoch 872/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.8162 - acc: 0.8094 - val_loss: 0.9429 - val_acc: 0.7540\n",
      "Epoch 873/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.8172 - acc: 0.8049 - val_loss: 0.9313 - val_acc: 0.7590\n",
      "Epoch 874/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8169 - acc: 0.8079 - val_loss: 0.9102 - val_acc: 0.7600\n",
      "Epoch 875/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8158 - acc: 0.8109 - val_loss: 0.9115 - val_acc: 0.7680\n",
      "Epoch 876/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8171 - acc: 0.8087 - val_loss: 0.9177 - val_acc: 0.7650\n",
      "Epoch 877/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8170 - acc: 0.8086 - val_loss: 0.9126 - val_acc: 0.7690\n",
      "Epoch 878/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8161 - acc: 0.8087 - val_loss: 0.9372 - val_acc: 0.7490\n",
      "Epoch 879/1000\n",
      "7000/7000 [==============================] - 1s 93us/step - loss: 0.8181 - acc: 0.8090 - val_loss: 0.9229 - val_acc: 0.7580\n",
      "Epoch 880/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8154 - acc: 0.8089 - val_loss: 0.9239 - val_acc: 0.7620\n",
      "Epoch 881/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8163 - acc: 0.8071 - val_loss: 0.9094 - val_acc: 0.7600\n",
      "Epoch 882/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8139 - acc: 0.8100 - val_loss: 0.9224 - val_acc: 0.7560\n",
      "Epoch 883/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8166 - acc: 0.8103 - val_loss: 0.9218 - val_acc: 0.7610\n",
      "Epoch 884/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8149 - acc: 0.8081 - val_loss: 0.9159 - val_acc: 0.7570\n",
      "Epoch 885/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8134 - acc: 0.8081 - val_loss: 0.9201 - val_acc: 0.7620\n",
      "Epoch 886/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8161 - acc: 0.8077 - val_loss: 0.9158 - val_acc: 0.7620\n",
      "Epoch 887/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8152 - acc: 0.8087 - val_loss: 0.9422 - val_acc: 0.7470\n",
      "Epoch 888/1000\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8160 - acc: 0.8086 - val_loss: 0.9382 - val_acc: 0.7510\n",
      "Epoch 889/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8149 - acc: 0.8083 - val_loss: 0.9109 - val_acc: 0.7610\n",
      "Epoch 890/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8147 - acc: 0.8093 - val_loss: 0.9176 - val_acc: 0.7580\n",
      "Epoch 891/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8137 - acc: 0.8091 - val_loss: 0.9110 - val_acc: 0.7540\n",
      "Epoch 892/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8150 - acc: 0.8111 - val_loss: 0.9152 - val_acc: 0.7620\n",
      "Epoch 893/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8146 - acc: 0.8109 - val_loss: 0.9292 - val_acc: 0.7550\n",
      "Epoch 894/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8127 - acc: 0.8114 - val_loss: 0.9229 - val_acc: 0.7570\n",
      "Epoch 895/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8135 - acc: 0.8097 - val_loss: 0.9136 - val_acc: 0.7680\n",
      "Epoch 896/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8131 - acc: 0.8106 - val_loss: 0.9205 - val_acc: 0.7600\n",
      "Epoch 897/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8120 - acc: 0.8117 - val_loss: 0.9174 - val_acc: 0.7620\n",
      "Epoch 898/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8120 - acc: 0.8109 - val_loss: 0.9177 - val_acc: 0.7670\n",
      "Epoch 899/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8157 - acc: 0.8077 - val_loss: 0.9535 - val_acc: 0.7510\n",
      "Epoch 900/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8124 - acc: 0.8094 - val_loss: 0.9105 - val_acc: 0.7620\n",
      "Epoch 901/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8131 - acc: 0.8089 - val_loss: 0.9126 - val_acc: 0.7650\n",
      "Epoch 902/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8119 - acc: 0.8114 - val_loss: 0.9191 - val_acc: 0.7620\n",
      "Epoch 903/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8123 - acc: 0.8104 - val_loss: 0.9099 - val_acc: 0.7660\n",
      "Epoch 904/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8131 - acc: 0.8096 - val_loss: 0.9233 - val_acc: 0.7510\n",
      "Epoch 905/1000\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 0.8114 - acc: 0.8113 - val_loss: 0.9152 - val_acc: 0.7570\n",
      "Epoch 906/1000\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 0.8110 - acc: 0.8080 - val_loss: 0.9105 - val_acc: 0.7630\n",
      "Epoch 907/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8098 - acc: 0.8124 - val_loss: 0.9087 - val_acc: 0.7590\n",
      "Epoch 908/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8087 - acc: 0.8137 - val_loss: 0.9152 - val_acc: 0.7570\n",
      "Epoch 909/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8108 - acc: 0.8127 - val_loss: 0.9197 - val_acc: 0.7700\n",
      "Epoch 910/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8092 - acc: 0.8123 - val_loss: 0.9138 - val_acc: 0.7750\n",
      "Epoch 911/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8111 - acc: 0.8107 - val_loss: 0.9135 - val_acc: 0.7710\n",
      "Epoch 912/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8113 - acc: 0.8129 - val_loss: 0.9368 - val_acc: 0.7550\n",
      "Epoch 913/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8108 - acc: 0.8106 - val_loss: 0.9489 - val_acc: 0.7520\n",
      "Epoch 914/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8112 - acc: 0.8110 - val_loss: 0.9118 - val_acc: 0.7660A: 0s - loss: 0.8081 - acc: 0.81\n",
      "Epoch 915/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8100 - acc: 0.8120 - val_loss: 0.9089 - val_acc: 0.7590\n",
      "Epoch 916/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8108 - acc: 0.8119 - val_loss: 0.9112 - val_acc: 0.7660\n",
      "Epoch 917/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8091 - acc: 0.8126 - val_loss: 0.9113 - val_acc: 0.7590\n",
      "Epoch 918/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8086 - acc: 0.8119 - val_loss: 0.9180 - val_acc: 0.7650\n",
      "Epoch 919/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8087 - acc: 0.8129 - val_loss: 0.9308 - val_acc: 0.7650\n",
      "Epoch 920/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8083 - acc: 0.8131 - val_loss: 0.9115 - val_acc: 0.7620\n",
      "Epoch 921/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8113 - acc: 0.8121 - val_loss: 0.9131 - val_acc: 0.7610\n",
      "Epoch 922/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8073 - acc: 0.8101 - val_loss: 0.9158 - val_acc: 0.7690\n",
      "Epoch 923/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8090 - acc: 0.8136 - val_loss: 0.9536 - val_acc: 0.7530\n",
      "Epoch 924/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8102 - acc: 0.8114 - val_loss: 0.9058 - val_acc: 0.7640\n",
      "Epoch 925/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8085 - acc: 0.8123 - val_loss: 0.9106 - val_acc: 0.7740\n",
      "Epoch 926/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8106 - acc: 0.8129 - val_loss: 0.9106 - val_acc: 0.7710\n",
      "Epoch 927/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8099 - acc: 0.8130 - val_loss: 0.9213 - val_acc: 0.7600\n",
      "Epoch 928/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8064 - acc: 0.8121 - val_loss: 0.9131 - val_acc: 0.7660\n",
      "Epoch 929/1000\n",
      "7000/7000 [==============================] - 1s 97us/step - loss: 0.8084 - acc: 0.8163 - val_loss: 0.9195 - val_acc: 0.7540\n",
      "Epoch 930/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8070 - acc: 0.8150 - val_loss: 0.9221 - val_acc: 0.7580\n",
      "Epoch 931/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8081 - acc: 0.8147 - val_loss: 0.9147 - val_acc: 0.7630\n",
      "Epoch 932/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8057 - acc: 0.8124 - val_loss: 0.9120 - val_acc: 0.7700\n",
      "Epoch 933/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8074 - acc: 0.8111 - val_loss: 0.9215 - val_acc: 0.7690\n",
      "Epoch 934/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8068 - acc: 0.8140 - val_loss: 0.9054 - val_acc: 0.7670\n",
      "Epoch 935/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8049 - acc: 0.8157 - val_loss: 0.9100 - val_acc: 0.7720\n",
      "Epoch 936/1000\n",
      "7000/7000 [==============================] - 1s 95us/step - loss: 0.8055 - acc: 0.8147 - val_loss: 0.9132 - val_acc: 0.7580\n",
      "Epoch 937/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.8083 - acc: 0.8140 - val_loss: 0.9254 - val_acc: 0.7560\n",
      "Epoch 938/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8066 - acc: 0.8141 - val_loss: 0.9247 - val_acc: 0.7620\n",
      "Epoch 939/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8074 - acc: 0.8150 - val_loss: 0.9273 - val_acc: 0.7610\n",
      "Epoch 940/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8059 - acc: 0.8114 - val_loss: 0.9232 - val_acc: 0.7550\n",
      "Epoch 941/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8066 - acc: 0.8147 - val_loss: 0.9121 - val_acc: 0.7650\n",
      "Epoch 942/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8058 - acc: 0.8157 - val_loss: 0.9196 - val_acc: 0.7580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 943/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8040 - acc: 0.8157 - val_loss: 0.9312 - val_acc: 0.7530\n",
      "Epoch 944/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8063 - acc: 0.8144 - val_loss: 0.9082 - val_acc: 0.7650\n",
      "Epoch 945/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8046 - acc: 0.8169 - val_loss: 0.9088 - val_acc: 0.7700\n",
      "Epoch 946/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8048 - acc: 0.8150 - val_loss: 0.9265 - val_acc: 0.7640\n",
      "Epoch 947/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8061 - acc: 0.8134 - val_loss: 0.9079 - val_acc: 0.7720\n",
      "Epoch 948/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8038 - acc: 0.8181 - val_loss: 0.9347 - val_acc: 0.7630\n",
      "Epoch 949/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8049 - acc: 0.8133 - val_loss: 0.9098 - val_acc: 0.7750\n",
      "Epoch 950/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8029 - acc: 0.8147 - val_loss: 0.9468 - val_acc: 0.7550\n",
      "Epoch 951/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8052 - acc: 0.8146 - val_loss: 0.9130 - val_acc: 0.7700\n",
      "Epoch 952/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8038 - acc: 0.8147 - val_loss: 0.9076 - val_acc: 0.7720\n",
      "Epoch 953/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8024 - acc: 0.8166 - val_loss: 0.9120 - val_acc: 0.7610\n",
      "Epoch 954/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8035 - acc: 0.8164 - val_loss: 0.9062 - val_acc: 0.7720\n",
      "Epoch 955/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8020 - acc: 0.8136 - val_loss: 0.9087 - val_acc: 0.7600\n",
      "Epoch 956/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.8023 - acc: 0.8153 - val_loss: 0.9251 - val_acc: 0.7650\n",
      "Epoch 957/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8024 - acc: 0.8170 - val_loss: 0.9429 - val_acc: 0.7560\n",
      "Epoch 958/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8057 - acc: 0.8146 - val_loss: 0.9016 - val_acc: 0.7680\n",
      "Epoch 959/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8017 - acc: 0.8153 - val_loss: 0.9137 - val_acc: 0.7680\n",
      "Epoch 960/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8021 - acc: 0.8193 - val_loss: 0.9316 - val_acc: 0.7580\n",
      "Epoch 961/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8026 - acc: 0.8150 - val_loss: 0.9022 - val_acc: 0.7650\n",
      "Epoch 962/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8003 - acc: 0.8180 - val_loss: 0.9117 - val_acc: 0.7580\n",
      "Epoch 963/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8008 - acc: 0.8180 - val_loss: 0.9098 - val_acc: 0.7720\n",
      "Epoch 964/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8011 - acc: 0.8176 - val_loss: 0.9119 - val_acc: 0.7650\n",
      "Epoch 965/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8019 - acc: 0.8139 - val_loss: 0.9071 - val_acc: 0.7660\n",
      "Epoch 966/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.8024 - acc: 0.8167 - val_loss: 0.9137 - val_acc: 0.7650\n",
      "Epoch 967/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8018 - acc: 0.8157 - val_loss: 0.9098 - val_acc: 0.7750\n",
      "Epoch 968/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.8017 - acc: 0.8169 - val_loss: 0.9078 - val_acc: 0.7730\n",
      "Epoch 969/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.7996 - acc: 0.8193 - val_loss: 0.9219 - val_acc: 0.7620\n",
      "Epoch 970/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.7997 - acc: 0.8187 - val_loss: 0.9184 - val_acc: 0.7680\n",
      "Epoch 971/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8008 - acc: 0.8180 - val_loss: 0.9049 - val_acc: 0.7700\n",
      "Epoch 972/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.8003 - acc: 0.8174 - val_loss: 0.9222 - val_acc: 0.7600\n",
      "Epoch 973/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.7994 - acc: 0.8160 - val_loss: 0.9159 - val_acc: 0.7630\n",
      "Epoch 974/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8029 - acc: 0.8140 - val_loss: 0.9246 - val_acc: 0.7570\n",
      "Epoch 975/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.8012 - acc: 0.8180 - val_loss: 0.9057 - val_acc: 0.7730\n",
      "Epoch 976/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.7995 - acc: 0.8173 - val_loss: 0.9231 - val_acc: 0.7590\n",
      "Epoch 977/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.8003 - acc: 0.8180 - val_loss: 0.9091 - val_acc: 0.7730\n",
      "Epoch 978/1000\n",
      "7000/7000 [==============================] - 1s 92us/step - loss: 0.7989 - acc: 0.8169 - val_loss: 0.9319 - val_acc: 0.7540\n",
      "Epoch 979/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.7986 - acc: 0.8179 - val_loss: 0.9087 - val_acc: 0.7700\n",
      "Epoch 980/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.7981 - acc: 0.8183 - val_loss: 0.9256 - val_acc: 0.7580\n",
      "Epoch 981/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.7997 - acc: 0.8159 - val_loss: 0.9112 - val_acc: 0.7710\n",
      "Epoch 982/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.7977 - acc: 0.8190 - val_loss: 0.9364 - val_acc: 0.7480\n",
      "Epoch 983/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.7984 - acc: 0.8206 - val_loss: 0.9220 - val_acc: 0.7560\n",
      "Epoch 984/1000\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.7962 - acc: 0.820 - 1s 90us/step - loss: 0.7971 - acc: 0.8190 - val_loss: 0.9111 - val_acc: 0.7730\n",
      "Epoch 985/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.7974 - acc: 0.8201 - val_loss: 0.9086 - val_acc: 0.7680\n",
      "Epoch 986/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.7970 - acc: 0.8193 - val_loss: 0.9153 - val_acc: 0.7590\n",
      "Epoch 987/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.7965 - acc: 0.8179 - val_loss: 0.9111 - val_acc: 0.7680\n",
      "Epoch 988/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 0.7981 - acc: 0.8180 - val_loss: 0.9185 - val_acc: 0.7550\n",
      "Epoch 989/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.7974 - acc: 0.8161 - val_loss: 0.9063 - val_acc: 0.7700\n",
      "Epoch 990/1000\n",
      "7000/7000 [==============================] - 1s 91us/step - loss: 0.7977 - acc: 0.8189 - val_loss: 0.9011 - val_acc: 0.7680\n",
      "Epoch 991/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.7994 - acc: 0.8194 - val_loss: 0.9075 - val_acc: 0.7730\n",
      "Epoch 992/1000\n",
      "7000/7000 [==============================] - 1s 88us/step - loss: 0.7977 - acc: 0.8171 - val_loss: 0.8997 - val_acc: 0.7680\n",
      "Epoch 993/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.7982 - acc: 0.8210 - val_loss: 0.9057 - val_acc: 0.7650\n",
      "Epoch 994/1000\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.7983 - acc: 0.8163 - val_loss: 0.9063 - val_acc: 0.7740\n",
      "Epoch 995/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.7961 - acc: 0.8213 - val_loss: 0.9106 - val_acc: 0.7700\n",
      "Epoch 996/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.7941 - acc: 0.8197 - val_loss: 0.9170 - val_acc: 0.7640\n",
      "Epoch 997/1000\n",
      "7000/7000 [==============================] - 1s 87us/step - loss: 0.7983 - acc: 0.8194 - val_loss: 0.9191 - val_acc: 0.7670\n",
      "Epoch 998/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.7956 - acc: 0.8204 - val_loss: 0.9049 - val_acc: 0.7710\n",
      "Epoch 999/1000\n",
      "7000/7000 [==============================] - 1s 86us/step - loss: 0.7970 - acc: 0.8209 - val_loss: 0.9013 - val_acc: 0.7700\n",
      "Epoch 1000/1000\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 0.7948 - acc: 0.8190 - val_loss: 0.9075 - val_acc: 0.7710\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L1 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 75us/step\n",
      "2000/2000 [==============================] - 0s 75us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7966832892554147, 0.8211428572109767]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9052576532363892, 0.7725]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\IBM\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7000/7000 [==============================] - 2s 307us/step - loss: 1.9869 - acc: 0.1347 - val_loss: 1.9475 - val_acc: 0.1490\n",
      "Epoch 2/200\n",
      "7000/7000 [==============================] - 1s 127us/step - loss: 1.9603 - acc: 0.1491 - val_loss: 1.9353 - val_acc: 0.1690\n",
      "Epoch 3/200\n",
      "7000/7000 [==============================] - 1s 120us/step - loss: 1.9464 - acc: 0.1621 - val_loss: 1.9268 - val_acc: 0.1800\n",
      "Epoch 4/200\n",
      "7000/7000 [==============================] - 1s 125us/step - loss: 1.9356 - acc: 0.1809 - val_loss: 1.9187 - val_acc: 0.1790\n",
      "Epoch 5/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.9270 - acc: 0.1856 - val_loss: 1.9101 - val_acc: 0.2060\n",
      "Epoch 6/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.9200 - acc: 0.1937 - val_loss: 1.9009 - val_acc: 0.2230\n",
      "Epoch 7/200\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 1.9126 - acc: 0.204 - 1s 118us/step - loss: 1.9125 - acc: 0.2046 - val_loss: 1.8916 - val_acc: 0.2340\n",
      "Epoch 8/200\n",
      "7000/7000 [==============================] - 1s 131us/step - loss: 1.9021 - acc: 0.2109 - val_loss: 1.8811 - val_acc: 0.2430\n",
      "Epoch 9/200\n",
      "7000/7000 [==============================] - 1s 125us/step - loss: 1.8890 - acc: 0.2230 - val_loss: 1.8692 - val_acc: 0.2500\n",
      "Epoch 10/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 1.8813 - acc: 0.2331 - val_loss: 1.8563 - val_acc: 0.2590\n",
      "Epoch 11/200\n",
      "7000/7000 [==============================] - 1s 124us/step - loss: 1.8757 - acc: 0.2317 - val_loss: 1.8444 - val_acc: 0.2720\n",
      "Epoch 12/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 1.8678 - acc: 0.2330 - val_loss: 1.8324 - val_acc: 0.2800\n",
      "Epoch 13/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.8546 - acc: 0.2479 - val_loss: 1.8178 - val_acc: 0.2920\n",
      "Epoch 14/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 1.8441 - acc: 0.2594 - val_loss: 1.8024 - val_acc: 0.3100\n",
      "Epoch 15/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.8319 - acc: 0.2617 - val_loss: 1.7864 - val_acc: 0.3220\n",
      "Epoch 16/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 1.8163 - acc: 0.2783 - val_loss: 1.7690 - val_acc: 0.3420\n",
      "Epoch 17/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.8068 - acc: 0.2783 - val_loss: 1.7515 - val_acc: 0.3800\n",
      "Epoch 18/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.7909 - acc: 0.2921 - val_loss: 1.7290 - val_acc: 0.3910\n",
      "Epoch 19/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.7784 - acc: 0.3061 - val_loss: 1.7096 - val_acc: 0.4240\n",
      "Epoch 20/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.7635 - acc: 0.3133 - val_loss: 1.6884 - val_acc: 0.4320\n",
      "Epoch 21/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.7527 - acc: 0.3176 - val_loss: 1.6683 - val_acc: 0.4490\n",
      "Epoch 22/200\n",
      "7000/7000 [==============================] - 1s 124us/step - loss: 1.7303 - acc: 0.3301 - val_loss: 1.6464 - val_acc: 0.4750\n",
      "Epoch 23/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.7220 - acc: 0.3320 - val_loss: 1.6271 - val_acc: 0.4950\n",
      "Epoch 24/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.6856 - acc: 0.3567 - val_loss: 1.6029 - val_acc: 0.5140\n",
      "Epoch 25/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.6807 - acc: 0.3590 - val_loss: 1.5791 - val_acc: 0.5230\n",
      "Epoch 26/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.6748 - acc: 0.3637 - val_loss: 1.5577 - val_acc: 0.5430\n",
      "Epoch 27/200\n",
      "7000/7000 [==============================] - 1s 121us/step - loss: 1.6517 - acc: 0.3801 - val_loss: 1.5347 - val_acc: 0.5500\n",
      "Epoch 28/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.6355 - acc: 0.3770 - val_loss: 1.5138 - val_acc: 0.5550\n",
      "Epoch 29/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.6222 - acc: 0.3860 - val_loss: 1.4914 - val_acc: 0.5710\n",
      "Epoch 30/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.6062 - acc: 0.3967 - val_loss: 1.4687 - val_acc: 0.5730\n",
      "Epoch 31/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.5980 - acc: 0.3989 - val_loss: 1.4521 - val_acc: 0.5930\n",
      "Epoch 32/200\n",
      "7000/7000 [==============================] - 1s 121us/step - loss: 1.5688 - acc: 0.4114 - val_loss: 1.4278 - val_acc: 0.6040\n",
      "Epoch 33/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.5438 - acc: 0.4244 - val_loss: 1.4051 - val_acc: 0.6120\n",
      "Epoch 34/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.5436 - acc: 0.4207 - val_loss: 1.3858 - val_acc: 0.6180\n",
      "Epoch 35/200\n",
      "7000/7000 [==============================] - 1s 120us/step - loss: 1.5310 - acc: 0.4323 - val_loss: 1.3694 - val_acc: 0.6270\n",
      "Epoch 36/200\n",
      "7000/7000 [==============================] - 1s 122us/step - loss: 1.5138 - acc: 0.4391 - val_loss: 1.3475 - val_acc: 0.6320\n",
      "Epoch 37/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.4935 - acc: 0.4479 - val_loss: 1.3276 - val_acc: 0.6360\n",
      "Epoch 38/200\n",
      "7000/7000 [==============================] - 1s 120us/step - loss: 1.4838 - acc: 0.4457 - val_loss: 1.3106 - val_acc: 0.6400\n",
      "Epoch 39/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.4603 - acc: 0.4561 - val_loss: 1.2901 - val_acc: 0.6430\n",
      "Epoch 40/200\n",
      "7000/7000 [==============================] - 1s 131us/step - loss: 1.4456 - acc: 0.4657 - val_loss: 1.2702 - val_acc: 0.6470\n",
      "Epoch 41/200\n",
      "7000/7000 [==============================] - 1s 120us/step - loss: 1.4461 - acc: 0.4623 - val_loss: 1.2553 - val_acc: 0.6550\n",
      "Epoch 42/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.4268 - acc: 0.4686 - val_loss: 1.2395 - val_acc: 0.6550\n",
      "Epoch 43/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.4131 - acc: 0.4777 - val_loss: 1.2220 - val_acc: 0.6650\n",
      "Epoch 44/200\n",
      "7000/7000 [==============================] - 1s 120us/step - loss: 1.4071 - acc: 0.4721 - val_loss: 1.2063 - val_acc: 0.6590\n",
      "Epoch 45/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.4019 - acc: 0.4729 - val_loss: 1.1914 - val_acc: 0.6620\n",
      "Epoch 46/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.3808 - acc: 0.4783 - val_loss: 1.1779 - val_acc: 0.6700\n",
      "Epoch 47/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.3609 - acc: 0.4929 - val_loss: 1.1598 - val_acc: 0.6720\n",
      "Epoch 48/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.3470 - acc: 0.4937 - val_loss: 1.1464 - val_acc: 0.6750\n",
      "Epoch 49/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.3419 - acc: 0.5006 - val_loss: 1.1337 - val_acc: 0.6790\n",
      "Epoch 50/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.3376 - acc: 0.5040 - val_loss: 1.1227 - val_acc: 0.6820\n",
      "Epoch 51/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.3271 - acc: 0.5013 - val_loss: 1.1081 - val_acc: 0.6880\n",
      "Epoch 52/200\n",
      "7000/7000 [==============================] - 1s 121us/step - loss: 1.3192 - acc: 0.5120 - val_loss: 1.0959 - val_acc: 0.6880\n",
      "Epoch 53/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.3071 - acc: 0.5059 - val_loss: 1.0847 - val_acc: 0.6910\n",
      "Epoch 54/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 1.2837 - acc: 0.5217 - val_loss: 1.0704 - val_acc: 0.6900\n",
      "Epoch 55/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 1.2878 - acc: 0.5157 - val_loss: 1.0599 - val_acc: 0.6910\n",
      "Epoch 56/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.2775 - acc: 0.5274 - val_loss: 1.0502 - val_acc: 0.6930\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.2742 - acc: 0.5229 - val_loss: 1.0390 - val_acc: 0.7030\n",
      "Epoch 58/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.2593 - acc: 0.5223 - val_loss: 1.0266 - val_acc: 0.7030\n",
      "Epoch 59/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.2461 - acc: 0.5329 - val_loss: 1.0165 - val_acc: 0.7030\n",
      "Epoch 60/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.2459 - acc: 0.5350 - val_loss: 1.0059 - val_acc: 0.7010\n",
      "Epoch 61/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.2269 - acc: 0.5416 - val_loss: 0.9983 - val_acc: 0.7050\n",
      "Epoch 62/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.2311 - acc: 0.5407 - val_loss: 0.9879 - val_acc: 0.7020\n",
      "Epoch 63/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 1.2173 - acc: 0.5497 - val_loss: 0.9793 - val_acc: 0.7060\n",
      "Epoch 64/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.2104 - acc: 0.5496 - val_loss: 0.9712 - val_acc: 0.7080\n",
      "Epoch 65/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.2003 - acc: 0.5567 - val_loss: 0.9598 - val_acc: 0.7090\n",
      "Epoch 66/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.1957 - acc: 0.5453 - val_loss: 0.9549 - val_acc: 0.7110\n",
      "Epoch 67/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 1.1823 - acc: 0.5653 - val_loss: 0.9467 - val_acc: 0.7120\n",
      "Epoch 68/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 1.1705 - acc: 0.5587 - val_loss: 0.9388 - val_acc: 0.7100\n",
      "Epoch 69/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.1714 - acc: 0.5587 - val_loss: 0.9317 - val_acc: 0.7150\n",
      "Epoch 70/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.1671 - acc: 0.5676 - val_loss: 0.9242 - val_acc: 0.7200\n",
      "Epoch 71/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 1.1603 - acc: 0.5659 - val_loss: 0.9143 - val_acc: 0.7190\n",
      "Epoch 72/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.1413 - acc: 0.5694 - val_loss: 0.9068 - val_acc: 0.7190\n",
      "Epoch 73/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.1348 - acc: 0.5789 - val_loss: 0.8998 - val_acc: 0.7220\n",
      "Epoch 74/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 1.1464 - acc: 0.5793 - val_loss: 0.8957 - val_acc: 0.7290\n",
      "Epoch 75/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 1.1388 - acc: 0.5794 - val_loss: 0.8886 - val_acc: 0.7200\n",
      "Epoch 76/200\n",
      "7000/7000 [==============================] - 1s 121us/step - loss: 1.1246 - acc: 0.5740 - val_loss: 0.8816 - val_acc: 0.7290\n",
      "Epoch 77/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 1.1179 - acc: 0.5893 - val_loss: 0.8741 - val_acc: 0.7280\n",
      "Epoch 78/200\n",
      "7000/7000 [==============================] - 1s 121us/step - loss: 1.1077 - acc: 0.5870 - val_loss: 0.8669 - val_acc: 0.7220\n",
      "Epoch 79/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.1075 - acc: 0.5817 - val_loss: 0.8625 - val_acc: 0.7260\n",
      "Epoch 80/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.0845 - acc: 0.5973 - val_loss: 0.8558 - val_acc: 0.7280\n",
      "Epoch 81/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 1.0785 - acc: 0.5969 - val_loss: 0.8496 - val_acc: 0.7320\n",
      "Epoch 82/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.0802 - acc: 0.6011 - val_loss: 0.8422 - val_acc: 0.7310\n",
      "Epoch 83/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 1.0845 - acc: 0.5849 - val_loss: 0.8396 - val_acc: 0.7290\n",
      "Epoch 84/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.0643 - acc: 0.5997 - val_loss: 0.8325 - val_acc: 0.7310\n",
      "Epoch 85/200\n",
      "7000/7000 [==============================] - 1s 113us/step - loss: 1.0759 - acc: 0.6001 - val_loss: 0.8291 - val_acc: 0.7340\n",
      "Epoch 86/200\n",
      "7000/7000 [==============================] - 1s 113us/step - loss: 1.0608 - acc: 0.6106 - val_loss: 0.8225 - val_acc: 0.7340\n",
      "Epoch 87/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 1.0535 - acc: 0.6086 - val_loss: 0.8193 - val_acc: 0.7360\n",
      "Epoch 88/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 1.0502 - acc: 0.6043 - val_loss: 0.8124 - val_acc: 0.7420\n",
      "Epoch 89/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.0539 - acc: 0.6054 - val_loss: 0.8092 - val_acc: 0.7400\n",
      "Epoch 90/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.0407 - acc: 0.6114 - val_loss: 0.8046 - val_acc: 0.7400\n",
      "Epoch 91/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 1.0391 - acc: 0.6133 - val_loss: 0.8005 - val_acc: 0.7410\n",
      "Epoch 92/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 1.0437 - acc: 0.6111 - val_loss: 0.8005 - val_acc: 0.7420\n",
      "Epoch 93/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 1.0284 - acc: 0.6223 - val_loss: 0.7948 - val_acc: 0.7400\n",
      "Epoch 94/200\n",
      "7000/7000 [==============================] - 1s 121us/step - loss: 1.0119 - acc: 0.6211 - val_loss: 0.7884 - val_acc: 0.7420\n",
      "Epoch 95/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 1.0198 - acc: 0.6177 - val_loss: 0.7844 - val_acc: 0.7450\n",
      "Epoch 96/200\n",
      "7000/7000 [==============================] - 1s 124us/step - loss: 0.9998 - acc: 0.6314 - val_loss: 0.7777 - val_acc: 0.7400\n",
      "Epoch 97/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 1.0046 - acc: 0.6274 - val_loss: 0.7744 - val_acc: 0.7450\n",
      "Epoch 98/200\n",
      "7000/7000 [==============================] - 1s 122us/step - loss: 1.0078 - acc: 0.6294 - val_loss: 0.7710 - val_acc: 0.7430\n",
      "Epoch 99/200\n",
      "7000/7000 [==============================] - 1s 124us/step - loss: 0.9940 - acc: 0.6286 - val_loss: 0.7657 - val_acc: 0.7460\n",
      "Epoch 100/200\n",
      "7000/7000 [==============================] - 1s 121us/step - loss: 1.0119 - acc: 0.6184 - val_loss: 0.7658 - val_acc: 0.7480\n",
      "Epoch 101/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.9926 - acc: 0.6290 - val_loss: 0.7632 - val_acc: 0.7500\n",
      "Epoch 102/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.9919 - acc: 0.6386 - val_loss: 0.7588 - val_acc: 0.7500\n",
      "Epoch 103/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.9815 - acc: 0.6387 - val_loss: 0.7537 - val_acc: 0.7500\n",
      "Epoch 104/200\n",
      "7000/7000 [==============================] - 1s 123us/step - loss: 0.9703 - acc: 0.6411 - val_loss: 0.7494 - val_acc: 0.7500\n",
      "Epoch 105/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.9783 - acc: 0.6343 - val_loss: 0.7453 - val_acc: 0.7510\n",
      "Epoch 106/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.9706 - acc: 0.6364 - val_loss: 0.7427 - val_acc: 0.7520\n",
      "Epoch 107/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.9536 - acc: 0.6446 - val_loss: 0.7388 - val_acc: 0.7590\n",
      "Epoch 108/200\n",
      "7000/7000 [==============================] - 1s 128us/step - loss: 0.9632 - acc: 0.6413 - val_loss: 0.7358 - val_acc: 0.7570\n",
      "Epoch 109/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.9591 - acc: 0.6389 - val_loss: 0.7331 - val_acc: 0.7570\n",
      "Epoch 110/200\n",
      "7000/7000 [==============================] - 1s 121us/step - loss: 0.9517 - acc: 0.6390 - val_loss: 0.7327 - val_acc: 0.7550\n",
      "Epoch 111/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.9490 - acc: 0.6503 - val_loss: 0.7276 - val_acc: 0.7550\n",
      "Epoch 112/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.9577 - acc: 0.6427 - val_loss: 0.7254 - val_acc: 0.7560\n",
      "Epoch 113/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.9355 - acc: 0.6469 - val_loss: 0.7223 - val_acc: 0.7570\n",
      "Epoch 114/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.9355 - acc: 0.6467 - val_loss: 0.7172 - val_acc: 0.7550\n",
      "Epoch 115/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.9338 - acc: 0.6566 - val_loss: 0.7151 - val_acc: 0.7550\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.9303 - acc: 0.6600 - val_loss: 0.7147 - val_acc: 0.7560\n",
      "Epoch 117/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.9479 - acc: 0.6510 - val_loss: 0.7104 - val_acc: 0.7580\n",
      "Epoch 118/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.9357 - acc: 0.6566 - val_loss: 0.7090 - val_acc: 0.7580\n",
      "Epoch 119/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.9139 - acc: 0.6570 - val_loss: 0.7043 - val_acc: 0.7590\n",
      "Epoch 120/200\n",
      "7000/7000 [==============================] - 1s 120us/step - loss: 0.9285 - acc: 0.6567 - val_loss: 0.7034 - val_acc: 0.7580\n",
      "Epoch 121/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.9209 - acc: 0.6621 - val_loss: 0.7017 - val_acc: 0.7610\n",
      "Epoch 122/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.9089 - acc: 0.6667 - val_loss: 0.6985 - val_acc: 0.7610\n",
      "Epoch 123/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.9192 - acc: 0.6584 - val_loss: 0.6967 - val_acc: 0.7650\n",
      "Epoch 124/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.8994 - acc: 0.6654 - val_loss: 0.6917 - val_acc: 0.7600\n",
      "Epoch 125/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.9034 - acc: 0.6626 - val_loss: 0.6887 - val_acc: 0.7620\n",
      "Epoch 126/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.9027 - acc: 0.6596 - val_loss: 0.6869 - val_acc: 0.7570\n",
      "Epoch 127/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.8839 - acc: 0.6709 - val_loss: 0.6829 - val_acc: 0.7610\n",
      "Epoch 128/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8764 - acc: 0.6769 - val_loss: 0.6810 - val_acc: 0.7620\n",
      "Epoch 129/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.9028 - acc: 0.6676 - val_loss: 0.6818 - val_acc: 0.7650\n",
      "Epoch 130/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.8844 - acc: 0.6747 - val_loss: 0.6814 - val_acc: 0.7600\n",
      "Epoch 131/200\n",
      "7000/7000 [==============================] - 1s 123us/step - loss: 0.8780 - acc: 0.6779 - val_loss: 0.6780 - val_acc: 0.7600\n",
      "Epoch 132/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.8790 - acc: 0.6681 - val_loss: 0.6751 - val_acc: 0.7630\n",
      "Epoch 133/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.8759 - acc: 0.6783 - val_loss: 0.6726 - val_acc: 0.7650\n",
      "Epoch 134/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.8789 - acc: 0.6689 - val_loss: 0.6698 - val_acc: 0.7660\n",
      "Epoch 135/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.8788 - acc: 0.6736 - val_loss: 0.6702 - val_acc: 0.7700\n",
      "Epoch 136/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.8726 - acc: 0.6776 - val_loss: 0.6677 - val_acc: 0.7640\n",
      "Epoch 137/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8714 - acc: 0.6753 - val_loss: 0.6666 - val_acc: 0.7680\n",
      "Epoch 138/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.8731 - acc: 0.6814 - val_loss: 0.6638 - val_acc: 0.7660\n",
      "Epoch 139/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8581 - acc: 0.6774 - val_loss: 0.6608 - val_acc: 0.7650\n",
      "Epoch 140/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8696 - acc: 0.6797 - val_loss: 0.6605 - val_acc: 0.7660\n",
      "Epoch 141/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.8522 - acc: 0.6839 - val_loss: 0.6578 - val_acc: 0.7670\n",
      "Epoch 142/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.8522 - acc: 0.6799 - val_loss: 0.6560 - val_acc: 0.7630\n",
      "Epoch 143/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.8507 - acc: 0.6779 - val_loss: 0.6542 - val_acc: 0.7650\n",
      "Epoch 144/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8394 - acc: 0.6873 - val_loss: 0.6523 - val_acc: 0.7660\n",
      "Epoch 145/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.8446 - acc: 0.6850 - val_loss: 0.6502 - val_acc: 0.7650\n",
      "Epoch 146/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.8391 - acc: 0.6880 - val_loss: 0.6482 - val_acc: 0.7680\n",
      "Epoch 147/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8299 - acc: 0.6880 - val_loss: 0.6450 - val_acc: 0.7690\n",
      "Epoch 148/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.8474 - acc: 0.6800 - val_loss: 0.6446 - val_acc: 0.7720\n",
      "Epoch 149/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.8396 - acc: 0.6831 - val_loss: 0.6441 - val_acc: 0.7710\n",
      "Epoch 150/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8388 - acc: 0.6913 - val_loss: 0.6429 - val_acc: 0.7720\n",
      "Epoch 151/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.8226 - acc: 0.6917 - val_loss: 0.6406 - val_acc: 0.7730\n",
      "Epoch 152/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.8413 - acc: 0.6887 - val_loss: 0.6389 - val_acc: 0.7740\n",
      "Epoch 153/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.8160 - acc: 0.6939 - val_loss: 0.6361 - val_acc: 0.7760\n",
      "Epoch 154/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8252 - acc: 0.6936 - val_loss: 0.6361 - val_acc: 0.7700\n",
      "Epoch 155/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.8195 - acc: 0.6986 - val_loss: 0.6334 - val_acc: 0.7740\n",
      "Epoch 156/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.8174 - acc: 0.6981 - val_loss: 0.6331 - val_acc: 0.7710\n",
      "Epoch 157/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.8185 - acc: 0.6969 - val_loss: 0.6312 - val_acc: 0.7730\n",
      "Epoch 158/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.8224 - acc: 0.6960 - val_loss: 0.6298 - val_acc: 0.7710\n",
      "Epoch 159/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.8081 - acc: 0.7054 - val_loss: 0.6284 - val_acc: 0.7710\n",
      "Epoch 160/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.8156 - acc: 0.7004 - val_loss: 0.6277 - val_acc: 0.7730\n",
      "Epoch 161/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.8066 - acc: 0.7043 - val_loss: 0.6269 - val_acc: 0.7710\n",
      "Epoch 162/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.7978 - acc: 0.7064 - val_loss: 0.6241 - val_acc: 0.7720\n",
      "Epoch 163/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.7993 - acc: 0.6981 - val_loss: 0.6222 - val_acc: 0.7740\n",
      "Epoch 164/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8049 - acc: 0.7071 - val_loss: 0.6213 - val_acc: 0.7730\n",
      "Epoch 165/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8030 - acc: 0.6987 - val_loss: 0.6233 - val_acc: 0.7740\n",
      "Epoch 166/200\n",
      "7000/7000 [==============================] - 1s 120us/step - loss: 0.7975 - acc: 0.7070 - val_loss: 0.6212 - val_acc: 0.7710\n",
      "Epoch 167/200\n",
      "7000/7000 [==============================] - 1s 124us/step - loss: 0.8014 - acc: 0.7027 - val_loss: 0.6213 - val_acc: 0.7700\n",
      "Epoch 168/200\n",
      "7000/7000 [==============================] - 1s 125us/step - loss: 0.7948 - acc: 0.7060 - val_loss: 0.6199 - val_acc: 0.7720\n",
      "Epoch 169/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.7962 - acc: 0.7073 - val_loss: 0.6189 - val_acc: 0.7710\n",
      "Epoch 170/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.8022 - acc: 0.6999 - val_loss: 0.6185 - val_acc: 0.7730\n",
      "Epoch 171/200\n",
      "7000/7000 [==============================] - 1s 119us/step - loss: 0.7825 - acc: 0.7074 - val_loss: 0.6169 - val_acc: 0.7760\n",
      "Epoch 172/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.7797 - acc: 0.7107 - val_loss: 0.6163 - val_acc: 0.7740\n",
      "Epoch 173/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.7925 - acc: 0.7089 - val_loss: 0.6152 - val_acc: 0.7690\n",
      "Epoch 174/200\n",
      "7000/7000 [==============================] - 1s 113us/step - loss: 0.7772 - acc: 0.7153 - val_loss: 0.6128 - val_acc: 0.7710\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - ETA: 0s - loss: 0.7907 - acc: 0.704 - 1s 113us/step - loss: 0.7923 - acc: 0.7040 - val_loss: 0.6142 - val_acc: 0.7720\n",
      "Epoch 176/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.7635 - acc: 0.7187 - val_loss: 0.6103 - val_acc: 0.7770\n",
      "Epoch 177/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.7756 - acc: 0.7099 - val_loss: 0.6120 - val_acc: 0.7740\n",
      "Epoch 178/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.7712 - acc: 0.7140 - val_loss: 0.6097 - val_acc: 0.7780\n",
      "Epoch 179/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.7635 - acc: 0.7164 - val_loss: 0.6088 - val_acc: 0.7740\n",
      "Epoch 180/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.7715 - acc: 0.7121 - val_loss: 0.6064 - val_acc: 0.7730\n",
      "Epoch 181/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.7745 - acc: 0.7134 - val_loss: 0.6054 - val_acc: 0.7710\n",
      "Epoch 182/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.7682 - acc: 0.7189 - val_loss: 0.6072 - val_acc: 0.7740\n",
      "Epoch 183/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.7698 - acc: 0.7207 - val_loss: 0.6065 - val_acc: 0.7740\n",
      "Epoch 184/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.7566 - acc: 0.7150 - val_loss: 0.6046 - val_acc: 0.7780\n",
      "Epoch 185/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.7638 - acc: 0.7233 - val_loss: 0.6047 - val_acc: 0.7700\n",
      "Epoch 186/200\n",
      "7000/7000 [==============================] - 1s 132us/step - loss: 0.7634 - acc: 0.7120 - val_loss: 0.6028 - val_acc: 0.7740\n",
      "Epoch 187/200\n",
      "7000/7000 [==============================] - 1s 123us/step - loss: 0.7587 - acc: 0.7156 - val_loss: 0.6017 - val_acc: 0.7760\n",
      "Epoch 188/200\n",
      "7000/7000 [==============================] - 1s 123us/step - loss: 0.7573 - acc: 0.7159 - val_loss: 0.6021 - val_acc: 0.7740\n",
      "Epoch 189/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.7416 - acc: 0.7299 - val_loss: 0.5991 - val_acc: 0.7750\n",
      "Epoch 190/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.7649 - acc: 0.7129 - val_loss: 0.5993 - val_acc: 0.7750\n",
      "Epoch 191/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.7565 - acc: 0.7164 - val_loss: 0.5976 - val_acc: 0.7730\n",
      "Epoch 192/200\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 0.7458 - acc: 0.725 - 1s 117us/step - loss: 0.7462 - acc: 0.7250 - val_loss: 0.5968 - val_acc: 0.7740\n",
      "Epoch 193/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.7290 - acc: 0.7267 - val_loss: 0.5966 - val_acc: 0.7730\n",
      "Epoch 194/200\n",
      "7000/7000 [==============================] - 1s 117us/step - loss: 0.7422 - acc: 0.7220 - val_loss: 0.5949 - val_acc: 0.7760\n",
      "Epoch 195/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.7605 - acc: 0.7169 - val_loss: 0.5969 - val_acc: 0.7750\n",
      "Epoch 196/200\n",
      "7000/7000 [==============================] - 1s 115us/step - loss: 0.7333 - acc: 0.7273 - val_loss: 0.5949 - val_acc: 0.7750\n",
      "Epoch 197/200\n",
      "7000/7000 [==============================] - 1s 116us/step - loss: 0.7377 - acc: 0.7291 - val_loss: 0.5947 - val_acc: 0.7820\n",
      "Epoch 198/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.7364 - acc: 0.7273 - val_loss: 0.5934 - val_acc: 0.7790\n",
      "Epoch 199/200\n",
      "7000/7000 [==============================] - 1s 114us/step - loss: 0.7258 - acc: 0.7319 - val_loss: 0.5922 - val_acc: 0.7730\n",
      "Epoch 200/200\n",
      "7000/7000 [==============================] - 1s 118us/step - loss: 0.7462 - acc: 0.7216 - val_loss: 0.5921 - val_acc: 0.7720\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 71us/step\n",
      "2000/2000 [==============================] - 0s 74us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45303837176731654, 0.8452857142857143]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5935160813331604, 0.782]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 4s 107us/step - loss: 1.9148 - acc: 0.1907 - val_loss: 1.8750 - val_acc: 0.2450\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 1.8136 - acc: 0.2851 - val_loss: 1.7468 - val_acc: 0.3403\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 1.6479 - acc: 0.4253 - val_loss: 1.5508 - val_acc: 0.4900\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 1.4353 - acc: 0.5456 - val_loss: 1.3369 - val_acc: 0.5923\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 1.2279 - acc: 0.6272 - val_loss: 1.1462 - val_acc: 0.6523\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 1.0599 - acc: 0.6705 - val_loss: 1.0031 - val_acc: 0.6877\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.9366 - acc: 0.6975 - val_loss: 0.9013 - val_acc: 0.7060\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.8483 - acc: 0.7185 - val_loss: 0.8300 - val_acc: 0.7220\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.7849 - acc: 0.7339 - val_loss: 0.7762 - val_acc: 0.7327\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.7377 - acc: 0.7441 - val_loss: 0.7355 - val_acc: 0.74539 - acc: 0. - ETA: 0s - loss: 0.7408 - acc: 0.7 - ETA: 0s - loss: 0.7386 - acc: 0.7\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 3s 84us/step - loss: 0.7021 - acc: 0.7547 - val_loss: 0.7065 - val_acc: 0.7480\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.6742 - acc: 0.7623 - val_loss: 0.6826 - val_acc: 0.7537\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 3s 83us/step - loss: 0.6517 - acc: 0.7683 - val_loss: 0.6643 - val_acc: 0.7560\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.6327 - acc: 0.7741 - val_loss: 0.6502 - val_acc: 0.7640\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.6167 - acc: 0.7803 - val_loss: 0.6399 - val_acc: 0.7697\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.6027 - acc: 0.7835 - val_loss: 0.6293 - val_acc: 0.7663- loss: 0.6029 - acc: 0.7\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.5905 - acc: 0.7879 - val_loss: 0.6191 - val_acc: 0.7733\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 3s 85us/step - loss: 0.5794 - acc: 0.7922 - val_loss: 0.6102 - val_acc: 0.7773\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 3s 84us/step - loss: 0.5693 - acc: 0.7944 - val_loss: 0.6040 - val_acc: 0.7760\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.5602 - acc: 0.7986 - val_loss: 0.5964 - val_acc: 0.7793\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.5518 - acc: 0.8032 - val_loss: 0.5902 - val_acc: 0.7803\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.5437 - acc: 0.8051 - val_loss: 0.5852 - val_acc: 0.7820\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.5366 - acc: 0.8078 - val_loss: 0.5804 - val_acc: 0.7847\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.5296 - acc: 0.8105 - val_loss: 0.5770 - val_acc: 0.7890\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.5231 - acc: 0.8115 - val_loss: 0.5735 - val_acc: 0.7860\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.5166 - acc: 0.8147 - val_loss: 0.5700 - val_acc: 0.7920\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.5109 - acc: 0.8172 - val_loss: 0.5688 - val_acc: 0.7877\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.5053 - acc: 0.8200 - val_loss: 0.5659 - val_acc: 0.7887\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.4999 - acc: 0.8221 - val_loss: 0.5601 - val_acc: 0.7903\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.4949 - acc: 0.8222 - val_loss: 0.5609 - val_acc: 0.7907\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.4899 - acc: 0.8250 - val_loss: 0.5575 - val_acc: 0.7950\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.4850 - acc: 0.8258 - val_loss: 0.5537 - val_acc: 0.7963\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 3s 86us/step - loss: 0.4803 - acc: 0.8288 - val_loss: 0.5522 - val_acc: 0.7947\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.4759 - acc: 0.8302 - val_loss: 0.5510 - val_acc: 0.7993\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 3s 83us/step - loss: 0.4715 - acc: 0.8327 - val_loss: 0.5488 - val_acc: 0.7970\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.4677 - acc: 0.8337 - val_loss: 0.5488 - val_acc: 0.7953\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.4638 - acc: 0.8356 - val_loss: 0.5464 - val_acc: 0.7993\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.4596 - acc: 0.8368 - val_loss: 0.5464 - val_acc: 0.7997\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.4562 - acc: 0.8380 - val_loss: 0.5436 - val_acc: 0.8023\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.4526 - acc: 0.8394 - val_loss: 0.5442 - val_acc: 0.8010\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 3s 83us/step - loss: 0.4489 - acc: 0.8411 - val_loss: 0.5413 - val_acc: 0.8037\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.4459 - acc: 0.8422 - val_loss: 0.5408 - val_acc: 0.8053\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.4425 - acc: 0.8435 - val_loss: 0.5407 - val_acc: 0.8060\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 3s 83us/step - loss: 0.4390 - acc: 0.8451 - val_loss: 0.5413 - val_acc: 0.8053\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 3s 86us/step - loss: 0.4359 - acc: 0.8467 - val_loss: 0.5399 - val_acc: 0.8050\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 3s 84us/step - loss: 0.4332 - acc: 0.8482 - val_loss: 0.5399 - val_acc: 0.8057\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.4299 - acc: 0.8490 - val_loss: 0.5374 - val_acc: 0.8083\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.4268 - acc: 0.8501 - val_loss: 0.5382 - val_acc: 0.8080\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 3s 84us/step - loss: 0.4241 - acc: 0.8518 - val_loss: 0.5373 - val_acc: 0.8100\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.4215 - acc: 0.8520 - val_loss: 0.5365 - val_acc: 0.8100\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.4183 - acc: 0.8538 - val_loss: 0.5383 - val_acc: 0.8090\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 3s 85us/step - loss: 0.4162 - acc: 0.8548 - val_loss: 0.5383 - val_acc: 0.8130\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 3s 85us/step - loss: 0.4134 - acc: 0.8550 - val_loss: 0.5388 - val_acc: 0.8083\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.4110 - acc: 0.8554 - val_loss: 0.5375 - val_acc: 0.8113\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.4083 - acc: 0.8570 - val_loss: 0.5373 - val_acc: 0.8120\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.4056 - acc: 0.8583 - val_loss: 0.5375 - val_acc: 0.8107\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 3s 84us/step - loss: 0.4034 - acc: 0.8591 - val_loss: 0.5378 - val_acc: 0.8133\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.4010 - acc: 0.8599 - val_loss: 0.5382 - val_acc: 0.8110\n",
      "Epoch 59/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3989 - acc: 0.8612 - val_loss: 0.5376 - val_acc: 0.8150\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3965 - acc: 0.8615 - val_loss: 0.5408 - val_acc: 0.8137\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3943 - acc: 0.8620 - val_loss: 0.5405 - val_acc: 0.8097\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3924 - acc: 0.8633 - val_loss: 0.5368 - val_acc: 0.8117\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.3901 - acc: 0.8634 - val_loss: 0.5423 - val_acc: 0.8093\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3879 - acc: 0.8641 - val_loss: 0.5384 - val_acc: 0.8110\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3859 - acc: 0.8649 - val_loss: 0.5395 - val_acc: 0.8117\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3840 - acc: 0.8662 - val_loss: 0.5415 - val_acc: 0.8117\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3817 - acc: 0.8666 - val_loss: 0.5409 - val_acc: 0.8110\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3799 - acc: 0.8671 - val_loss: 0.5393 - val_acc: 0.8093\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3780 - acc: 0.8681 - val_loss: 0.5419 - val_acc: 0.8100\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3761 - acc: 0.8679 - val_loss: 0.5413 - val_acc: 0.8117\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3739 - acc: 0.8692 - val_loss: 0.5446 - val_acc: 0.8077\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3721 - acc: 0.8702 - val_loss: 0.5403 - val_acc: 0.8120\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3705 - acc: 0.8698 - val_loss: 0.5434 - val_acc: 0.8110\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3686 - acc: 0.8715 - val_loss: 0.5476 - val_acc: 0.8097\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3667 - acc: 0.8718 - val_loss: 0.5462 - val_acc: 0.8063\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3651 - acc: 0.8720 - val_loss: 0.5459 - val_acc: 0.8043\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3632 - acc: 0.8731 - val_loss: 0.5468 - val_acc: 0.8060\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3614 - acc: 0.8741 - val_loss: 0.5457 - val_acc: 0.8027\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3599 - acc: 0.8738 - val_loss: 0.5473 - val_acc: 0.8037\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.3580 - acc: 0.8750 - val_loss: 0.5480 - val_acc: 0.8140\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3564 - acc: 0.8760 - val_loss: 0.5524 - val_acc: 0.8053\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3551 - acc: 0.8749 - val_loss: 0.5500 - val_acc: 0.8090\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3532 - acc: 0.8756 - val_loss: 0.5524 - val_acc: 0.8087\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3518 - acc: 0.8772 - val_loss: 0.5501 - val_acc: 0.8067\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3501 - acc: 0.8774 - val_loss: 0.5509 - val_acc: 0.8047\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3485 - acc: 0.8784 - val_loss: 0.5511 - val_acc: 0.8087\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3468 - acc: 0.8784 - val_loss: 0.5515 - val_acc: 0.8033\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3453 - acc: 0.8781 - val_loss: 0.5530 - val_acc: 0.8063\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3435 - acc: 0.8795 - val_loss: 0.5568 - val_acc: 0.8080\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3426 - acc: 0.8802 - val_loss: 0.5536 - val_acc: 0.8033\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3411 - acc: 0.8805 - val_loss: 0.5603 - val_acc: 0.8053\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3393 - acc: 0.8814 - val_loss: 0.5567 - val_acc: 0.8017\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3381 - acc: 0.8813 - val_loss: 0.5580 - val_acc: 0.8057\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3367 - acc: 0.8828 - val_loss: 0.5578 - val_acc: 0.8033\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3352 - acc: 0.8834 - val_loss: 0.5613 - val_acc: 0.8047\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3336 - acc: 0.8834 - val_loss: 0.5576 - val_acc: 0.8050\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3320 - acc: 0.8843 - val_loss: 0.5608 - val_acc: 0.8047\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3306 - acc: 0.8842 - val_loss: 0.5614 - val_acc: 0.8063\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3295 - acc: 0.8840 - val_loss: 0.5631 - val_acc: 0.8033\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3281 - acc: 0.8850 - val_loss: 0.5631 - val_acc: 0.8060\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3267 - acc: 0.8862 - val_loss: 0.5650 - val_acc: 0.8010\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3254 - acc: 0.8868 - val_loss: 0.5673 - val_acc: 0.8043\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3241 - acc: 0.8862 - val_loss: 0.5699 - val_acc: 0.8010\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3228 - acc: 0.8878 - val_loss: 0.5700 - val_acc: 0.8013\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3216 - acc: 0.8872 - val_loss: 0.5694 - val_acc: 0.8010\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3199 - acc: 0.8883 - val_loss: 0.5682 - val_acc: 0.8047\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 3s 77us/step - loss: 0.3188 - acc: 0.8889 - val_loss: 0.5707 - val_acc: 0.8027\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 3s 81us/step - loss: 0.3173 - acc: 0.8899 - val_loss: 0.5714 - val_acc: 0.8057\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 3s 82us/step - loss: 0.3164 - acc: 0.8894 - val_loss: 0.5721 - val_acc: 0.80400 \n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3151 - acc: 0.8903 - val_loss: 0.5741 - val_acc: 0.8003\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 3s 77us/step - loss: 0.3137 - acc: 0.8911 - val_loss: 0.5741 - val_acc: 0.8043\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3123 - acc: 0.8922 - val_loss: 0.5771 - val_acc: 0.8037\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 3s 77us/step - loss: 0.3111 - acc: 0.8924 - val_loss: 0.5809 - val_acc: 0.8013\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 3s 77us/step - loss: 0.3101 - acc: 0.8909 - val_loss: 0.5856 - val_acc: 0.7990\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3085 - acc: 0.8928 - val_loss: 0.5768 - val_acc: 0.8033\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 3s 78us/step - loss: 0.3076 - acc: 0.8925 - val_loss: 0.5786 - val_acc: 0.8020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3064 - acc: 0.8938 - val_loss: 0.5816 - val_acc: 0.8047cc: \n",
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.3051 - acc: 0.8946 - val_loss: 0.5834 - val_acc: 0.8020\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3038 - acc: 0.8950 - val_loss: 0.5863 - val_acc: 0.8023\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 3s 80us/step - loss: 0.3025 - acc: 0.8943 - val_loss: 0.5841 - val_acc: 0.7980\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 65us/step\n",
      "4000/4000 [==============================] - 0s 67us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2985123092477972, 0.8981515151515151]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6130976296663284, 0.7995]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
